"""
TPR Analysis Tool for Data Analysis V3

This tool provides TPR-specific functionality including detection, calculation,
and preparation for risk analysis pipeline.
"""

import os
import json
import logging
import tempfile
import zipfile
import glob
from pathlib import Path
from typing import Dict, Any, Optional, List, Annotated
import pandas as pd
from ..core.encoding_handler import EncodingHandler
import geopandas as gpd
import numpy as np
from functools import lru_cache
import rasterio
from rasterio.mask import mask
from shapely.geometry import mapping
import warnings

from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState

# Custom JSON encoder to handle numpy types
class NumpyEncoder(json.JSONEncoder):
    """Custom JSON encoder that handles numpy types."""
    def default(self, obj):
        if isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        return super().default(obj)

from app.core.tpr_utils import (
    is_tpr_data, 
    calculate_ward_tpr,
    normalize_ward_name,
    extract_state_from_data,
    get_geopolitical_zone,
    prepare_tpr_summary,
    validate_tpr_data
)
import plotly.graph_objects as go

logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore', category=FutureWarning)

# Cache the Nigeria shapefile to avoid repeated loading
@lru_cache(maxsize=1)
def load_nigeria_shapefile():
    """Load and cache the Nigeria master shapefile."""
    # ALWAYS use absolute path - this is critical for production
    shapefile_path = '/home/ec2-user/ChatMRPT/www/complete_names_wards/wards.shp'
    
    # Debug logging
    try:
        with open('/tmp/shapefile_debug.log', 'a') as f:
            f.write(f"\nload_nigeria_shapefile called at {os.getcwd()}\n")
            f.write(f"Checking path: {shapefile_path}\n")
            f.write(f"Path exists: {os.path.exists(shapefile_path)}\n")
    except:
        pass
    
    if os.path.exists(shapefile_path):
        try:
            gdf = gpd.read_file(shapefile_path)
            logger.info(f"Loaded Nigeria shapefile from {shapefile_path} with {len(gdf)} wards")
            
            with open('/tmp/shapefile_debug.log', 'a') as f:
                f.write(f"SUCCESS: Loaded {len(gdf)} wards\n")
            
            return gdf
        except Exception as e:
            logger.error(f"Error loading shapefile from {shapefile_path}: {e}")
            with open('/tmp/shapefile_debug.log', 'a') as f:
                f.write(f"ERROR loading shapefile: {e}\n")
            return None
    else:
        logger.warning(f"Nigeria shapefile not found at {shapefile_path}")
        with open('/tmp/shapefile_debug.log', 'a') as f:
            f.write(f"FILE NOT FOUND: {shapefile_path}\n")
        return None


def extract_environmental_variables(ward_geometries: gpd.GeoDataFrame, state_name: str = None) -> pd.DataFrame:
    """
    Extract zone-specific environmental variables from raster files for ward geometries.
    
    Args:
        ward_geometries: GeoDataFrame with ward boundaries
        state_name: State name to determine geopolitical zone for variable selection
        
    Returns:
        DataFrame with zone-appropriate environmental variables
    """
    from app.core.tpr_utils import get_geopolitical_zone
    
    # Determine geopolitical zone for variable selection
    if state_name:
        zone = get_geopolitical_zone(state_name.replace(' State', '').strip())
        logger.info(f"Extracting variables for {state_name} (Zone: {zone})")
    else:
        zone = 'Unknown'
        logger.warning("No state provided, using default North-East variables")
    
    # Zone-specific variable selection (scientifically validated)
    # CRITICAL FIX: Added 'urban_extent' to ALL zones for ITN planning
    zone_variables = {
        'North-East': ['pfpr', 'housing_quality', 'evi', 'ndwi', 'soil_wetness', 'urban_extent'],
        'North-West': ['pfpr', 'housing_quality', 'elevation', 'evi', 'distance_to_waterbodies', 'soil_wetness', 'urban_extent'],
        'North-Central': ['pfpr', 'nighttime_lights', 'evi', 'ndmi', 'ndwi', 'soil_wetness', 'rainfall', 'temp', 'urban_extent'],
        'South-East': ['pfpr', 'rainfall', 'elevation', 'evi', 'nighttime_lights', 'soil_wetness', 'temp', 'urban_extent'],
        'South-South': ['pfpr', 'elevation', 'housing_quality', 'temp', 'evi', 'ndwi', 'ndmi', 'urban_extent'],
        'South-West': ['pfpr', 'rainfall', 'elevation', 'evi', 'nighttime_lights', 'urban_extent']
    }
    
    selected_vars = zone_variables.get(zone, zone_variables['North-East'])
    logger.info(f"Selected {len(selected_vars)} variables for {zone}: {selected_vars}")
    
    # Use configuration for paths that work both locally and on AWS
    from app.config.data_paths import RASTER_DIR
    raster_base = RASTER_DIR
    
    # Full map of available rasters (but we'll only use zone-specific ones)
    raster_map = {
        'rainfall': 'rainfall_monthly/2021/X2021_rainfall_year_2021_month_*.tif',
        'temp': 'temperature_monthly/2021/X2021_temperature_year_2021_month_*.tif',
        'evi': 'EVI/EVI_v6.2018.*.mean.1km.tif',
        'ndmi': 'NDMI/NDMI_Nigeria_2023.tif',
        'ndwi': 'NDWI/Nigeria_NDWI_2023.tif',
        'elevation': 'Elevation/MERIT_Elevation.max.1km.tif',
        'distance_to_waterbodies': 'distance_to_water_bodies/distance_to_water.tif',
        'nighttime_lights': 'night_timel_lights/2024/VIIRS_NTL_2024_Nigeria.tif',
        'housing_quality': 'housing/2019_Nature_Africa_Housing_2015_NGA.tiff',
        'urban_extent': 'urban_extent/UrbanPercentage_2024_Nigeria.tif',  # Use most recent year
        'pfpr': 'pf_parasite_rate/202406_Global_Pf_Parasite_Rate_NGA_2022.tiff',
        'soil_wetness': 'soil_wetness/soil_wetness_2023.tif'  # May need actual path
    }
    
    env_data = pd.DataFrame()
    env_data['WardCode'] = ward_geometries['WardCode'] if 'WardCode' in ward_geometries.columns else range(len(ward_geometries))
    
    # Only process variables selected for this zone
    for var_name in selected_vars:
        if var_name not in raster_map:
            logger.warning(f"No raster mapping for variable: {var_name}")
            env_data[var_name.title()] = None
            continue
            
        raster_pattern = raster_map[var_name]
        values = []
        
        # Special handling for urban_extent - try multiple years if needed
        if var_name == 'urban_extent':
            raster_files = []
            # Try years in descending order (2024, 2023, 2022, etc.)
            for year in [2024, 2023, 2022, 2021, 2020]:
                year_pattern = f'urban_extent/UrbanPercentage_{year}_Nigeria.tif'
                year_path = os.path.join(raster_base, year_pattern)
                if os.path.exists(year_path):
                    raster_files = [year_path]
                    logger.info(f"Using urban percentage data from year {year}")
                    break
            
            if not raster_files:
                # Fall back to glob pattern
                raster_path_pattern = os.path.join(raster_base, 'urban_extent/*.tif')
                raster_files = glob.glob(raster_path_pattern)
                if raster_files:
                    # Use the most recent file based on filename
                    raster_files = sorted(raster_files, reverse=True)[:1]
        else:
            raster_path_pattern = os.path.join(raster_base, raster_pattern)
            raster_files = glob.glob(raster_path_pattern)
        
        if not raster_files:
            logger.warning(f"No raster files found for {var_name} at {raster_path_pattern}")
            # Generate mock values for testing (zone-specific ranges)
            import numpy as np
            np.random.seed(42)  # For reproducibility
            
            mock_ranges = {
                'pfpr': (0.1, 0.5),  # Parasite prevalence rate
                'housing_quality': (0.0, 0.2),  # Housing quality index
                'evi': (0.2, 0.6),  # Enhanced vegetation index
                'ndwi': (-0.5, -0.3),  # Normalized difference water index
                'ndmi': (-0.1, 0.1),  # Normalized difference moisture index
                'soil_wetness': (0.3, 0.7),  # Soil wetness
                'elevation': (150, 300),  # Elevation in meters
                'distance_to_waterbodies': (1000, 10000),  # Distance in meters
                'nighttime_lights': (0.0, 0.5),  # Nighttime lights intensity
                'rainfall': (800, 1200),  # Annual rainfall in mm
                'temp': (25, 35),  # Temperature in Celsius
                'urban_extent': (10, 60)  # Urban percentage (10-60% for realistic variation)
            }
            
            if var_name in mock_ranges:
                min_val, max_val = mock_ranges[var_name]
                # Add some spatial variation
                mock_values = np.random.uniform(min_val, max_val, len(ward_geometries))
                
                # CRITICAL: Use correct column name for urban percentage
                if var_name == 'urban_extent':
                    env_data['urban_percentage'] = mock_values
                    logger.info(f"Generated mock urban percentage data: mean={mock_values.mean():.2f}%")
                else:
                    env_data[var_name.title().replace('_', '')] = mock_values
                    logger.info(f"Generated mock data for {var_name}: mean={mock_values.mean():.2f}")
            else:
                env_data[var_name.title()] = None
            continue
        
        # Use the first matching file (or aggregate for monthly data)
        raster_file = raster_files[0]
        
        try:
            with rasterio.open(raster_file) as src:
                # Extract values for each ward
                for idx, row in ward_geometries.iterrows():
                    try:
                        # Get ward geometry
                        geom = row.geometry
                        
                        # Extract value at centroid
                        if geom is not None:
                            centroid = geom.centroid
                            # Sample raster at centroid
                            for val in src.sample([(centroid.x, centroid.y)]):
                                value = val[0] if val[0] != src.nodata else None
                                values.append(value)
                        else:
                            values.append(None)
                    except Exception as e:
                        logger.debug(f"Error extracting {var_name} for ward {idx}: {e}")
                        values.append(None)
            
            # For rainfall and temperature, calculate annual values
            if 'rainfall' in var_name.lower() and len(raster_files) > 1:
                # Sum monthly rainfall
                monthly_values = []
                for rf in raster_files[:12]:  # Use up to 12 months
                    with rasterio.open(rf) as src:
                        month_vals = []
                        for idx, row in ward_geometries.iterrows():
                            if row.geometry is not None:
                                centroid = row.geometry.centroid
                                for val in src.sample([(centroid.x, centroid.y)]):
                                    month_vals.append(val[0] if val[0] != src.nodata else 0)
                            else:
                                month_vals.append(0)
                        monthly_values.append(month_vals)
                
                # Sum across months
                values = [sum(month_vals) for month_vals in zip(*monthly_values)]
            
            elif 'temperature' in var_name.lower() and len(raster_files) > 1:
                # Average monthly temperature
                monthly_values = []
                for rf in raster_files[:12]:
                    with rasterio.open(rf) as src:
                        month_vals = []
                        for idx, row in ward_geometries.iterrows():
                            if row.geometry is not None:
                                centroid = row.geometry.centroid
                                for val in src.sample([(centroid.x, centroid.y)]):
                                    month_vals.append(val[0] if val[0] != src.nodata else None)
                            else:
                                month_vals.append(None)
                        monthly_values.append(month_vals)
                
                # Average across months
                values = []
                for month_vals in zip(*monthly_values):
                    valid_vals = [v for v in month_vals if v is not None]
                    if valid_vals:
                        values.append(sum(valid_vals) / len(valid_vals))
                    else:
                        values.append(None)
            
            # CRITICAL: Use correct column name for urban percentage
            if var_name == 'urban_extent':
                env_data['urban_percentage'] = values
                logger.info(f"Extracted urban_percentage: {sum(1 for v in values if v is not None)}/{len(values)} valid values")
            else:
                env_data[var_name] = values
                logger.info(f"Extracted {var_name}: {sum(1 for v in values if v is not None)}/{len(values)} valid values")
            
        except Exception as e:
            logger.error(f"Error processing raster {var_name}: {e}")
            if var_name == 'urban_extent':
                # Ensure we always have urban_percentage column even if extraction fails
                env_data['urban_percentage'] = [30.0] * len(ward_geometries)  # Default 30% urban
                logger.warning("Using default urban percentage of 30% due to extraction error")
            else:
                env_data[var_name] = None
    
    # FINAL CHECK: Ensure urban_percentage column exists
    if 'urban_percentage' not in env_data.columns:
        logger.warning("Urban percentage column missing after extraction, adding default values")
        env_data['urban_percentage'] = 30.0  # Default fallback
    
    return env_data


def match_and_merge_data(tpr_df: pd.DataFrame, state_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """
    Match TPR data with shapefile using ward name normalization.
    
    Args:
        tpr_df: DataFrame with TPR results
        state_gdf: GeoDataFrame with state ward boundaries
        
    Returns:
        Merged GeoDataFrame
    """
    logger.info(f"üîÑ Starting ward matching: {len(tpr_df)} TPR records, {len(state_gdf)} shapefile wards")
    
    # Normalize ward names in both datasets
    tpr_df['WardName_norm'] = tpr_df['WardName'].apply(normalize_ward_name)
    state_gdf['WardName_norm'] = state_gdf['WardName'].apply(normalize_ward_name)
    
    # Log sample normalized names for debugging
    logger.debug(f"Sample TPR ward names: {list(tpr_df['WardName_norm'].head(3))}")
    logger.debug(f"Sample shapefile ward names: {list(state_gdf['WardName_norm'].head(3))}")
    
    # Try exact match first
    merged = state_gdf.merge(
        tpr_df,
        on='WardName_norm',
        how='left',
        suffixes=('', '_tpr')
    )
    
    # Check match rate
    matched = merged['TPR'].notna().sum()
    total = len(merged)
    match_rate = matched / total * 100 if total > 0 else 0
    
    logger.info(f"üìä Initial ward matching: {matched}/{total} ({match_rate:.1f}%) matched")
    
    # If match rate is low, try fuzzy matching
    if match_rate < 80:
        from difflib import get_close_matches
        
        unmatched_shapefile = merged[merged['TPR'].isna()]['WardName_norm'].unique()
        unmatched_tpr = tpr_df['WardName_norm'].unique()
        
        fuzzy_matches = {}
        for ward in unmatched_shapefile:
            matches = get_close_matches(ward, unmatched_tpr, n=1, cutoff=0.8)
            if matches:
                fuzzy_matches[ward] = matches[0]
        
        # Apply fuzzy matches
        for shapefile_ward, tpr_ward in fuzzy_matches.items():
            tpr_data = tpr_df[tpr_df['WardName_norm'] == tpr_ward].iloc[0]
            merged.loc[merged['WardName_norm'] == shapefile_ward, 'TPR'] = tpr_data['TPR']
            merged.loc[merged['WardName_norm'] == shapefile_ward, 'Total_Tested'] = tpr_data['Total_Tested']
            merged.loc[merged['WardName_norm'] == shapefile_ward, 'Total_Positive'] = tpr_data['Total_Positive']
        
        new_matched = merged['TPR'].notna().sum()
        logger.info(f"After fuzzy matching: {new_matched}/{total} matched")
    
    return merged


def create_tpr_map(tpr_results: pd.DataFrame, session_folder: str, state_name: str) -> bool:
    """
    Create an interactive TPR distribution map.
    
    Args:
        tpr_results: DataFrame with TPR results
        session_folder: Session folder path
        state_name: State name
        
    Returns:
        True if map was created successfully
    """
    logger.info(f"üó∫Ô∏è create_tpr_map called for {state_name} with {len(tpr_results)} TPR results")
    
    try:
        # Load Nigeria shapefile and filter to state
        logger.info("üìÇ Loading Nigeria shapefile for map creation...")
        master_gdf = load_nigeria_shapefile()
        if master_gdf is None:
            logger.warning("‚ùå Nigeria shapefile not found, skipping map creation")
            return False
        
        logger.info(f"‚úÖ Shapefile loaded with {len(master_gdf)} total wards")
        
        # Clean state name for matching
        clean_state = state_name.replace(' State', '').strip()
        logger.info(f"üîç Filtering shapefile for state: '{clean_state}'")
        
        state_gdf = master_gdf[master_gdf['StateName'] == clean_state].copy()
        
        if state_gdf.empty:
            logger.warning(f"‚ö†Ô∏è No exact match, trying case-insensitive...")
            # Try case-insensitive match
            state_gdf = master_gdf[master_gdf['StateName'].str.lower() == clean_state.lower()].copy()
        
        if state_gdf.empty:
            logger.warning(f"‚ùå No shapefile data for {state_name}, skipping map")
            return False
        
        logger.info(f"‚úÖ Found {len(state_gdf)} wards for {state_name}")
        
        # Match and merge TPR data with shapefile
        logger.info("üîÑ Matching TPR data with shapefile...")
        merged_gdf = match_and_merge_data(tpr_results, state_gdf)
        
        # Log matching results
        matched = merged_gdf['TPR'].notna().sum()
        logger.info(f"‚úÖ Matched {matched}/{len(merged_gdf)} wards with TPR data")
        
        # Create hover text
        hover_text = []
        for _, row in merged_gdf.iterrows():
            if pd.notna(row.get('TPR')):
                text = f"<b>{row['WardName']}</b><br>"
                text += f"LGA: {row.get('LGAName', 'Unknown')}<br>"
                text += f"TPR: {row['TPR']:.1f}%<br>"
                text += f"Tested: {int(row.get('Total_Tested', 0)):,}<br>"
                text += f"Positive: {int(row.get('Total_Positive', 0)):,}"
            else:
                text = f"<b>{row['WardName']}</b><br>No TPR data available"
            hover_text.append(text)
        
        # Convert to GeoJSON
        geojson = merged_gdf.__geo_interface__
        
        # Create figure
        fig = go.Figure()
        
        # Add choropleth layer
        fig.add_trace(go.Choroplethmapbox(
            geojson=geojson,
            locations=merged_gdf.index,
            z=merged_gdf['TPR'],
            colorscale=[
                [0, '#2ecc71'],      # Green for low TPR
                [0.2, '#f1c40f'],    # Yellow
                [0.4, '#e67e22'],    # Orange
                [0.6, '#e74c3c'],    # Red
                [1.0, '#9b59b6']     # Purple for very high TPR
            ],
            marker_opacity=0.8,
            marker_line_width=0.5,
            marker_line_color='black',
            hovertemplate='%{hovertext}<extra></extra>',
            hovertext=hover_text,
            colorbar=dict(
                title=dict(
                    text="TPR (%)",
                    font=dict(size=12)
                ),
                tickmode='array',
                tickvals=[0, 20, 40, 60, 80, 100],
                ticktext=['0%', '20%', '40%', '60%', '80%', '100%']
            ),
            zmin=0,
            zmax=100
        ))
        
        # Update layout
        center_lat = merged_gdf.geometry.centroid.y.mean()
        center_lon = merged_gdf.geometry.centroid.x.mean()
        
        fig.update_layout(
            title=dict(
                text=f"TPR Distribution - {state_name}",
                font=dict(size=16, family="Arial, sans-serif"),
                x=0.5,
                xanchor='center'
            ),
            mapbox=dict(
                style="open-street-map",
                center=dict(lat=center_lat, lon=center_lon),
                zoom=6.5
            ),
            height=700,
            margin=dict(t=50, b=30, l=30, r=30)
        )
        
        # Save the map
        map_path = os.path.join(session_folder, 'tpr_distribution_map.html')
        logger.info(f"üíæ Attempting to save map to: {map_path}")
        
        try:
            fig.write_html(
                map_path,
                include_plotlyjs='cdn',
                config={'displayModeBar': True, 'displaylogo': False}
            )
            
            # Verify file was created
            if os.path.exists(map_path):
                file_size = os.path.getsize(map_path)
                logger.info(f"‚úÖ TPR distribution map saved successfully ({file_size} bytes)")
                return True
            else:
                logger.error(f"‚ùå Map file was not created at {map_path}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Failed to write HTML file: {e}")
            return False
        
    except Exception as e:
        import traceback
        logger.error(f"‚ùå Error creating TPR map: {e}")
        logger.error(f"Traceback:\n{traceback.format_exc()}")
        return False


def create_shapefile_package(gdf: gpd.GeoDataFrame, output_dir: str) -> str:
    """
    Create a shapefile ZIP package from GeoDataFrame.
    
    Args:
        gdf: GeoDataFrame to save
        output_dir: Directory to save the ZIP file
        
    Returns:
        Path to created ZIP file
    """
    # Create temporary directory for shapefile components
    temp_dir = tempfile.mkdtemp()
    shapefile_base = os.path.join(temp_dir, 'ward_boundaries')
    
    # Save shapefile
    gdf.to_file(shapefile_base + '.shp')
    
    # Create ZIP
    zip_path = os.path.join(output_dir, 'raw_shapefile.zip')
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
        for ext in ['.shp', '.shx', '.dbf', '.prj', '.cpg']:
            file_path = shapefile_base + ext
            if os.path.exists(file_path):
                arcname = 'ward_boundaries' + ext
                zf.write(file_path, arcname)
    
    # Clean up temp directory
    import shutil
    shutil.rmtree(temp_dir)
    
    logger.info(f"Created shapefile package: {zip_path}")
    return zip_path


@tool
def analyze_tpr_data(
    thought: str,
    action: str = "analyze",
    options: str = "{}",
    graph_state: Annotated[dict, InjectedState] = None
) -> str:
    """
    Analyze TPR (Test Positivity Rate) data with multiple capabilities.
    
    This tool detects TPR data, calculates test positivity rates using production logic,
    and prepares data for risk analysis pipeline.
    
    Actions:
    - analyze: Basic exploration and summary of TPR data
    - calculate_tpr: Calculate ward-level TPR with user selections
    - prepare_for_risk: Create raw_data.csv and raw_shapefile.zip for risk analysis
    
    Options for calculate_tpr:
    - age_group: 'all_ages' (default), 'u5' (Under 5), 'o5' (Over 5), 'pw' (Pregnant women)
    - test_method: 'both' (default), 'rdt' (RDT only), 'microscopy' (Microscopy only)  
    - facility_level: 'all' (default), 'primary', 'secondary', 'tertiary'
    
    Example options JSON:
    {"age_group": "u5", "test_method": "both", "facility_level": "primary"}
    
    Args:
        thought: Internal reasoning about the analysis
        action: The action to perform (analyze, calculate_tpr, prepare_for_risk)
        options: JSON string with additional options
        
    Returns:
        String with results or status message
    """
    try:
        # Parse options
        try:
            opts = json.loads(options) if options else {}
        except:
            opts = {}
        
        # Get session information
        session_id = graph_state.get('session_id', 'default') if graph_state else 'default'
        session_folder = f"instance/uploads/{session_id}"
        
        # Load data from session
        data_files = glob.glob(os.path.join(session_folder, "*.xlsx")) + \
                    glob.glob(os.path.join(session_folder, "*.csv"))
        
        if not data_files:
            return "No data files found in session. Please upload TPR data first."
        
        # Load the most recent file
        data_file = max(data_files, key=os.path.getctime)
        
        if data_file.endswith('.csv'):
            # Load TPR data with original column names
            df = EncodingHandler.read_csv_with_encoding(data_file)
        else:
            df = pd.read_excel(data_file)
        
        # Check if this is TPR data
        is_tpr, tpr_info = is_tpr_data(df)
        
        if not is_tpr:
            return f"This doesn't appear to be TPR data. Confidence: {tpr_info['confidence']:.2f}"
        
        logger.info(f"TPR data detected with confidence {tpr_info['confidence']:.2f}")
        
        # Validate TPR data
        is_valid, errors = validate_tpr_data(df)
        if not is_valid and action != "analyze":
            return f"TPR data validation failed:\n" + "\n".join(errors)
        
        # Extract state name
        state_name = extract_state_from_data(df)
        
        # Perform requested action
        if action == "analyze":
            # Basic analysis and exploration
            summary = {
                "file": os.path.basename(data_file),
                "rows": len(df),
                "columns": len(df.columns),
                "state": state_name,
                "tpr_confidence": tpr_info['confidence'],
                "has_rdt": tpr_info['has_rdt'],
                "has_microscopy": tpr_info['has_microscopy'],
                "validation_errors": errors if not is_valid else None
            }
            
            # Get column info
            column_info = []
            for col in df.columns[:20]:  # First 20 columns
                dtype = str(df[col].dtype)
                null_count = df[col].isna().sum()
                column_info.append(f"  - {col}: {dtype} ({null_count} nulls)")
            
            result = f"""TPR Data Analysis Results:
            
File: {summary['file']}
State: {summary['state']}
Shape: {summary['rows']} rows √ó {summary['columns']} columns
TPR Detection Confidence: {summary['tpr_confidence']:.2%}

Data Quality:
- Has RDT data: {summary['has_rdt']}
- Has Microscopy data: {summary['has_microscopy']}
- Validation: {'Passed' if is_valid else 'Failed - ' + ', '.join(errors[:3])}

Sample Columns:
{chr(10).join(column_info)}

You can now:
1. Calculate TPR: Use action="calculate_tpr"
2. Prepare for risk analysis: Use action="prepare_for_risk"
"""
            return result
            
        elif action == "calculate_tpr":
            # Get user selections from options (with validation)
            age_group = opts.get('age_group', 'all_ages')
            test_method = opts.get('test_method', 'both')
            facility_level = opts.get('facility_level', 'all')
            
            # Validate selections
            valid_age_groups = ['all_ages', 'u5', 'o5', 'pw']
            valid_test_methods = ['both', 'rdt', 'microscopy']
            valid_facility_levels = ['all', 'primary', 'secondary', 'tertiary']
            
            if age_group not in valid_age_groups:
                logger.warning(f"Invalid age group '{age_group}', using 'all_ages'")
                age_group = 'all_ages'
            
            if test_method not in valid_test_methods:
                logger.warning(f"Invalid test method '{test_method}', using 'both'")
                test_method = 'both'
            
            if facility_level not in valid_facility_levels:
                logger.warning(f"Invalid facility level '{facility_level}', using 'all'")
                facility_level = 'all'
            
            logger.info(f"Calculating TPR - Age: {age_group}, Method: {test_method}, Facilities: {facility_level}")
            
            try:
                tpr_results = calculate_ward_tpr(df, 
                                                age_group=age_group,
                                                test_method=test_method,
                                                facility_level=facility_level)
            except Exception as e:
                logger.error(f"Error calculating TPR: {e}")
                return f"Error calculating TPR: {str(e)}. Please check your data format and try again."
            
            if tpr_results.empty:
                # Provide helpful feedback about what might be missing
                available_cols = [col for col in df.columns if any(
                    keyword in col.lower() for keyword in ['rdt', 'microscopy', 'tested', 'positive']
                )]
                
                return f"""No TPR results calculated. This could be because:
1. No data found for selected age group '{age_group}'
2. No {test_method} test data available
3. No facilities matching level '{facility_level}'

Available test-related columns in your data:
{', '.join(available_cols[:10])}

Try using 'all_ages' with 'both' test methods and 'all' facilities for the broadest analysis."""
            
            # Save TPR results
            tpr_output_path = os.path.join(session_folder, 'tpr_results.csv')
            tpr_results.to_csv(tpr_output_path, index=False)
            
            # Prepare summary
            summary = prepare_tpr_summary(tpr_results)
            
            # Format selections for display
            age_display = {
                'all_ages': 'All age groups',
                'u5': 'Under 5 years',
                'o5': 'Over 5 years',
                'pw': 'Pregnant women'
            }.get(age_group, age_group)
            
            method_display = {
                'both': 'RDT and Microscopy (max TPR)',
                'rdt': 'RDT only',
                'microscopy': 'Microscopy only'
            }.get(test_method, test_method)
            
            facility_display = {
                'all': 'All facilities',
                'primary': 'Primary health centers',
                'secondary': 'Secondary facilities',
                'tertiary': 'Tertiary facilities'
            }.get(facility_level, facility_level)
            
            result = f"""TPR Calculation Complete!

State: {state_name}
Selections:
- Age Group: {age_display}
- Test Method: {method_display}
- Facility Level: {facility_display}

Wards Analyzed: {summary['total_wards']}

Overall Statistics:
- Mean TPR: {summary['mean_tpr']}%
- Median TPR: {summary['median_tpr']}%
- Max TPR: {summary['max_tpr']}%
- Overall TPR: {summary['overall_tpr']}%
- Total Tested: {summary['total_tested']:,}
- Total Positive: {summary['total_positive']:,}

High Risk Wards (TPR > 20%):"""
            
            if summary['high_risk_wards']:
                for ward in summary['high_risk_wards'][:5]:
                    result += f"\n  - {ward['WardName']} ({ward['LGA']}): {ward['TPR']}%"
            else:
                result += "\n  None identified"
            
            result += f"\n\nResults saved to: tpr_results.csv"
            
            # AUTOMATICALLY PREPARE FOR RISK ANALYSIS (production approach)
            logger.info("üöÄ Starting automatic preparation for risk analysis")
            
            # Create comprehensive debug tracking
            debug_stages = {
                "shapefile_loading": {},
                "ward_matching": {},
                "env_extraction": {},
                "file_creation": {},
                "map_creation": {}
            }
            
            try:
                # Load Nigeria shapefile and filter to state
                logger.info(f"üìÇ Attempting to load Nigeria shapefile for risk preparation...")
                
                # Debug to file
                with open('/tmp/tpr_debug.log', 'a') as f:
                    f.write(f"\n--- TPR Debug {session_id} ---\n")
                    f.write(f"Timestamp: {pd.Timestamp.now()}\n")
                    f.write(f"State: {state_name}\n")
                    f.write(f"About to call load_nigeria_shapefile()\n")
                
                master_gdf = load_nigeria_shapefile()
                
                with open('/tmp/tpr_debug.log', 'a') as f:
                    f.write(f"Shapefile load result: {master_gdf is not None}\n")
                    if master_gdf is not None:
                        f.write(f"Loaded {len(master_gdf)} wards\n")
                    else:
                        f.write("ERROR: master_gdf is None!\n")
                
                if master_gdf is not None:
                    logger.info(f"‚úÖ Successfully loaded shapefile with {len(master_gdf)} wards")
                    debug_stages["shapefile_loading"] = {
                        "success": True,
                        "total_wards": len(master_gdf),
                        "columns": list(master_gdf.columns)[:10]  # First 10 columns
                    }
                    
                    # Clean state name for matching
                    clean_state = state_name.replace(' State', '').strip()
                    logger.info(f"üîç Filtering for state: '{clean_state}'")
                    
                    state_gdf = master_gdf[master_gdf['StateName'] == clean_state].copy()
                    
                    if state_gdf.empty:
                        logger.warning(f"‚ö†Ô∏è No exact match for '{clean_state}', trying case-insensitive")
                        # Try case-insensitive match
                        state_gdf = master_gdf[master_gdf['StateName'].str.lower() == clean_state.lower()].copy()
                    
                    if not state_gdf.empty:
                        logger.info(f"‚úÖ Found {len(state_gdf)} wards in shapefile for {state_name}")
                        debug_stages["shapefile_loading"]["state_wards"] = int(len(state_gdf))
                        
                        # Match and merge TPR data with shapefile
                        logger.info("üîÑ Matching TPR data with shapefile wards...")
                        try:
                            merged_gdf = match_and_merge_data(tpr_results, state_gdf)
                            
                            # Count matches
                            matched_count = int(merged_gdf['TPR'].notna().sum())
                            total_count = int(len(merged_gdf))
                            match_rate = (matched_count / total_count * 100) if total_count > 0 else 0
                            
                            logger.info(f"‚úÖ Ward matching complete: {matched_count}/{total_count} ({match_rate:.1f}%)")
                            debug_stages["ward_matching"] = {
                                "success": True,
                                "matched": matched_count,
                                "total": total_count,
                                "match_rate": match_rate
                            }
                        except Exception as e:
                            logger.error(f"‚ùå Ward matching failed: {e}")
                            debug_stages["ward_matching"] = {"success": False, "error": str(e)}
                            merged_gdf = state_gdf.copy()
                            merged_gdf['TPR'] = 0
                        
                        # Extract environmental variables (zone-specific)
                        logger.info("üåç Extracting zone-specific environmental variables from rasters")
                        try:
                            env_data = extract_environmental_variables(merged_gdf, state_name)
                            logger.info(f"‚úÖ Environmental data extracted: {len(env_data.columns)} variables")
                            debug_stages["env_extraction"] = {
                                "success": True,
                                "variables": list(env_data.columns),
                                "rows": int(len(env_data))
                            }
                        except Exception as e:
                            logger.error(f"‚ùå Environmental extraction failed: {e}")
                            debug_stages["env_extraction"] = {"success": False, "error": str(e)}
                            # Create minimal env_data
                            env_data = pd.DataFrame()
                            env_data['WardCode'] = range(len(merged_gdf))
                        
                        # Prepare final dataset
                        logger.info("üìä Preparing final dataset for risk analysis")
                        final_df = pd.DataFrame()
                        
                        # Add identifiers (CRITICAL for risk analysis)
                        final_df['WardCode'] = merged_gdf['WardCode'] if 'WardCode' in merged_gdf.columns else range(len(merged_gdf))
                        final_df['StateCode'] = merged_gdf['StateCode'] if 'StateCode' in merged_gdf.columns else state_name[:2].upper()
                        final_df['LGACode'] = merged_gdf['LGACode'] if 'LGACode' in merged_gdf.columns else ''
                        final_df['WardName'] = merged_gdf['WardName']
                        final_df['LGA'] = merged_gdf['LGAName'] if 'LGAName' in merged_gdf.columns else merged_gdf.get('LGA', '')
                        final_df['State'] = state_name
                        final_df['GeopoliticalZone'] = get_geopolitical_zone(state_name)
                        
                        # Add TPR metrics
                        final_df['TPR'] = merged_gdf['TPR'].fillna(0)
                        # Fix: Convert to native Python int to avoid JSON serialization issues
                        final_df['Total_Tested'] = merged_gdf['Total_Tested'].fillna(0).apply(lambda x: int(x) if pd.notna(x) else 0)
                        final_df['Total_Positive'] = merged_gdf['Total_Positive'].fillna(0).apply(lambda x: int(x) if pd.notna(x) else 0)
                        
                        # Add environmental variables
                        for col in env_data.columns:
                            if col != 'WardCode':
                                final_df[col] = env_data[col]
                        
                        logger.info(f"üìã Final dataset prepared: {len(final_df)} rows, {len(final_df.columns)} columns")
                        
                        # Save raw_data.csv
                        raw_data_path = os.path.join(session_folder, 'raw_data.csv')
                        try:
                            final_df.to_csv(raw_data_path, index=False)
                            logger.info(f"‚úÖ Saved raw_data.csv with {len(final_df)} wards")
                            debug_stages["file_creation"]["raw_data"] = {
                                "success": True,
                                "path": raw_data_path,
                                "rows": len(final_df),
                                "columns": len(final_df.columns)
                            }
                        except Exception as e:
                            logger.error(f"‚ùå Failed to save raw_data.csv: {e}")
                            debug_stages["file_creation"]["raw_data"] = {"success": False, "error": str(e)}
                        
                        # Create raw_shapefile.zip
                        logger.info("üì¶ Creating shapefile package...")
                        try:
                            # Ensure shapefile has same columns as CSV
                            for col in final_df.columns:
                                if col not in merged_gdf.columns:
                                    merged_gdf[col] = final_df[col]
                            
                            shapefile_path = create_shapefile_package(merged_gdf, session_folder)
                            logger.info(f"‚úÖ Created raw_shapefile.zip")
                            debug_stages["file_creation"]["shapefile"] = {
                                "success": True,
                                "path": shapefile_path
                            }
                        except Exception as e:
                            logger.error(f"‚ùå Failed to create shapefile: {e}")
                            debug_stages["file_creation"]["shapefile"] = {"success": False, "error": str(e)}
                        
                        # Create TPR map visualization
                        logger.info("üó∫Ô∏è Creating TPR distribution map...")
                        try:
                            map_created = create_tpr_map(tpr_results, session_folder, state_name)
                            if map_created:
                                logger.info("‚úÖ TPR map created successfully")
                                result += f"\n\nüìç TPR Map Visualization created: tpr_distribution_map.html"
                                debug_stages["map_creation"] = {"success": True}
                            else:
                                logger.warning("‚ö†Ô∏è Map creation returned False")
                                debug_stages["map_creation"] = {"success": False, "error": "Function returned False"}
                        except Exception as e:
                            logger.error(f"‚ùå Map creation failed with exception: {e}")
                            import traceback
                            debug_stages["map_creation"] = {
                                "success": False,
                                "error": str(e),
                                "traceback": traceback.format_exc()
                            }
                        
                        # Set session flags for risk analysis
                        flag_file = os.path.join(session_folder, '.risk_ready')
                        Path(flag_file).touch()
                        
                        result += f"\n\n‚úÖ Data automatically prepared for risk analysis"
                        result += f"\n   ‚Ä¢ raw_data.csv with environmental variables"
                        result += f"\n   ‚Ä¢ raw_shapefile.zip for geographic analysis"
                    else:
                        logger.warning(f"‚ö†Ô∏è No shapefile data found for {state_name}")
                        debug_stages["shapefile_loading"]["state_wards"] = 0
                        # Still create map if possible (it has its own shapefile loading)
                        logger.info("üó∫Ô∏è Attempting map creation despite empty state data...")
                        try:
                            map_created = create_tpr_map(tpr_results, session_folder, state_name)
                            if map_created:
                                result += f"\n\nüìç TPR Map Visualization created: tpr_distribution_map.html"
                                debug_stages["map_creation"] = {"success": True, "fallback": True}
                            else:
                                debug_stages["map_creation"] = {"success": False, "error": "No state data"}
                        except Exception as e:
                            logger.error(f"‚ùå Fallback map creation failed: {e}")
                            debug_stages["map_creation"] = {"success": False, "error": str(e)}
                else:
                    logger.error("‚ùå CRITICAL: Nigeria shapefile returned None")
                    debug_stages["shapefile_loading"] = {"success": False, "error": "Shapefile is None"}
                    # Still try to create map
                    logger.info("üó∫Ô∏è Attempting map creation without shapefile...")
                    try:
                        map_created = create_tpr_map(tpr_results, session_folder, state_name)
                        if map_created:
                            result += f"\n\nüìç TPR Map Visualization created: tpr_distribution_map.html"
                            debug_stages["map_creation"] = {"success": True, "emergency": True}
                        else:
                            debug_stages["map_creation"] = {"success": False, "error": "No shapefile"}
                    except Exception as e:
                        logger.error(f"‚ùå Emergency map creation failed: {e}")
                        debug_stages["map_creation"] = {"success": False, "error": str(e)}
                
                # Save debug stages to file
                debug_file_path = os.path.join(session_folder, 'tpr_analysis_debug.json')
                with open(debug_file_path, 'w') as f:
                    json.dump(debug_stages, f, indent=2, cls=NumpyEncoder)
                logger.info(f"üíæ Debug stages saved to {debug_file_path}")
            
            except Exception as e:
                import traceback
                error_trace = traceback.format_exc()
                logger.error(f"‚ùå Error preparing for risk analysis: {e}\n{error_trace}")
                
                # Save error debug info
                debug_stages["critical_error"] = {
                    "message": str(e),
                    "traceback": error_trace
                }
                debug_file_path = os.path.join(session_folder, 'tpr_analysis_error.json')
                with open(debug_file_path, 'w') as f:
                    json.dump(debug_stages, f, indent=2, cls=NumpyEncoder)
                
                # Don't fail the whole TPR calculation, just log the error
                result += f"\n\n‚ö†Ô∏è Note: Could not prepare risk analysis files automatically (see tpr_analysis_error.json)"
            
            return result
            
        elif action == "prepare_for_risk":
            # Prepare data for risk analysis pipeline
            logger.info("Preparing TPR data for risk analysis")
            
            # Step 1: Calculate TPR if not already done
            tpr_results_file = os.path.join(session_folder, 'tpr_results.csv')
            if os.path.exists(tpr_results_file):
                # Load TPR results
                tpr_results = EncodingHandler.read_csv_with_encoding(tpr_results_file)
            else:
                tpr_results = calculate_ward_tpr(df, age_group='all_ages')
            
            # Step 2: Load Nigeria shapefile and filter to state
            master_gdf = load_nigeria_shapefile()
            if master_gdf is None:
                return "Error: Nigeria shapefile not found. Cannot prepare for risk analysis."
            
            # Clean state name for matching
            clean_state = state_name.replace(' State', '').strip()
            state_gdf = master_gdf[master_gdf['StateName'] == clean_state].copy()
            
            if state_gdf.empty:
                # Try case-insensitive match
                state_gdf = master_gdf[master_gdf['StateName'].str.lower() == clean_state.lower()].copy()
            
            if state_gdf.empty:
                return f"No shapefile data found for {state_name}. Available states: {', '.join(master_gdf['StateName'].unique()[:10])}"
            
            logger.info(f"Found {len(state_gdf)} wards in shapefile for {state_name}")
            
            # Step 3: Match and merge TPR data with shapefile
            merged_gdf = match_and_merge_data(tpr_results, state_gdf)
            
            # Step 4: Extract environmental variables (zone-specific)
            logger.info("Extracting zone-specific environmental variables from rasters")
            env_data = extract_environmental_variables(merged_gdf, state_name)
            
            # Step 5: Prepare final dataset
            final_df = pd.DataFrame()
            
            # Add identifiers (CRITICAL for risk analysis)
            final_df['WardCode'] = merged_gdf['WardCode'] if 'WardCode' in merged_gdf.columns else range(len(merged_gdf))
            final_df['StateCode'] = merged_gdf['StateCode'] if 'StateCode' in merged_gdf.columns else state_name[:2].upper()
            final_df['LGACode'] = merged_gdf['LGACode'] if 'LGACode' in merged_gdf.columns else ''
            final_df['WardName'] = merged_gdf['WardName']
            final_df['LGA'] = merged_gdf['LGAName'] if 'LGAName' in merged_gdf.columns else merged_gdf.get('LGA', '')
            final_df['State'] = state_name
            final_df['GeopoliticalZone'] = get_geopolitical_zone(state_name)
            
            # Add TPR metrics
            final_df['TPR'] = merged_gdf['TPR'].fillna(0)
            # Fix: Convert to native Python int to avoid JSON serialization issues
            final_df['Total_Tested'] = merged_gdf['Total_Tested'].fillna(0).apply(lambda x: int(x) if pd.notna(x) else 0)
            final_df['Total_Positive'] = merged_gdf['Total_Positive'].fillna(0).apply(lambda x: int(x) if pd.notna(x) else 0)
            
            # Add environmental variables
            for col in env_data.columns:
                if col != 'WardCode':
                    final_df[col] = env_data[col]
            
            # Step 6: Save raw_data.csv
            raw_data_path = os.path.join(session_folder, 'raw_data.csv')
            final_df.to_csv(raw_data_path, index=False)
            logger.info(f"Saved raw_data.csv with {len(final_df)} wards")
            
            # Step 7: Create raw_shapefile.zip
            # Ensure shapefile has same columns as CSV
            for col in final_df.columns:
                if col not in merged_gdf.columns:
                    merged_gdf[col] = final_df[col]
            
            shapefile_path = create_shapefile_package(merged_gdf, session_folder)
            
            # Step 8: Set session flags for risk analysis
            flag_file = os.path.join(session_folder, '.risk_ready')
            Path(flag_file).touch()
            
            # Set flag to indicate TPR is waiting for user confirmation
            waiting_flag = os.path.join(session_folder, '.tpr_waiting_confirmation')
            Path(waiting_flag).touch()
            logger.info(f"Set TPR waiting confirmation flag for session {session_id}")
            
            # Update session if possible
            if graph_state:
                graph_state['data_loaded'] = True
                graph_state['csv_loaded'] = True
                graph_state['shapefile_loaded'] = True
                graph_state['upload_type'] = 'csv_shapefile'
                graph_state['tpr_completed'] = True
            
            result = f"""‚úÖ TPR Data Prepared for Risk Analysis!

State: {state_name}
Wards: {len(final_df)}
TPR Coverage: {(final_df['TPR'] > 0).sum()}/{len(final_df)} wards

Files Created:
1. raw_data.csv - Ward-level data with TPR and environmental variables
2. raw_shapefile.zip - Geographic boundaries with attributes

Data Summary:
- Columns: {len(final_df.columns)}
- Environmental variables: {sum(1 for col in env_data.columns if col != 'WardCode')}
- Mean TPR: {final_df['TPR'].mean():.2f}%
- Wards with data: {(final_df['Total_Tested'] > 0).sum()}

---
**Next Step:** I've finished preparing the TPR data for analysis. Would you like to proceed to the risk analysis stage to rank wards and plan for ITN distribution?
"""
            return result
            
        else:
            return f"Unknown action: {action}. Use 'analyze', 'calculate_tpr', or 'prepare_for_risk'"
            
    except Exception as e:
        logger.error(f"Error in TPR analysis: {e}", exc_info=True)
        return f"Error during TPR analysis: {str(e)}"