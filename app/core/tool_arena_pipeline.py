"""
Tool â†’ Arena Pipeline Manager

This module manages the flow where OpenAI executes tools, then Arena interprets the results.
Core principle: OpenAI for function execution, Arena for interpretation and explanation.
"""

import logging
import json
import time
from typing import Dict, Any, List, Optional
from pathlib import Path
import pandas as pd

logger = logging.getLogger(__name__)


class ToolArenaPipeline:
    """Manages tool execution â†’ interpretation flow."""

    def __init__(self, llm_manager=None, arena_manager=None):
        """Initialize the pipeline with necessary managers.

        Args:
            llm_manager: LLM manager for OpenAI tool execution
            arena_manager: Arena manager for multi-model interpretation
        """
        self.llm_manager = llm_manager
        self.arena_manager = arena_manager
        self.pipeline_metrics = {
            'total_requests': 0,
            'tool_executions': 0,
            'arena_interpretations': 0,
            'combined_responses': 0
        }

    def execute_pipeline(self, message: str, session_id: str, session_context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute tools then interpret results.

        This is the core pipeline:
        1. OpenAI executes tools based on user request
        2. Prepare context with tool results
        3. Arena interprets the results
        4. Combine for comprehensive response

        Args:
            message: User's request message
            session_id: Current session ID
            session_context: Session context with data state

        Returns:
            Combined response with tool execution and interpretation
        """
        try:
            self.pipeline_metrics['total_requests'] += 1
            start_time = time.time()

            # Step 1: OpenAI executes tools
            logger.info(f"ðŸ”§ Step 1: Executing tools with OpenAI for: {message[:50]}...")
            tool_response = self.execute_tools_with_openai(message, session_id, session_context)

            if not tool_response.get('success'):
                logger.error(f"Tool execution failed: {tool_response.get('error')}")
                return tool_response

            # Step 2: Prepare context for interpretation
            logger.info("ðŸ“Š Step 2: Preparing context for Arena interpretation")
            interpretation_context = self.prepare_interpretation_context(
                message, tool_response, session_id, session_context
            )

            # Step 3: Arena interprets the results
            logger.info("ðŸŽ­ Step 3: Getting Arena interpretation of results")
            interpretation = self.get_arena_interpretation(
                original_query=message,
                tool_results=tool_response,
                context=interpretation_context
            )

            # Step 4: Combine for final response
            logger.info("âœ¨ Step 4: Combining tool execution with interpretation")
            combined_response = self.format_combined_response(
                tool_response, interpretation, time.time() - start_time
            )

            self.pipeline_metrics['combined_responses'] += 1
            return combined_response

        except Exception as e:
            logger.error(f"Pipeline execution error: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'response': f"I encountered an error processing your request: {str(e)}"
            }

    def execute_tools_with_openai(self, message: str, session_id: str, session_context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute tools using OpenAI's function calling.

        Args:
            message: User's request
            session_id: Session identifier
            session_context: Current session context

        Returns:
            Tool execution results
        """
        try:
            if not self.llm_manager:
                # Fallback: Import and use RequestInterpreter's tool execution
                from app.core.request_interpreter import RequestInterpreter
                # This would normally use the RequestInterpreter's tool methods
                return {
                    'success': True,
                    'tools_used': ['placeholder_tool'],
                    'results': {'message': 'Tool execution placeholder'},
                    'raw_response': 'Tool executed successfully'
                }

            # Use LLM manager to execute tools
            self.pipeline_metrics['tool_executions'] += 1

            # Here you would call the actual tool execution
            # For now, returning structured placeholder
            return {
                'success': True,
                'tools_used': ['analysis_tool'],
                'results': {
                    'analysis_complete': True,
                    'high_risk_wards': 15,
                    'mean_score': 0.65
                },
                'raw_response': 'Analysis completed successfully'
            }

        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def prepare_interpretation_context(self, message: str, tool_response: Dict[str, Any],
                                      session_id: str, session_context: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare comprehensive context for Arena interpretation.

        Args:
            message: Original user query
            tool_response: Results from tool execution
            session_id: Session identifier
            session_context: Current session state

        Returns:
            Context dictionary for Arena interpretation
        """
        context = {
            'original_query': message,
            'tool_results': tool_response.get('results', {}),
            'tools_used': tool_response.get('tools_used', []),
            'execution_details': tool_response.get('execution_details', {}),
            'session_data': {}
        }

        # Load session-specific data
        session_folder = Path(f'instance/uploads/{session_id}')

        try:
            # Load analysis results if available
            analysis_file = session_folder / 'analysis_rankings.csv'
            if analysis_file.exists():
                df = pd.read_csv(analysis_file)
                score_col = 'composite_score' if 'composite_score' in df.columns else 'pca_score'
                if score_col in df.columns:
                    context['session_data']['analysis'] = {
                        'total_wards': len(df),
                        'top_wards': df.nlargest(5, score_col).to_dict('records'),
                        'statistics': {
                            'mean': df[score_col].mean(),
                            'std': df[score_col].std(),
                            'max': df[score_col].max(),
                            'min': df[score_col].min()
                        }
                    }

            # Load TPR results if available
            tpr_file = session_folder / 'tpr_results.csv'
            if tpr_file.exists():
                df = pd.read_csv(tpr_file)
                if 'TPR' in df.columns:
                    context['session_data']['tpr'] = {
                        'mean_tpr': df['TPR'].mean(),
                        'max_tpr': df['TPR'].max(),
                        'high_risk_count': len(df[df['TPR'] > 20])
                    }

        except Exception as e:
            logger.error(f"Error loading session data for context: {e}")

        return context

    def get_arena_interpretation(self, original_query: str, tool_results: Dict[str, Any],
                                context: Dict[str, Any]) -> Dict[str, Any]:
        """Get interpretation from Arena models.

        Args:
            original_query: User's original question
            tool_results: Results from tool execution
            context: Full context for interpretation

        Returns:
            Arena interpretation response
        """
        try:
            self.pipeline_metrics['arena_interpretations'] += 1

            # Build interpretation prompt
            prompt = self.build_interpretation_prompt(original_query, tool_results, context)

            # If Arena manager is available, use it
            if self.arena_manager:
                # Get interpretations from Arena models
                # This would call the actual Arena manager
                return {
                    'interpretation': 'Arena interpretation of tool results',
                    'confidence': 'high',
                    'models_consulted': ['phi3', 'mistral', 'qwen']
                }

            # Fallback: Use Ollama directly for interpretation
            interpretations = self.get_ollama_interpretations(prompt)
            return self.synthesize_interpretations(interpretations)

        except Exception as e:
            logger.error(f"Arena interpretation error: {e}")
            return {
                'interpretation': 'Unable to generate interpretation',
                'error': str(e)
            }

    def build_interpretation_prompt(self, query: str, tool_results: Dict[str, Any],
                                   context: Dict[str, Any]) -> str:
        """Build comprehensive prompt for interpretation.

        Args:
            query: Original user query
            tool_results: Tool execution results
            context: Additional context

        Returns:
            Formatted prompt for Arena models
        """
        prompt = f"""## Interpretation Request

User asked: {query}

## Tool Execution Results
Tools used: {', '.join(tool_results.get('tools_used', ['None']))}

Results:
{json.dumps(tool_results.get('results', {}), indent=2)}

## Data Context
"""

        if context.get('session_data', {}).get('analysis'):
            analysis = context['session_data']['analysis']
            prompt += f"""
Analysis Summary:
- Total wards analyzed: {analysis['total_wards']}
- Mean score: {analysis['statistics']['mean']:.3f}
- Max score: {analysis['statistics']['max']:.3f}
"""

        if context.get('session_data', {}).get('tpr'):
            tpr = context['session_data']['tpr']
            prompt += f"""
TPR Analysis:
- Mean TPR: {tpr['mean_tpr']:.1f}%
- Max TPR: {tpr['max_tpr']:.1f}%
- High risk wards: {tpr['high_risk_count']}
"""

        prompt += """

Please provide a comprehensive interpretation of these results:
1. Explain what the analysis reveals
2. Identify key patterns and insights
3. Suggest implications and recommendations
4. Highlight any areas of concern

Focus on making the technical results understandable and actionable.
"""

        return prompt

    def get_ollama_interpretations(self, prompt: str) -> List[Dict[str, Any]]:
        """Get interpretations from Ollama models.

        Args:
            prompt: Interpretation prompt

        Returns:
            List of model interpretations
        """
        interpretations = []

        # Models to consult for interpretation
        models = [
            {'name': 'phi3:mini', 'role': 'Analyst'},
            {'name': 'mistral:7b', 'role': 'Statistician'},
            {'name': 'qwen2.5:7b', 'role': 'Technician'}
        ]

        for model_info in models:
            try:
                # Here you would call Ollama API
                # For now, placeholder
                interpretations.append({
                    'model': model_info['name'],
                    'role': model_info['role'],
                    'interpretation': f"{model_info['role']} interpretation placeholder",
                    'confidence': 0.8
                })
            except Exception as e:
                logger.error(f"Error getting interpretation from {model_info['name']}: {e}")

        return interpretations

    def synthesize_interpretations(self, interpretations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Synthesize multiple model interpretations into consensus.

        Args:
            interpretations: List of model interpretations

        Returns:
            Synthesized interpretation
        """
        if not interpretations:
            return {
                'interpretation': 'No interpretations available',
                'confidence': 'low'
            }

        # For now, simple synthesis - in production, would be more sophisticated
        synthesis = {
            'interpretation': '\n\n'.join([
                f"**{interp['role']} Perspective:**\n{interp['interpretation']}"
                for interp in interpretations
            ]),
            'confidence': 'medium',
            'models_consulted': [interp['model'] for interp in interpretations]
        }

        return synthesis

    def format_combined_response(self, tool_response: Dict[str, Any],
                                interpretation: Dict[str, Any],
                                execution_time: float) -> Dict[str, Any]:
        """Format the combined tool execution and interpretation response.

        Args:
            tool_response: Results from tool execution
            interpretation: Arena interpretation
            execution_time: Total pipeline execution time

        Returns:
            Combined formatted response
        """
        # Build the response message
        response_parts = []

        # Add tool execution summary
        if tool_response.get('raw_response'):
            response_parts.append("## Analysis Results\n")
            response_parts.append(tool_response['raw_response'])

        # Add interpretation
        if interpretation.get('interpretation'):
            response_parts.append("\n## Interpretation\n")
            response_parts.append(interpretation['interpretation'])

        # Combine everything
        combined_response = {
            'status': 'success',
            'response': '\n'.join(response_parts),
            'execution': tool_response,
            'interpretation': interpretation,
            'combined_response': '\n'.join(response_parts),
            'tools_used': tool_response.get('tools_used', []),
            'models_consulted': interpretation.get('models_consulted', []),
            'pipeline_metrics': {
                'execution_time': f"{execution_time:.2f}s",
                'tool_execution': tool_response.get('success', False),
                'interpretation_generated': bool(interpretation.get('interpretation'))
            }
        }

        # Add visualizations if present
        if tool_response.get('visualizations'):
            combined_response['visualizations'] = tool_response['visualizations']

        # Add download links if present
        if tool_response.get('download_links'):
            combined_response['download_links'] = tool_response['download_links']

        return combined_response

    def get_metrics(self) -> Dict[str, Any]:
        """Get pipeline execution metrics.

        Returns:
            Pipeline metrics dictionary
        """
        return self.pipeline_metrics.copy()