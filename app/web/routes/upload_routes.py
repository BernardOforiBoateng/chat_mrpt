# app/web/routes/upload_routes.py
"""
Upload Routes module for file upload operations.

This module contains the file upload routes for the ChatMRPT web application:
- Modern dual file upload (CSV + shapefile)
- Legacy single file upload 
- Sample data loading
- File validation and processing logic
"""

import os
import uuid
import logging
import time
import traceback
from flask import Blueprint, session, request, current_app, jsonify
from werkzeug.utils import secure_filename

from ...core.decorators import handle_errors, log_execution_time, validate_session
from ...core.exceptions import ValidationError, DataProcessingError
from ...core.utils import convert_to_json_serializable
from ...core.responses import ResponseBuilder

# TPR Module imports
try:
    from ...tpr_module.integration.upload_detector import TPRUploadDetector
    from ...tpr_module.integration.tpr_handler import get_tpr_handler
    TPR_MODULE_AVAILABLE = True
except ImportError:
    logger.warning("TPR module not available")
    TPR_MODULE_AVAILABLE = False

logger = logging.getLogger(__name__)

# Create the upload routes blueprint
upload_bp = Blueprint('upload', __name__)

# File upload configurations
ALLOWED_EXTENSIONS_CSV = {'csv', 'txt', 'xlsx', 'xls'}  # Added Excel file extensions
ALLOWED_EXTENSIONS_SHP = {'zip'}  # Shapefiles are uploaded as ZIP files


def allowed_file(filename, allowed_extensions):
    """Check if file extension is allowed."""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in allowed_extensions


class UploadTypeDetector:
    """
    Level 2: Upload Type Detection (Following Diagram Structure)
    Detects upload type according to diagram: CSV+Shapefile vs CSV-Only
    Now includes TPR file detection for NMEP Excel files
    """
    
    def __init__(self):
        # Initialize TPR detector if available
        self.tpr_detector = TPRUploadDetector() if TPR_MODULE_AVAILABLE else None
        self._tpr_detection_info = None
    
    def detect_upload_type(self, files: dict, csv_content=None) -> str:
        """
        Detect upload type according to diagram flow
        
        Returns:
        - 'tpr_excel': NMEP TPR Excel file
        - 'tpr_shapefile': TPR Excel + Shapefile
        - 'csv_shapefile': Full Dataset Path (CSV + Shapefile)
        - 'csv_only': Regular CSV without shapefile
        - 'invalid': Invalid upload combination
        """
        csv_file = files.get('csv_file')
        shapefile = files.get('shapefile')
        
        has_csv = csv_file and csv_file.filename != ''
        has_shapefile = shapefile and shapefile.filename != ''
        
        # Check for TPR files first if module is available
        if TPR_MODULE_AVAILABLE and self.tpr_detector and has_csv:
            tpr_type, tpr_info = self.tpr_detector.detect_tpr_upload(
                csv_file, shapefile, csv_content
            )
            
            if self.tpr_detector.should_use_tpr_workflow(tpr_type):
                logger.info(f"TPR workflow detected: {tpr_type}")
                self._tpr_detection_info = tpr_info
                return tpr_type
        
        # Standard detection logic
        if has_csv and has_shapefile:
            return 'csv_shapefile'  # Full Dataset Path
        elif has_csv and not has_shapefile:
            return 'csv_only'  # Regular CSV without shapefile
        else:
            return 'invalid'
    
    
    def get_upload_summary(self, upload_type: str, file_info: dict) -> dict:
        """Generate upload type summary for user"""
        summaries = {
            'csv_shapefile': {
                'path': 'Full Dataset Path',
                'description': 'CSV data + Shapefile boundaries',
                'next_step': 'Store raw data and generate summary'
            },
            'csv_only': {
                'path': 'CSV-Only Path',
                'description': 'CSV data without geographic boundaries',
                'next_step': 'Data analysis without mapping capabilities'
            },
            'tpr_excel': {
                'path': 'TPR Analysis Path',
                'description': 'NMEP TPR Excel file for Test Positivity Rate calculation',
                'next_step': 'Select state and configure TPR analysis parameters'
            },
            'tpr_shapefile': {
                'path': 'TPR + Shapefile Path', 
                'description': 'NMEP TPR Excel + Custom shapefile boundaries',
                'next_step': 'Select state for TPR analysis with custom boundaries'
            },
            'invalid': {
                'path': 'Invalid Upload',
                'description': 'No valid files detected',
                'next_step': 'Please upload valid CSV and/or shapefile'
            }
        }
        
        result = {
            'upload_type': upload_type,
            'file_info': file_info,
            **summaries.get(upload_type, summaries['invalid'])
        }
        
        # Add TPR-specific info if available
        if hasattr(self, '_tpr_detection_info') and self._tpr_detection_info and upload_type.startswith('tpr'):
            result['tpr_info'] = self.tpr_detector.get_tpr_upload_summary(
                upload_type, self._tpr_detection_info
            ).get('tpr_info', {})
        
        return result




@upload_bp.route('/upload_both_files', methods=['POST'])
@handle_errors
@validate_session
@log_execution_time
def upload_both_files():
    """
    Enhanced upload handler following diagram structure:
    Level 1: User Uploads Data ‚Üí Level 2: Upload Type Detection ‚Üí Level 3A/3B: Path Selection
    
    Supports: Full Dataset Path (CSV+Shapefile) and CSV-Only Path
    """
    session_id = session.get('session_id')
    
    
    # Level 1: User Uploads Data - Extract files from request
    csv_file = request.files.get('csv_file')
    shapefile = request.files.get('shapefile')
    
    # Basic file validation
    if csv_file and csv_file.filename == '':
        csv_file = None
    if shapefile and shapefile.filename == '':
        shapefile = None
        
    if not csv_file and not shapefile:
        raise ValidationError("No files selected for upload")
    
    # Level 2: Upload Type Detection (Following Diagram)
    detector = UploadTypeDetector()
    
    # Read CSV content if needed
    csv_content = None
    if csv_file:
        csv_content = csv_file.read()
        csv_file.seek(0)  # Reset file pointer for later use
    
    upload_type = detector.detect_upload_type(
        {'csv_file': csv_file, 'shapefile': shapefile}, 
        csv_content
    )
    
    file_info = {
        'csv_filename': csv_file.filename if csv_file else None,
        'shapefile_filename': shapefile.filename if shapefile else None,
        'session_id': session_id
    }
    
    upload_summary = detector.get_upload_summary(upload_type, file_info)
    
    logger.info(f"üîç Upload Type Detected: {upload_type} for session {session_id}")
    logger.info(f"üìä Upload Summary: {upload_summary['description']}")
    
    # Route to appropriate path based on detection
    if upload_type == 'csv_shapefile':
        return handle_full_dataset_path(session_id, csv_file, shapefile, upload_summary)
    elif upload_type == 'csv_only':
        return handle_csv_only_path(session_id, csv_file, upload_summary)
    elif upload_type in ['tpr_excel', 'tpr_shapefile'] and TPR_MODULE_AVAILABLE:
        return handle_tpr_path(session_id, csv_file, shapefile, upload_type, detector._tpr_detection_info, upload_summary)
    else:
        raise ValidationError(f"Invalid upload combination: {upload_summary['description']}")


def handle_full_dataset_path(session_id: str, csv_file, shapefile, upload_summary: dict):
    """
    Level 3A: Full Dataset Path (CSV + Shapefile)
    Level 4A: Store Raw Data (raw_data.csv, shapefile.zip, NO cleaning yet)
    Level 5A: Generate Dynamic Summary
    """
    upload_start_time = time.time()
    
    # Create session folder for file storage
    session_folder = os.path.join(current_app.config['UPLOAD_FOLDER'], session_id)
    os.makedirs(session_folder, exist_ok=True)
    
    logger.info(f"üìÅ Starting Full Dataset Path for session {session_id}")
    
    # üéØ LOG FILE UPLOAD START - CRITICAL FOR DEMO ANALYTICS
    if hasattr(current_app, 'services') and current_app.services.interaction_logger:
        interaction_logger = current_app.services.interaction_logger
        
        # Log CSV file upload
        if csv_file:
            csv_size = csv_file.content_length if csv_file.content_length else 0
            # If content_length not available, read and reset
            if csv_size == 0:
                csv_content = csv_file.read()
                csv_size = len(csv_content)
                csv_file.seek(0)  # Reset file pointer
            
            interaction_logger.log_file_upload(
                session_id=session_id,
                file_type='csv',
                file_name=csv_file.filename,
                file_size=csv_size,
                metadata={
                    'upload_type': 'csv_shapefile',
                    'upload_path': 'full_dataset',
                    'storage_path': 'raw_data.csv',
                    'start_timestamp': upload_start_time
                }
            )
        
        # Log shapefile upload
        if shapefile:
            shapefile_size = shapefile.content_length if shapefile.content_length else 0
            if shapefile_size == 0:
                shapefile_content = shapefile.read()
                shapefile_size = len(shapefile_content)
                shapefile.seek(0)  # Reset file pointer
            
            interaction_logger.log_file_upload(
                session_id=session_id,
                file_type='shapefile',
                file_name=shapefile.filename,
                file_size=shapefile_size,
                metadata={
                    'upload_type': 'csv_shapefile',
                    'upload_path': 'full_dataset',
                    'storage_path': 'raw_shapefile.zip',
                    'start_timestamp': upload_start_time
                }
            )
        
        # Log upload milestone
        interaction_logger.log_analysis_event(
            session_id=session_id,
            event_type='user_journey',
            details={
                'milestone': 'file_upload_started',
                'upload_type': 'csv_shapefile',
                'files_count': 2,
                'total_size_bytes': (csv_size if csv_file else 0) + (shapefile_size if shapefile else 0)
            },
            success=True
        )
    
    # Level 4A: Store Raw Data (Following Diagram - NO cleaning yet)
    raw_storage_result = store_raw_data_files(session_folder, csv_file, shapefile)
    
    if raw_storage_result['status'] != 'success':
        raise DataProcessingError(f"Failed to store raw data: {raw_storage_result['message']}")
    
    # Level 5A: Generate Dynamic Summary (Following Diagram)
    summary_result = generate_dynamic_data_summary(session_id, session_folder, 'csv_shapefile')
    
    # üéØ LOG UPLOAD COMPLETION - CRITICAL FOR DEMO ANALYTICS
    upload_duration = time.time() - upload_start_time
    if hasattr(current_app, 'services') and current_app.services.interaction_logger:
        interaction_logger = current_app.services.interaction_logger
        
        # Log upload completion
        interaction_logger.log_analysis_event(
            session_id=session_id,
            event_type='file_upload_completed',
            details={
                'upload_type': 'csv_shapefile',
                'files_stored': raw_storage_result.get('files_stored', 0),
                'upload_duration_seconds': upload_duration,
                'storage_success': raw_storage_result['status'] == 'success',
                'summary_generated': summary_result and 'status' not in summary_result,
                'next_milestone': 'analysis_permission'
            },
            success=True
        )
        
        # Log user journey milestone
        interaction_logger.log_analysis_event(
            session_id=session_id,
            event_type='user_journey',
            details={
                'milestone': 'file_upload_completed',
                'time_to_complete': upload_duration,
                'workflow_stage': 'data_preparation',
                'success_indicators': ['files_stored', 'summary_generated']
            },
            success=True
        )
    
    # Set session flags for next steps
    session['upload_type'] = 'csv_shapefile'
    session['raw_data_stored'] = True
    # session['should_describe_data'] = True  # DISABLED - Frontend handles this now
    session['should_ask_analysis_permission'] = True  # Trigger permission system
    
    # CRITICAL: Set data loaded flags so request interpreter recognizes the data
    session['csv_loaded'] = True
    session['shapefile_loaded'] = True
    session['data_loaded'] = True
    
    # ALSO update SessionStateManager for streaming endpoint
    try:
        from app.core.session_state import SessionStateManager
        state_manager = SessionStateManager()
        state_manager.update_state(session_id, {
            'should_ask_analysis_permission': True,
            'data_loaded': True
        })
    except Exception as e:
        logger.debug(f"SessionStateManager update failed: {e}")
    
    # Return comprehensive result
    return jsonify({
        'status': 'success',
        'upload_type': 'csv_shapefile',
        'upload_summary': upload_summary,
        'raw_storage': raw_storage_result,
        'data_summary': summary_result,
        'message': f"Full dataset uploaded successfully. {raw_storage_result['files_stored']} files stored as raw data.",
        'next_step': 'Data summary will be presented for your review and permission.',
        'upload_duration': f"{upload_duration:.2f}s"
    })


def clear_analysis_results(session_folder: str):
    """
    Clear old analysis results from session folder when new files are uploaded.
    This prevents data contamination from previous analyses.
    """
    import glob
    
    # Define patterns for analysis result files
    analysis_patterns = [
        'analysis_*.csv',
        'unified_dataset.*',
        'composite_scores*.csv',
        'vulnerability_rankings*.csv',
        'normalized_data.csv',
        'cleaned_data.csv',
        '*_map_*.html',
        '*_chart_*.html',
        'itn_distribution_*.csv',
        'model_*.csv',
        '*.geoparquet'
    ]
    
    files_removed = 0
    for pattern in analysis_patterns:
        files = glob.glob(os.path.join(session_folder, pattern))
        for file_path in files:
            try:
                os.remove(file_path)
                files_removed += 1
                logger.debug(f"Removed old analysis file: {os.path.basename(file_path)}")
            except Exception as e:
                logger.warning(f"Could not remove {file_path}: {e}")
    
    if files_removed > 0:
        logger.info(f"üßπ Cleared {files_removed} old analysis files from session folder")
    
    return files_removed


def store_raw_data_files(session_folder: str, csv_file, shapefile):
    """
    Level 4A: Store Raw Data (Following Diagram)
    Store files exactly as uploaded - NO cleaning, NO processing, NO modifications
    """
    try:
        stored_files = []
        
        # Clear old analysis results before storing new files
        clear_analysis_results(session_folder)
        
        # Store CSV as raw_data.csv (preserving original data)
        if csv_file and allowed_file(csv_file.filename, ALLOWED_EXTENSIONS_CSV):
            raw_csv_path = os.path.join(session_folder, 'raw_data.csv')
            csv_file.save(raw_csv_path)
            stored_files.append('raw_data.csv')
            logger.info(f"‚úÖ Stored raw CSV: {csv_file.filename} ‚Üí raw_data.csv")
        
        # Store shapefile as raw_shapefile.zip (preserving original)
        if shapefile and allowed_file(shapefile.filename, ALLOWED_EXTENSIONS_SHP):
            raw_shp_path = os.path.join(session_folder, 'raw_shapefile.zip')
            shapefile.save(raw_shp_path)
            stored_files.append('raw_shapefile.zip')
            logger.info(f"‚úÖ Stored raw shapefile: {shapefile.filename} ‚Üí raw_shapefile.zip")
        
        return {
            'status': 'success',
            'message': 'Raw data files stored successfully',
            'files_stored': len(stored_files),
            'stored_files': stored_files,
            'preservation_note': 'Original files preserved without any modifications'
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error storing raw data files: {e}")
        return {
            'status': 'error',
            'message': f'Failed to store raw data: {str(e)}'
        }


def generate_dynamic_data_summary(session_id: str, session_folder: str, upload_type: str):
    """
    Level 5A: Generate Dynamic Summary (Following Diagram)
    Row/column count, Data preview, Column type detection, Data completeness
    """
    try:
        import pandas as pd
        
        summary = {
            'session_id': session_id,
            'upload_type': upload_type,
            'analysis_timestamp': pd.Timestamp.now().isoformat()
        }
        
        # Analyze raw CSV data
        raw_csv_path = os.path.join(session_folder, 'raw_data.csv')
        if os.path.exists(raw_csv_path):
            # Try to read as CSV first, then as Excel if that fails
            try:
                raw_data = pd.read_csv(raw_csv_path)
            except UnicodeDecodeError:
                # If CSV reading fails, try reading as Excel (since we save .xlsx as .csv)
                try:
                    raw_data = pd.read_excel(raw_csv_path)
                except Exception as e:
                    logger.error(f"Failed to read both CSV and Excel format: {e}")
                    raise
            
            summary.update({
                'total_rows': len(raw_data),
                'total_columns': len(raw_data.columns),
                'column_names': raw_data.columns.tolist(),
                'preview_rows': raw_data.head(5).fillna('N/A').to_dict('records'),
                'column_types': detect_column_types(raw_data),
                'data_completeness': calculate_data_completeness(raw_data),
                'data_quality_assessment': assess_data_quality(raw_data)
            })
        
        # Analyze shapefile if present
        raw_shp_path = os.path.join(session_folder, 'raw_shapefile.zip')
        if os.path.exists(raw_shp_path):
            # Basic shapefile info (without processing)
            summary['shapefile_info'] = {
                'filename': 'raw_shapefile.zip',
                'size_mb': round(os.path.getsize(raw_shp_path) / (1024*1024), 2),
                'status': 'stored'
            }
        
        logger.info(f"üìä Generated dynamic summary for session {session_id}: {summary['total_rows']} rows, {summary['total_columns']} columns")
        
        return summary
        
    except Exception as e:
        logger.error(f"‚ùå Error generating dynamic summary: {e}")
        return {
            'status': 'error',
            'message': f'Failed to generate data summary: {str(e)}'
        }


def detect_column_types(df):
    """Detect and categorize column types for summary"""
    types = {}
    for col in df.columns:
        if df[col].dtype in ['int64', 'float64']:
            types[col] = 'numeric'
        elif df[col].dtype == 'object':
            # Check if it's categorical vs text
            unique_ratio = df[col].nunique() / len(df)
            if unique_ratio < 0.1:  # Less than 10% unique values
                types[col] = 'categorical'
            else:
                types[col] = 'text'
        else:
            types[col] = 'other'
    return types


def calculate_data_completeness(df):
    """Calculate completeness percentage for each column"""
    completeness = {}
    for col in df.columns:
        total_values = len(df)
        non_null_values = df[col].count()
        completeness[col] = round((non_null_values / total_values) * 100, 2)
    
    overall_completeness = round(df.count().sum() / (len(df) * len(df.columns)) * 100, 2)
    
    return {
        'by_column': completeness,
        'overall': overall_completeness
    }


def assess_data_quality(df):
    """Basic data quality assessment"""
    issues = []
    
    # Check for completely empty columns
    empty_cols = df.columns[df.isnull().all()].tolist()
    if empty_cols:
        issues.append(f"Empty columns detected: {empty_cols}")
    
    # Check for duplicate rows
    duplicate_count = df.duplicated().sum()
    if duplicate_count > 0:
        issues.append(f"Duplicate rows detected: {duplicate_count}")
    
    # Check for missing ward names (common issue)
    if 'ward_name' in df.columns or 'WardName' in df.columns:
        ward_col = 'ward_name' if 'ward_name' in df.columns else 'WardName'
        missing_wards = df[ward_col].isnull().sum()
        if missing_wards > 0:
            issues.append(f"Missing ward names: {missing_wards}")
    
    return {
        'issues_found': len(issues),
        'issues': issues,
        'quality_score': max(0, 100 - (len(issues) * 20))  # Simple scoring
    }




def handle_csv_only_path(session_id: str, csv_file, upload_summary: dict):
    """
    CSV-Only Path (Basic CSV without shapefile)
    Store raw CSV and generate summary
    """
    session_folder = os.path.join(current_app.config['UPLOAD_FOLDER'], session_id)
    os.makedirs(session_folder, exist_ok=True)
    
    logger.info(f"üìä CSV-Only path for session {session_id}")
    
    # Store raw CSV
    raw_storage_result = store_raw_csv_only(session_folder, csv_file)
    
    if raw_storage_result['status'] != 'success':
        raise DataProcessingError(f"Failed to store raw CSV: {raw_storage_result['message']}")
    
    # Generate summary
    summary_result = generate_dynamic_data_summary(session_id, session_folder, 'csv_only')
    
    # Set session flags
    session['upload_type'] = 'csv_only'
    session['raw_data_stored'] = True
    # session['should_describe_data'] = True  # DISABLED - Frontend handles this now
    session['should_ask_analysis_permission'] = True
    
    # CRITICAL: Set data loaded flags so request interpreter recognizes the data
    session['csv_loaded'] = True
    session['shapefile_loaded'] = False  # No shapefile in CSV-only path
    session['data_loaded'] = True
    
    return jsonify({
        'status': 'success',
        'upload_type': 'csv_only',
        'upload_summary': upload_summary,
        'raw_storage': raw_storage_result,
        'data_summary': summary_result,
        'message': 'CSV uploaded successfully. Note: No mapping capabilities without shapefile.',
        'next_step': 'Data summary will be presented for analysis without geographic visualization.'
    })


def store_raw_csv_only(session_folder: str, csv_file):
    """Store raw CSV file only"""
    try:
        # Clear old analysis results before storing new file
        clear_analysis_results(session_folder)
        
        if csv_file and allowed_file(csv_file.filename, ALLOWED_EXTENSIONS_CSV):
            raw_csv_path = os.path.join(session_folder, 'raw_data.csv')
            csv_file.save(raw_csv_path)
            logger.info(f"‚úÖ Stored raw CSV: {csv_file.filename} ‚Üí raw_data.csv")
            
            return {
                'status': 'success',
                'message': 'Raw CSV file stored successfully',
                'files_stored': 1,
                'stored_files': ['raw_data.csv']
            }
        else:
            return {
                'status': 'error',
                'message': 'Invalid CSV file'
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error storing raw CSV: {e}")
        return {
            'status': 'error',
            'message': f'Failed to store raw CSV: {str(e)}'
        }



def handle_tpr_path(session_id: str, csv_file, shapefile, upload_type: str, detection_info: dict, upload_summary: dict):
    """
    Handle TPR file uploads (NMEP Excel files).
    Routes to TPR-specific workflow with conversational interface.
    """
    upload_start_time = time.time()
    
    # Create session folder
    session_folder = os.path.join(current_app.config['UPLOAD_FOLDER'], session_id)
    os.makedirs(session_folder, exist_ok=True)
    
    logger.info(f"üìä Starting TPR Path for session {session_id}, type: {upload_type}")
    
    # Store raw files
    try:
        # Save Excel file
        excel_path = os.path.join(session_folder, 'tpr_data.xlsx')
        csv_file.save(excel_path)
        
        # Save shapefile if provided
        shapefile_path = None
        if shapefile and upload_type == 'tpr_shapefile':
            shapefile_path = os.path.join(session_folder, 'custom_shapefile.zip')
            shapefile.save(shapefile_path)
        
        # Log upload
        if hasattr(current_app, 'services') and current_app.services.interaction_logger:
            current_app.services.interaction_logger.log_file_upload(
                session_id=session_id,
                file_type='tpr_excel',
                file_name=csv_file.filename,
                file_size=os.path.getsize(excel_path),
                metadata={
                    'upload_type': upload_type,
                    'has_shapefile': shapefile_path is not None,
                    'tpr_year': detection_info.get('metadata', {}).get('year'),
                    'tpr_month': detection_info.get('metadata', {}).get('month')
                }
            )
    
    except Exception as e:
        logger.error(f"Error storing TPR files: {e}")
        raise DataProcessingError(f"Failed to store TPR files: {str(e)}")
    
    # Initialize TPR handler
    tpr_handler = get_tpr_handler(session_id)
    
    # Process TPR upload
    result = tpr_handler.handle_tpr_upload(
        excel_path,
        shapefile_path,
        upload_type,
        detection_info
    )
    
    if result['status'] != 'success':
        raise DataProcessingError(result.get('message', 'TPR processing failed'))
    
    # Set session flags for TPR workflow
    session['upload_type'] = upload_type
    session['tpr_workflow_active'] = True
    session['tpr_session_id'] = session_id
    session['data_loaded'] = True  # Mark data as loaded
    session['csv_loaded'] = True
    session['shapefile_loaded'] = shapefile_path is not None
    
    # Don't trigger standard analysis permission - TPR has its own flow
    session['should_ask_analysis_permission'] = False
    
    # Log completion
    upload_duration = time.time() - upload_start_time
    if hasattr(current_app, 'services') and current_app.services.interaction_logger:
        current_app.services.interaction_logger.log_analysis_event(
            session_id=session_id,
            event_type='tpr_upload_completed',
            details={
                'upload_type': upload_type,
                'upload_duration': upload_duration,
                'states_available': len(result.get('available_states', [])),
                'workflow_stage': result.get('stage', 'unknown')
            },
            success=True
        )
    
    # Return TPR-specific response
    return jsonify({
        'status': 'success',
        'upload_type': upload_type,
        'workflow': 'tpr',
        'upload_summary': upload_summary,
        'tpr_response': result.get('response', ''),
        'available_states': result.get('available_states', []),
        'metadata': result.get('metadata', {}),
        'next_step': result.get('next_step', 'Please select a state to analyze'),
        'upload_duration': f"{upload_duration:.2f}s",
        'message': 'TPR file uploaded successfully. Starting TPR analysis workflow.'
    })


# ================================================================
# PHASE 1 IMPLEMENTATION COMPLETE: Upload Type Detection & Raw Storage
# ================================================================
# ‚úÖ Level 2: Upload Type Detection (csv_shapefile, csv_only)
# ‚úÖ Level 3A: Full Dataset Path Handler
# ‚úÖ Level 4A: Raw Data Storage (NO cleaning)
# ‚úÖ Level 5A: Dynamic Summary Generation
# 
# Next Phase: Level 6-7 Summary Presentation & Permission System
# ================================================================



# === LEGACY ROUTES (kept for backward compatibility) ===

@upload_bp.route('/upload', methods=['POST'])
@handle_errors
@validate_session
@log_execution_time
def upload():
    """
    Legacy upload route for backwards compatibility.
    Redirects to upload_both_files function.
    """
    return upload_both_files()


@upload_bp.route('/load_sample_data', methods=['POST'])
@validate_session
@handle_errors
@log_execution_time
def load_sample_data():
    """Load sample data for demonstration purposes."""
    try:
        session_id = session.get('session_id')
        logger.info(f"Loading sample data for session {session_id}")
        
        return jsonify({
            'status': 'success',
            'message': 'Sample data loading will be implemented in Phase 2'
        })
    
    except Exception as e:
        logger.error(f"Error loading sample data: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Failed to load sample data: {str(e)}'
        })


def validate_csv_file(file_obj):
    """
    Validate CSV file structure and content.
    """
    try:
        import pandas as pd
        import io
        
        # Read file content
        content = file_obj.read().decode('utf-8')
        file_obj.seek(0)  # Reset file pointer
        
        # Try to parse as CSV
        csv_io = io.StringIO(content)
        df = pd.read_csv(csv_io, nrows=5)  # Read first 5 rows for validation
        
        return {
            'status': 'success',
            'message': 'CSV file is valid',
            'rows_sample': len(df),
            'columns': len(df.columns)
        }
    
    except Exception as e:
        return {
            'status': 'error',
            'message': f'Invalid CSV file: {str(e)}'
        }


def validate_shapefile(file_obj):
    """
    Validate shapefile ZIP structure.
    """
    try:
        import zipfile
        
        # Check if it's a valid ZIP file
        if not zipfile.is_zipfile(file_obj):
            return {
                'status': 'error',
                'message': 'File is not a valid ZIP archive'
            }
        
        return {
            'status': 'success',
            'message': 'Shapefile ZIP is valid'
        }
    
    except Exception as e:
        return {
            'status': 'error',
            'message': f'Invalid shapefile: {str(e)}'
        }


def create_session_upload_folder(session_id):
    """Create upload folder for session if it doesn't exist."""
    try:
        session_folder = os.path.join(current_app.config['UPLOAD_FOLDER'], session_id)
        os.makedirs(session_folder, exist_ok=True)
        return session_folder
    except Exception as e:
        logger.error(f"Error creating session folder: {e}")
        raise


def cleanup_session_files(session_id, file_types=None):
    """
    Clean up uploaded files for a session.
    
    Args:
        session_id: Session identifier
        file_types: List of file types to clean up (optional)
    
    Returns:
        dict: Cleanup result with status and message
    """
    try:
        session_folder = os.path.join(current_app.config['UPLOAD_FOLDER'], session_id)
        
        if not os.path.exists(session_folder):
            return {'status': 'success', 'message': 'No files to clean up'}
        
        files_removed = 0
        
        if file_types:
            # Remove specific file types
            for filename in os.listdir(session_folder):
                file_ext = filename.rsplit('.', 1)[-1].lower() if '.' in filename else ''
                if file_ext in file_types:
                    file_path = os.path.join(session_folder, filename)
                    os.remove(file_path)
                    files_removed += 1
                    logger.info(f"Removed file: {filename}")
        else:
            # Remove all files in session folder
            for filename in os.listdir(session_folder):
                file_path = os.path.join(session_folder, filename)
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    files_removed += 1
                    logger.info(f"Removed file: {filename}")
        
        # Remove empty directory
        if not os.listdir(session_folder):
            os.rmdir(session_folder)
            logger.info(f"Removed empty session folder: {session_folder}")
        
        return {'status': 'success', 'message': f'Cleaned up {files_removed} files'}
    
    except Exception as e:
        logger.error(f"Error cleaning up files for session {session_id}: {e}")
        return {'status': 'error', 'message': f'File cleanup failed: {str(e)}'}


@upload_bp.route('/api/download/processed-csv', methods=['GET'])
@validate_session
@handle_errors
def download_processed_csv():
    """Download processed CSV data from standard upload"""
    session_id = session.get('session_id')
    session_folder = os.path.join(current_app.config['UPLOAD_FOLDER'], session_id)
    
    # Look for raw_data.csv (convergence output)
    csv_path = os.path.join(session_folder, 'raw_data.csv')
    
    if not os.path.exists(csv_path):
        return jsonify({
            'status': 'error',
            'message': 'No processed CSV data available. Please upload and process data first.'
        }), 404
    
    try:
        from flask import send_file
        return send_file(
            csv_path,
            as_attachment=True,
            download_name=f'processed_data_{session_id}.csv',
            mimetype='text/csv'
        )
    except Exception as e:
        logger.error(f"Error downloading CSV: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Failed to download CSV: {str(e)}'
        }), 500


@upload_bp.route('/api/download/processed-shapefile', methods=['GET'])
@validate_session  
@handle_errors
def download_processed_shapefile():
    """Download processed shapefile data from standard upload"""
    session_id = session.get('session_id')
    session_folder = os.path.join(current_app.config['UPLOAD_FOLDER'], session_id)
    
    # Look for raw_shapefile.zip (convergence output)
    shapefile_path = os.path.join(session_folder, 'raw_shapefile.zip')
    
    if not os.path.exists(shapefile_path):
        return jsonify({
            'status': 'error',
            'message': 'No processed shapefile data available. Please upload and process data first.'
        }), 404
    
    try:
        from flask import send_file
        return send_file(
            shapefile_path,
            as_attachment=True,
            download_name=f'processed_shapefile_{session_id}.zip',
            mimetype='application/zip'
        )
    except Exception as e:
        logger.error(f"Error downloading shapefile: {e}")
        return jsonify({
            'status': 'error',
            'message': f'Failed to download shapefile: {str(e)}'
        }), 500


