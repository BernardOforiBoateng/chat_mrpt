"""
Complete Analysis Tools 

This module provides the coordinated dual-method analysis workflow without any
settlement integration, as per the updated post-permission workflow overhaul.
"""

import logging
import os
import time
import traceback
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field
from flask import current_app

from .base import BaseTool, ToolCategory, ToolExecutionResult, DataAnalysisTool

logger = logging.getLogger(__name__)


class RunCompleteAnalysisInput(BaseModel):
    """Input for complete analysis workflow with custom variable selection support"""
    session_id: str = Field(..., description="Session identifier for data access")
    composite_variables: Optional[List[str]] = Field(
        None, 
        description="Custom variables for composite analysis. If None, uses region-aware auto-selection"
    )
    pca_variables: Optional[List[str]] = Field(
        None, 
        description="Custom variables for PCA analysis. If None, uses region-aware auto-selection"
    )
    create_unified_dataset: bool = Field(True, description="Whether to create/update unified dataset")
    validate_variables: bool = Field(True, description="Whether to validate custom variables against available data")


class RunCompleteAnalysis(DataAnalysisTool):
    """
    Run complete dual-method malaria risk analysis (Composite + PCA) without settlement integration logic.
    
    This tool implements the post-permission workflow overhaul:
    1. Start with raw data (CSV + shapefile)
    2. Apply deferred cleaning (spatial neighbor mean imputation)
    3. Use region-aware variable selection
    4. Run both composite and PCA analyses with the SAME variables
    5. Create unified dataset preserving original data structure
    
    Settlement integration logic is completely excluded from this workflow.
    Any existing settlement columns in the original data are preserved.
    """
    
    name: str = "run_complete_analysis"
    description: str = "Run complete dual-method malaria risk analysis (Composite Score + PCA) with support for custom variable selection. Allows different variables for each method or auto-selection based on region."
    composite_variables: Optional[List[str]] = Field(
        None, 
        description="Custom variables for composite analysis. If None, uses region-aware auto-selection"
    )
    pca_variables: Optional[List[str]] = Field(
        None, 
        description="Custom variables for PCA analysis. If None, uses region-aware auto-selection"
    )
    create_unified_dataset: bool = Field(True, description="Whether to create/update unified dataset")
    validate_variables: bool = Field(True, description="Whether to validate custom variables against available data")
    
    def execute(self, session_id: str, **kwargs) -> ToolExecutionResult:
        """Execute complete dual-method analysis workflow"""
        kwargs['session_id'] = session_id
        return self._execute(**kwargs)
    
    def _execute(self, **kwargs) -> ToolExecutionResult:
        """Execute complete dual-method analysis workflow with custom variable support"""
        session_id = kwargs.get('session_id')
        tool_start_time = time.time()
        
        # üéØ LOG TOOL EXECUTION START - CRITICAL FOR DEMO ANALYTICS
        interaction_logger = None
        if hasattr(current_app, 'services') and current_app.services.interaction_logger:
            interaction_logger = current_app.services.interaction_logger
            
            # Log tool execution start with comprehensive parameters
            interaction_logger.log_analysis_event(
                session_id=session_id,
                event_type='tool_execution_start',
                details={
                    'tool_name': self.name,
                    'tool_description': self.description,
                    'parameters': {
                        'composite_variables': kwargs.get('composite_variables'),
                        'pca_variables': kwargs.get('pca_variables'),
                        'create_unified_dataset': kwargs.get('create_unified_dataset', True),
                        'validate_variables': kwargs.get('validate_variables', True)
                    },
                    'start_timestamp': tool_start_time,
                    'execution_method': 'parallel_dual_method'
                },
                success=True
            )
        
        try:
            composite_variables = kwargs.get('composite_variables') or getattr(self, 'composite_variables', None)
            pca_variables = kwargs.get('pca_variables') or getattr(self, 'pca_variables', None)
            create_unified_dataset = kwargs.get('create_unified_dataset', getattr(self, 'create_unified_dataset', True))
            validate_variables = kwargs.get('validate_variables', getattr(self, 'validate_variables', True))
            
            logger.info(f"Starting complete dual-method analysis for session {session_id}")
            logger.info(f"Custom variables - Composite: {composite_variables}, PCA: {pca_variables}")
            
            # Variable validation and fallback logic
            validation_message = ""
            final_composite_vars = composite_variables
            final_pca_vars = pca_variables
            
            # For custom analysis, use variables directly without validation fallbacks
            if composite_variables or pca_variables:
                final_composite_vars = composite_variables
                final_pca_vars = pca_variables
                validation_message = f"‚úÖ Using custom variables - Composite: {len(final_composite_vars or [])}, PCA: {len(final_pca_vars or [])}"
                logger.info(f"Custom variable selection: Composite: {final_composite_vars}, PCA: {final_pca_vars}")
            else:
                # Auto-selection when no custom variables specified
                final_composite_vars = None
                final_pca_vars = None
                validation_message = ""
            
            # üéØ LOG VARIABLE VALIDATION - DEMO ANALYTICS
            if interaction_logger:
                interaction_logger.log_analysis_event(
                    session_id=session_id,
                    event_type='variable_validation',
                    details={
                        'original_composite_variables': composite_variables,
                        'original_pca_variables': pca_variables,
                        'validation_enabled': validate_variables,
                        'validation_message': validation_message
                    },
                    success=True
                )
            
            # Run both analyses sequentially for better reliability
            analysis_start_time = time.time()
            
            # Run composite analysis first
            logger.info("üöÄ Starting composite analysis...")
            composite_result = self._run_composite_analysis(session_id, final_composite_vars)
            if composite_result['success']:
                logger.info("‚úÖ Composite analysis completed successfully")
            else:
                logger.error(f"‚ùå Composite analysis failed: {composite_result['message']}")
            
            # Run PCA analysis second
            logger.info("üî¨ Starting PCA analysis...")  
            pca_result = self._run_pca_analysis(session_id, final_pca_vars)
            
            # Handle PCA skipped due to statistical tests
            if pca_result.get('pca_skipped'):
                logger.info(f"‚ö†Ô∏è PCA skipped: {pca_result['message']}")
                print("\n" + "="*60)
                print("üìä PCA ANALYSIS SKIPPED DUE TO STATISTICAL TESTS")
                print("="*60)
                print(f"Reason: {pca_result['message']}")
                print(f"KMO Value: {pca_result.get('data', {}).get('kmo_value', 'N/A')}")
                print(f"Bartlett's p-value: {pca_result.get('data', {}).get('bartlett_p_value', 'N/A')}")
                print("Recommendation: Continuing with composite analysis only")
                print("="*60 + "\n")
                
                # Mark as special case - not a failure but PCA was not suitable
                pca_result['success'] = False
                pca_result['skipped_reason'] = pca_result.get('data', {})
            elif pca_result['success']:
                logger.info("‚úÖ PCA analysis completed successfully")
            else:
                logger.error(f"‚ùå PCA analysis failed: {pca_result['message']}")
            
            analysis_execution_time = time.time() - analysis_start_time
            logger.info(f"üîÑ Both analyses completed sequentially in {analysis_execution_time:.2f} seconds")
            
            # üéØ LOG ANALYSIS COMPLETION - CRITICAL FOR DEMO ANALYTICS
            if interaction_logger:
                interaction_logger.log_analysis_event(
                    session_id=session_id,
                    event_type='sequential_analysis_complete',
                    details={
                        'composite_success': composite_result['success'],
                        'pca_success': pca_result['success'],
                        'analysis_time_seconds': analysis_execution_time,
                        'composite_error': composite_result.get('error_details') if not composite_result['success'] else None,
                        'pca_error': pca_result.get('error_details') if not pca_result['success'] else None,
                        'execution_method': 'sequential'
                    },
                    success=composite_result['success'] and pca_result['success']
                )
            
            # Check if either analysis failed
            if not composite_result['success']:
                # üéØ LOG COMPOSITE ANALYSIS FAILURE - DEMO ERROR TRACKING
                if interaction_logger:
                    interaction_logger.log_error(
                        session_id=session_id,
                        error_type='CompositeAnalysisFailure',
                        error_message=composite_result['message'],
                        stack_trace=composite_result.get('error_details', 'No stack trace available')
                    )
                
                return ToolExecutionResult(
                    success=False,
                    message=f"Composite analysis failed: {composite_result['message']}",
                    error_details=composite_result.get('error_details')
                )
            
            # Check for PCA recovery in unified dataset if initial PCA failed
            pca_recovered = False
            if not pca_result['success']:
                # Check if PCA was skipped due to statistical tests
                if pca_result.get('pca_skipped'):
                    logger.info(f"‚ÑπÔ∏è PCA was skipped due to statistical tests: {pca_result['message']}")
                    # This is not an error - continue with composite-only results
                    pca_result = {
                        'success': False,
                        'pca_skipped': True,
                        'message': pca_result['message'],
                        'data': pca_result.get('data', {}),
                        'skipped_reason': pca_result.get('skipped_reason', {})
                    }
                else:
                    logger.warning(f"‚ö†Ô∏è Initial PCA analysis failed: {pca_result['message']}")
                    logger.info("üîç Checking if PCA was recovered during unified dataset creation...")
                    
                    # Check if PCA rankings file exists (indicates successful PCA recovery)
                    pca_rankings_path = f"instance/uploads/{session_id}/analysis_vulnerability_rankings_pca.csv"
                    if os.path.exists(pca_rankings_path):
                        logger.info("‚úÖ PCA analysis recovered successfully in unified dataset phase")
                        pca_recovered = True
                        pca_result = {
                            'success': True,
                            'message': 'PCA analysis completed via unified dataset recovery',
                            'data': {'recovered_from_unified_dataset': True}
                        }
                    else:
                        # üéØ LOG PCA ANALYSIS FAILURE - DEMO ERROR TRACKING
                        if interaction_logger:
                            interaction_logger.log_error(
                                session_id=session_id,
                                error_type='PCAAnalysisFailure',
                                error_message=pca_result['message'],
                                stack_trace=pca_result.get('error_details', 'No stack trace available')
                            )
                        
                        return ToolExecutionResult(
                            success=False,
                            message=f"PCA analysis failed: {pca_result['message']}",
                            error_details=pca_result.get('error_details')
                        )
            
            # Create/update unified dataset without settlement integration
            if create_unified_dataset:
                # Actually attempt to create unified dataset instead of assuming it exists
                logger.info("üìä Creating unified dataset...")
                try:
                    from ..data.unified_dataset_builder import UnifiedDatasetBuilder
                    builder = UnifiedDatasetBuilder(session_id)
                    unified_result = builder.build_unified_dataset()
                    
                    if unified_result.get('status') == 'success':
                        logger.info(f"‚úÖ Unified dataset created successfully: {unified_result['message']}")
                        unified_result['success'] = True  # Add success key for compatibility
                    else:
                        logger.warning(f"‚ùå Unified dataset creation failed: {unified_result['message']}")
                        unified_result['success'] = False  # Add success key for compatibility
                        # Continue anyway - analyses succeeded
                except Exception as e:
                    logger.error(f"üí• Error creating unified dataset: {e}")
                    unified_result = {'success': False, 'message': f'Error: {str(e)}'}
                    # Continue anyway - analyses succeeded
            
            # Generate comparison summary (only if PCA was actually run)
            if pca_result.get('pca_skipped'):
                logger.info("üìã Skipping comparison summary - PCA was not suitable")
                comparison_summary = {
                    'summary': 'PCA analysis was not performed due to statistical test results',
                    'pca_skipped': True,
                    'reason': pca_result.get('message', 'Data not suitable for PCA')
                }
            else:
                logger.info("üìã Generating dual-method comparison summary...")
                comparison_summary = self._generate_comparison_summary(
                    composite_result['data'], 
                    pca_result['data']
                )
            
            # Note: Visualization auto-generation removed - users can request visualizations separately
            
            # Prepare comprehensive result (clean summary without auto-generated visualizations)
            result_data = {
                'composite_analysis': composite_result['data'],
                'pca_analysis': pca_result['data'],
                'comparison_summary': comparison_summary,
                'analyses_completed': ['composite_score', 'pca'],
                'unified_dataset_created': create_unified_dataset and unified_result.get('success', False),
                'execution_time_seconds': analysis_execution_time,
                'execution_method': 'sequential',
                'settlement_integration_logic': 'excluded',
                'original_settlement_data': 'preserved'
            }
            
            # Mark comprehensive analysis as complete for workflow guidance
            self._mark_analysis_complete(session_id)
            
            # Extract key metrics for success message
            wards_analyzed = composite_result.get('data', {}).get('wards_analyzed', 'N/A')
            components_found = pca_result.get('data', {}).get('components_found', 'N/A')
            variance_explained = pca_result.get('data', {}).get('variance_explained', 'N/A')
            
            # Generate comprehensive user-friendly summary
            success_message = self._generate_comprehensive_summary(
                composite_result, pca_result, comparison_summary, analysis_execution_time, session_id
            )
            
            total_execution_time = time.time() - tool_start_time
            
            # üéØ LOG TOOL EXECUTION SUCCESS - CRITICAL FOR DEMO ANALYTICS
            if interaction_logger:
                # Log successful tool completion with comprehensive metrics
                interaction_logger.log_analysis_event(
                    session_id=session_id,
                    event_type='tool_execution_complete',
                    details={
                        'tool_name': self.name,
                        'total_execution_time_seconds': total_execution_time,
                        'analysis_time_seconds': analysis_execution_time,
                        'overhead_time_seconds': total_execution_time - analysis_execution_time,
                        'wards_analyzed': composite_result.get('data', {}).get('wards_analyzed', 'N/A'),
                        'visualizations_created': len(result_data.get('visualizations', {})),
                        'unified_dataset_created': result_data['unified_dataset_created'],
                        'methods_completed': ['composite_score', 'pca'],
                        'variable_selection_methods': {
                            'composite': composite_result.get('data', {}).get('variable_selection_method', 'auto'),
                            'pca': pca_result.get('data', {}).get('variable_selection_method', 'auto')
                        },
                        'performance_metrics': {
                            'sequential_execution': True,
                            'execution_efficiency': round(analysis_execution_time / total_execution_time * 100, 1)
                        }
                    },
                    success=True
                )
            
            # Notify analysis state handler about completion
            try:
                from ..core.analysis_state_handler import get_analysis_state_handler
                state_handler = get_analysis_state_handler()
                state_result = state_handler.on_analysis_complete(session_id, result_data)
                logger.info(f"State handler notified: {state_result}")
            except Exception as e:
                logger.error(f"Failed to notify state handler: {e}")
                # Continue - not critical for analysis success
            
            # CRITICAL: Mark analysis complete in Redis for multi-worker consistency
            # This is the authoritative source of truth across ALL workers
            try:
                from ..core.redis_state_manager import get_redis_state_manager
                redis_manager = get_redis_state_manager()
                redis_success = redis_manager.mark_analysis_complete(session_id)
                if redis_success:
                    logger.info(f"üéØ Redis: Analysis marked complete for session {session_id}")
                else:
                    logger.warning(f"‚ö†Ô∏è Redis: Failed to mark analysis complete for {session_id}")
            except Exception as e:
                logger.error(f"Redis state manager error: {e}")
                # Continue - fallback to other methods
            
            # Set the general analysis_complete flag that ITN tool checks
            try:
                from flask import session
                session['analysis_complete'] = True
                logger.info(f"‚úÖ Set session['analysis_complete'] = True for ITN planning")
            except Exception as e:
                logger.warning(f"Failed to set analysis_complete flag: {e}")
            
            # Create analysis completion marker file for cross-worker reliability
            # NOTE: This is a FALLBACK when Redis is unavailable
            try:
                import os
                from pathlib import Path
                from flask import session
                
                session_folder = Path("instance/uploads") / session_id
                if session_folder.exists():
                    marker_file = session_folder / ".analysis_complete"
                    marker_file.touch()
                    logger.info(f"‚úÖ Created .analysis_complete marker file for session {session_id} (fallback)")
                    
                    # Update workflow stage to indicate analysis is now complete
                    session['workflow_stage'] = 'analysis_complete'
                    session.modified = True
                    logger.info(f"‚úÖ Updated workflow_stage to 'analysis_complete' for session {session_id}")
            except Exception as e:
                logger.warning(f"Failed to create analysis marker file or update session: {e}")
            
            return ToolExecutionResult(
                success=True,
                message=success_message,
                data=result_data,
                metadata={
                    'analyses_run': ['composite_score', 'pca'],
                    'execution_method': 'sequential',
                    'execution_time_seconds': analysis_execution_time,
                    'total_execution_time_seconds': total_execution_time,
                    'unified_dataset_status': 'created' if result_data['unified_dataset_created'] else 'failed',
                    'settlement_integration_logic': 'excluded',
                    'original_settlement_data': 'preserved',
                    'performance_benefits': 'Parallel execution reduces total analysis time',
                    'workflow_stage': 'comprehensive_analysis_complete',
                    'variable_consistency': 'region_aware_selection'
                }
            )
            
        except Exception as e:
            total_execution_time = time.time() - tool_start_time
            
            # üéØ LOG TOOL EXECUTION FAILURE - CRITICAL FOR DEMO ERROR TRACKING
            if interaction_logger:
                interaction_logger.log_error(
                    session_id=session_id,
                    error_type=f'ToolExecutionError:{type(e).__name__}',
                    error_message=str(e),
                    stack_trace=traceback.format_exc()
                )
                
                # Log detailed failure analysis event
                interaction_logger.log_analysis_event(
                    session_id=session_id,
                    event_type='tool_execution_failed',
                    details={
                        'tool_name': self.name,
                        'failure_time_seconds': total_execution_time,
                        'error_type': type(e).__name__,
                        'error_message': str(e),
                        'failure_stage': 'tool_execution',
                        'parameters': {
                            'composite_variables': kwargs.get('composite_variables'),
                            'pca_variables': kwargs.get('pca_variables'),
                            'validate_variables': kwargs.get('validate_variables', True)
                        }
                    },
                    success=False
                )
            
            logger.error(f"Complete analysis failed: {e}", exc_info=True)
            return ToolExecutionResult(
                success=False,
                message=f"Complete analysis failed: {str(e)}",
                error_details=str(e)
            )
    
    def _run_composite_analysis(self, session_id: str, custom_variables: Optional[List[str]] = None) -> Dict[str, Any]:
        """Run composite score analysis with optional custom variable selection"""
        composite_start_time = time.time()
        
        # üéØ LOG COMPOSITE ANALYSIS START - DEMO ANALYTICS
        if hasattr(current_app, 'services') and current_app.services.interaction_logger:
            interaction_logger = current_app.services.interaction_logger
            interaction_logger.log_analysis_event(
                session_id=session_id,
                event_type='composite_analysis_start',
                details={
                    'custom_variables': custom_variables,
                    'variable_count': len(custom_variables) if custom_variables else 0,
                    'selection_method': 'user_specified' if custom_variables else 'region_aware_auto'
                },
                success=True
            )
        
        try:
            from ..analysis.engine import AnalysisEngine
            from ..data import DataHandler
            
            # Convert session_id to session_folder path
            session_folder = f"instance/uploads/{session_id}"
            
            # Initialize data handler for the session
            data_handler = DataHandler(session_folder)
            analysis_engine = AnalysisEngine(data_handler)
            
            # Run composite analysis with custom variables (or auto-selection if None)
            result = analysis_engine.run_composite_analysis(session_id, variables=custom_variables)
            
            # Add custom variable information to result
            result_data = result.get('data', {})
            if custom_variables:
                result_data['custom_variables_used'] = custom_variables
                result_data['variable_selection_method'] = 'user_specified'
            else:
                result_data['variable_selection_method'] = 'region_aware_auto'
            
            composite_execution_time = time.time() - composite_start_time
            
            # üéØ LOG COMPOSITE ANALYSIS SUCCESS - DEMO ANALYTICS
            if hasattr(current_app, 'services') and current_app.services.interaction_logger:
                interaction_logger = current_app.services.interaction_logger
                interaction_logger.log_analysis_event(
                    session_id=session_id,
                    event_type='composite_analysis_complete',
                    details={
                        'execution_time_seconds': composite_execution_time,
                        'wards_analyzed': result_data.get('wards_analyzed', 'N/A'),
                        'variables_used': result_data.get('variables_used', []),
                        'selection_method': result_data.get('variable_selection_method'),
                        'visualizations_created': len(result_data.get('visualizations', {}))
                    },
                    success=result.get('status') == 'success'
                )
            
            return {
                'success': result.get('status') == 'success',
                'message': result.get('message', 'Composite analysis completed'),
                'data': result_data,
                'error_details': result.get('error_details')
            }
            
        except Exception as e:
            composite_execution_time = time.time() - composite_start_time
            
            # üéØ LOG COMPOSITE ANALYSIS FAILURE - DEMO ERROR TRACKING
            if hasattr(current_app, 'services') and current_app.services.interaction_logger:
                interaction_logger = current_app.services.interaction_logger
                interaction_logger.log_error(
                    session_id=session_id,
                    error_type=f'CompositeAnalysisError:{type(e).__name__}',
                    error_message=str(e),
                    stack_trace=traceback.format_exc()
                )
                
                interaction_logger.log_analysis_event(
                    session_id=session_id,
                    event_type='composite_analysis_failed',
                    details={
                        'execution_time_seconds': composite_execution_time,
                        'error_type': type(e).__name__,
                        'custom_variables': custom_variables,
                        'failure_stage': 'composite_analysis'
                    },
                    success=False
                )
            
            logger.error(f"Composite analysis error: {e}")
            return {
                'success': False,
                'message': f"Composite analysis failed: {str(e)}",
                'error_details': str(e)
            }
    
    def _run_pca_analysis(self, session_id: str, custom_variables: Optional[List[str]] = None) -> Dict[str, Any]:
        """Run PCA analysis with optional custom variable selection"""
        pca_start_time = time.time()
        
        # üéØ LOG PCA ANALYSIS START - DEMO ANALYTICS
        if hasattr(current_app, 'services') and current_app.services.interaction_logger:
            interaction_logger = current_app.services.interaction_logger
            interaction_logger.log_analysis_event(
                session_id=session_id,
                event_type='pca_analysis_start',
                details={
                    'custom_variables': custom_variables,
                    'variable_count': len(custom_variables) if custom_variables else 0,
                    'selection_method': 'user_specified' if custom_variables else 'region_aware_auto'
                },
                success=True
            )
        
        try:
            from ..analysis.pca_pipeline import run_independent_pca_analysis
            from ..data import DataHandler
            
            # Convert session_id to session_folder path
            session_folder = f"instance/uploads/{session_id}"
            
            # Initialize data handler for the session
            data_handler = DataHandler(session_folder)
            
            # Run PCA analysis with custom variables (or auto-selection if None)
            result = run_independent_pca_analysis(
                data_handler=data_handler,
                session_id=session_id,
                selected_variables=custom_variables
            )
            
            # Check if PCA was not suitable
            if result.get('status') == 'pca_not_suitable':
                logger.info(f"PCA not suitable: {result.get('message')}")
                
                print("\n" + "‚ö†Ô∏è"*20)
                print("PCA ANALYSIS NOT SUITABLE - STATISTICAL TESTS FAILED")
                print("‚ö†Ô∏è"*20)
                print(f"\nDetailed Results:")
                print(f"  - KMO Value: {result.get('data', {}).get('kmo_value', 0):.3f}")
                print(f"  - KMO Threshold: 0.5")
                print(f"  - Bartlett's p-value: {result.get('data', {}).get('bartlett_p_value', 1.0):.4e}")
                print(f"  - Required p-value: < 0.05")
                print(f"\nReason: {result.get('message')}")
                print(f"Action: Skipping PCA, will use composite analysis only")
                print("‚ö†Ô∏è"*20 + "\n")
                
                # Log the PCA suitability check result
                if hasattr(current_app, 'services') and current_app.services.interaction_logger:
                    interaction_logger = current_app.services.interaction_logger
                    interaction_logger.log_analysis_event(
                        session_id=session_id,
                        event_type='pca_not_suitable',
                        details={
                            'reason': result.get('message'),
                            'kmo_value': result.get('data', {}).get('kmo_value'),
                            'bartlett_p_value': result.get('data', {}).get('bartlett_p_value'),
                            'recommendation': result.get('data', {}).get('recommendation')
                        },
                        success=False
                    )
                
                return {
                    'success': False,
                    'pca_skipped': True,
                    'message': result.get('message'),
                    'data': result.get('data', {}),
                    'recommendation': 'Using composite analysis only due to statistical test results'
                }
            
            # Add custom variable information to result
            result_data = result.get('data', {})
            if custom_variables:
                result_data['custom_variables_used'] = custom_variables
                result_data['variable_selection_method'] = 'user_specified'
            else:
                result_data['variable_selection_method'] = 'region_aware_auto'
            
            pca_execution_time = time.time() - pca_start_time
            
            # üéØ LOG PCA ANALYSIS SUCCESS - DEMO ANALYTICS
            if hasattr(current_app, 'services') and current_app.services.interaction_logger:
                interaction_logger = current_app.services.interaction_logger
                interaction_logger.log_analysis_event(
                    session_id=session_id,
                    event_type='pca_analysis_complete',
                    details={
                        'execution_time_seconds': pca_execution_time,
                        'components_found': result_data.get('components_found', 'N/A'),
                        'variance_explained': result_data.get('variance_explained', 'N/A'),
                        'variables_used': result_data.get('variables_used', []),
                        'selection_method': result_data.get('variable_selection_method'),
                        'visualizations_created': len(result_data.get('visualizations', {}))
                    },
                    success=result.get('status') == 'success'
                )
            
            return {
                'success': result.get('status') == 'success',
                'message': result.get('message', 'PCA analysis completed'),
                'data': result_data,
                'error_details': result.get('error_details')
            }
            
        except Exception as e:
            pca_execution_time = time.time() - pca_start_time
            
            # üéØ LOG PCA ANALYSIS FAILURE - DEMO ERROR TRACKING
            if hasattr(current_app, 'services') and current_app.services.interaction_logger:
                interaction_logger = current_app.services.interaction_logger
                interaction_logger.log_error(
                    session_id=session_id,
                    error_type=f'PCAAnalysisError:{type(e).__name__}',
                    error_message=str(e),
                    stack_trace=traceback.format_exc()
                )
                
                interaction_logger.log_analysis_event(
                    session_id=session_id,
                    event_type='pca_analysis_failed',
                    details={
                        'execution_time_seconds': pca_execution_time,
                        'error_type': type(e).__name__,
                        'custom_variables': custom_variables,
                        'failure_stage': 'pca_analysis'
                    },
                    success=False
                )
            
            logger.error(f"PCA analysis error: {e}")
            return {
                'success': False,
                'message': f"PCA analysis failed: {str(e)}",
                'error_details': str(e)
            }
    
    def _create_settlement_free_unified_dataset(self, session_id: str) -> Dict[str, Any]:
        """Create unified dataset without any settlement integration"""
        try:
            from ..data.unified_dataset_builder import create_settlement_free_unified_dataset
            
            # Convert session_id to session_folder path
            session_folder = f"instance/uploads/{session_id}"
            
            # Create unified dataset without settlement integration
            result = create_settlement_free_unified_dataset(session_folder)
            
            return {
                'success': result.get('status') == 'success',
                'message': result.get('message', 'Settlement-free unified dataset created'),
                'data': result.get('data', {}),
                'error_details': result.get('error_details')
            }
            
        except Exception as e:
            logger.error(f"Settlement-free unified dataset creation error: {e}")
            return {
                'success': False,
                'message': f"Unified dataset creation failed: {str(e)}",
                'error_details': str(e)
            }
    
    def _generate_comparison_summary(self, composite_data: Dict, pca_data: Dict) -> Dict[str, Any]:
        """Generate summary comparing both methods"""
        try:
            summary = {
                'methods_compared': ['composite_score', 'pca'],
                'composite_top_wards': composite_data.get('top_vulnerable_wards', [])[:10],
                'pca_top_wards': pca_data.get('top_vulnerable_wards', [])[:10],
                'agreement_rate': 0,
                'consensus_wards': [],
                'method_differences': []
            }
            
            # Calculate agreement between methods
            composite_top = set([w.get('ward_name', '') for w in summary['composite_top_wards']])
            pca_top = set([w.get('ward_name', '') for w in summary['pca_top_wards']])
            
            if composite_top and pca_top:
                consensus = composite_top.intersection(pca_top)
                summary['consensus_wards'] = list(consensus)
                summary['agreement_rate'] = round((len(consensus) / max(len(composite_top), len(pca_top))) * 100, 1)
            
            return summary
            
        except Exception as e:
            logger.warning(f"Comparison summary generation failed: {e}")
            return {
                'methods_compared': ['composite_score', 'pca'],
                'agreement_rate': 'unavailable',
                'error': str(e)
            }
    
    def _generate_comprehensive_summary(self, composite_result, pca_result, comparison_summary, execution_time, session_id):
        """Generate conversational guidance after analysis completion"""
        try:
            # Load the unified dataset to get detailed rankings
            try:
                from ..data.unified_dataset_builder import load_unified_dataset
                gdf = load_unified_dataset(session_id)
            except Exception as load_error:
                logger.warning(f"Failed to load unified dataset: {load_error}")
                # Try alternative: load from tools base
                try:
                    from .base import get_session_unified_dataset
                    gdf = get_session_unified_dataset(session_id)
                except Exception as alt_error:
                    logger.warning(f"Alternative unified dataset load failed: {alt_error}")
                    gdf = None
            
            # üîç DEBUG: Check unified dataset structure
            if gdf is not None:
                print(f"üîç SUMMARY DEBUG: Unified dataset loaded: {len(gdf)} wards, {len(gdf.columns)} columns")
                print(f"üîç SUMMARY DEBUG: First few columns: {list(gdf.columns)[:10]}")
                
                # Check for key columns
                key_columns = ['composite_score', 'pca_score', 'WardName', 'WardCode']
                for col in key_columns:
                    if col in gdf.columns:
                        print(f"üîç SUMMARY DEBUG: ‚úÖ Found '{col}' column")
                    else:
                        print(f"üîç SUMMARY DEBUG: ‚ùå Missing '{col}' column")
            else:
                print(f"üîç SUMMARY DEBUG: ‚ùå Failed to load unified dataset")
            
            if gdf is None:
                # Try to generate summary from analysis results directly
                logger.info("Unified dataset not available, generating summary from analysis results")
                return self._generate_summary_from_analysis_results(composite_result, pca_result, comparison_summary, execution_time)
            
            # DEBUG: Check what columns exist in the unified dataset
            logger.info(f"üîç UNIFIED DATASET DEBUG: Available columns: {list(gdf.columns)}")
            logger.info(f"üîç UNIFIED DATASET DEBUG: Dataset shape: {gdf.shape}")
            
            # Use variable resolver for robust column detection
            from app.services.variable_resolution_service import variable_resolver
            
            # Find composite score column
            composite_candidates = ['composite_score', 'composite', 'comp_score']
            composite_score_col = None
            for candidate in composite_candidates:
                exists, resolved = variable_resolver.check_column_exists(candidate, list(gdf.columns))
                if exists:
                    composite_score_col = resolved
                    break
            
            # Find PCA score column
            pca_candidates = ['pca_score', 'pc1_risk_score', 'pca', 'pc1_score']
            pca_score_col = None
            for candidate in pca_candidates:
                exists, resolved = variable_resolver.check_column_exists(candidate, list(gdf.columns))
                if exists:
                    pca_score_col = resolved
                    break
            
            # Find ward name column
            ward_candidates = ['WardName', 'ward_name', 'ward', 'Ward']
            ward_name_col = None
            for candidate in ward_candidates:
                exists, resolved = variable_resolver.check_column_exists(candidate, list(gdf.columns))
                if exists:
                    ward_name_col = resolved
                    break
            
            logger.info(f"üîç COLUMN DETECTION: Composite={composite_score_col}, PCA={pca_score_col}, Ward={ward_name_col}")
            
            # Check if we found the required columns
            missing_analysis = []
            if not composite_score_col:
                missing_analysis.append("composite_score")
            if not pca_score_col:
                missing_analysis.append("pca_score")
            if not ward_name_col:
                missing_analysis.append("ward_name")
            
            if missing_analysis:
                logger.warning(f"üîç UNIFIED DATASET DEBUG: Missing analysis columns: {missing_analysis}")
                logger.info(f"üîç UNIFIED DATASET DEBUG: Falling back to analysis results directly")
                
                # Fall back to using analysis results directly
                return self._generate_summary_from_analysis_results(composite_result, pca_result, comparison_summary, execution_time)
            
            # Check for NaN values in score columns
            composite_nan_count = gdf[composite_score_col].isna().sum() if composite_score_col else 0
            pca_nan_count = gdf[pca_score_col].isna().sum() if pca_score_col else 0
            
            if composite_nan_count > 0 or pca_nan_count > 0:
                logger.warning(f"üîç UNIFIED DATASET DEBUG: NaN values found - Composite: {composite_nan_count}, PCA: {pca_nan_count}")
                logger.info(f"üîç UNIFIED DATASET DEBUG: Falling back to analysis results directly")
                
                # Fall back to using analysis results directly
                return self._generate_summary_from_analysis_results(composite_result, pca_result, comparison_summary, execution_time)
            
            # Detect category columns
            composite_category_col = None
            pca_category_col = None
            
            for col in gdf.columns:
                if 'composite' in col.lower() and 'categor' in col.lower():
                    composite_category_col = col
                elif ('pca' in col.lower() or 'vulnerabilit' in col.lower()) and 'categor' in col.lower():
                    pca_category_col = col
            
            # Get top and bottom 5 for both methods (using dynamically detected column names)
            composite_cols = [ward_name_col, composite_score_col]
            if composite_category_col:
                composite_cols.append(composite_category_col)
            
            composite_top5 = gdf.nlargest(5, composite_score_col)[composite_cols].to_dict('records')
            composite_bottom5 = gdf.nsmallest(5, composite_score_col)[composite_cols].to_dict('records')
            
            pca_cols = [ward_name_col, pca_score_col]
            if pca_category_col:
                pca_cols.append(pca_category_col)
                
            pca_top5 = gdf.nlargest(5, pca_score_col)[pca_cols].to_dict('records')
            pca_bottom5 = gdf.nsmallest(5, pca_score_col)[pca_cols].to_dict('records')
            
            # Get variables used for each method (including custom variable info)
            composite_data = composite_result.get('data', {})
            pca_data = pca_result.get('data', {})
            
            # Extract variables for composite analysis - IMPROVED DETECTION
            composite_vars = (
                composite_data.get('custom_variables_used') or 
                composite_data.get('variables_used', []) or
                composite_data.get('selected_variables', []) or
                composite_data.get('analysis_variables', [])
            )
            composite_selection_method = composite_data.get('variable_selection_method', 'auto')
            
            # Extract variables for PCA analysis - IMPROVED DETECTION
            pca_vars_raw = (
                pca_data.get('custom_variables_used') or 
                pca_data.get('variables_used', []) or
                pca_data.get('selected_variables', []) or
                pca_data.get('analysis_variables', [])
            )
            pca_selection_method = pca_data.get('variable_selection_method', 'auto')
            
            # Filter out ward identification columns from PCA variables
            pca_vars = [v for v in pca_vars_raw if v.lower() not in ['wardname', 'ward_name', 'ward', 'lga', 'state']]
            
            # DEBUG: Log variable detection results
            logger.info(f"üîç VARIABLE DETECTION: Composite vars: {composite_vars}")
            logger.info(f"üîç VARIABLE DETECTION: PCA vars: {pca_vars}")
            logger.info(f"üîç VARIABLE DETECTION: Composite data keys: {list(composite_data.keys())}")
            logger.info(f"üîç VARIABLE DETECTION: PCA data keys: {list(pca_data.keys())}")
            
            # Calculate method agreement
            agreement_rate = comparison_summary.get('agreement_rate', 0)
            consensus_wards = comparison_summary.get('consensus_wards', [])
            
            # Detect geographic region for contextualized recommendations
            region_info = self._extract_region_info(gdf)
            
            # Generate dynamic recommendations based on actual results
            recommendations = self._generate_smart_recommendations(
                composite_top5, pca_top5, composite_bottom5, pca_bottom5, 
                agreement_rate, consensus_wards, region_info, gdf
            )
            
            # Get analysis details for the conversational response
            ward_count = len(gdf)
            
            # Get geopolitical zone info
            geopolitical_zone = "your region"
            if 'detected_zone' in gdf.columns:
                zone = gdf['detected_zone'].iloc[0]
                if zone and zone != 'No zone detected':
                    geopolitical_zone = zone.replace('_', ' ').title()
            
            # Get number of variables used (already extracted earlier)
            num_variables = len(composite_vars) if composite_vars else "several"
            
            # Extract PCA test results if available
            pca_test_summary = ""
            if pca_result.get('pca_skipped'):
                # PCA was skipped due to statistical tests
                # Get values from the data field where they're stored
                kmo_value = pca_result.get('data', {}).get('kmo_value', 0)
                bartlett_p = pca_result.get('data', {}).get('bartlett_p_value', 1)
                # Interpret KMO value
                kmo_interpret = "Your data variables show weak relationships"
                if kmo_value < 0.3:
                    kmo_interpret = "Your data variables are mostly independent"
                elif kmo_value < 0.5:
                    kmo_interpret = "Your data variables show limited relationships"
                
                # Interpret Bartlett's test
                bartlett_interpret = "No significant patterns found between variables"
                if bartlett_p < 0.05:
                    bartlett_interpret = "Some patterns exist but not strong enough"
                
                pca_test_summary = f"""

**Behind the Scenes - Statistical Testing:**
I ran two tests to check if your data is suitable for advanced pattern analysis (PCA):

‚Ä¢ **KMO Test Result**: {kmo_value:.3f} (needed 0.5 or higher)
  ‚Üí {kmo_interpret}
  
‚Ä¢ **Bartlett's Test**: {"Failed" if bartlett_p >= 0.05 else "Passed"} (p-value = {bartlett_p:.3f})
  ‚Üí {bartlett_interpret}

‚Ä¢ **My Decision**: Used the Composite Score method only, which is more reliable for this type of data
"""
            elif pca_data:
                # PCA was performed - get test results from the analysis
                # Check if we have test results stored
                kmo_value = pca_data.get('kmo_value', None)
                bartlett_p = pca_data.get('bartlett_p_value', None)
                
                if kmo_value is not None and bartlett_p is not None:
                    # Interpret KMO value in user-friendly terms
                    if kmo_value >= 0.9:
                        kmo_interpret = "Your data variables have excellent relationships"
                    elif kmo_value >= 0.8:
                        kmo_interpret = "Your data variables have strong relationships"
                    elif kmo_value >= 0.7:
                        kmo_interpret = "Your data variables have good relationships"
                    elif kmo_value >= 0.6:
                        kmo_interpret = "Your data variables have moderate relationships"
                    elif kmo_value >= 0.5:
                        kmo_interpret = "Your data variables have adequate relationships"
                    else:
                        kmo_interpret = "Your data variables have weak relationships"
                    
                    # Format p-value in a readable way
                    if bartlett_p < 0.001:
                        bartlett_display = "< 0.001"
                    else:
                        bartlett_display = f"{bartlett_p:.3f}"
                    
                    pca_test_summary = f"""

**Behind the Scenes - Statistical Testing:**
I ran two tests to check if your data is suitable for advanced pattern analysis (PCA):

‚Ä¢ **KMO Test Result**: {kmo_value:.3f} (needed 0.5 or higher) ‚úì
  ‚Üí {kmo_interpret}
  
‚Ä¢ **Bartlett's Test**: Passed (p-value {bartlett_display}) ‚úì
  ‚Üí Significant patterns found between variables

‚Ä¢ **My Decision**: Used both methods (Composite Score and PCA) for comprehensive analysis
  ‚Üí This gives you two different perspectives on malaria risk in your wards
"""
                else:
                    # Default message if test results not stored
                    pca_test_summary = """

**Behind the Scenes - Statistical Testing:**
‚Ä¢ **Data Suitability**: Passed all required tests ‚úì
‚Ä¢ **My Decision**: Used both methods (Composite Score and PCA) for comprehensive analysis
"""
            
            # Build conversational response
            response = f"""Analysis complete! I've ranked all {ward_count} wards by malaria risk.

**Here's what I did:**
1. **Cleaned your data** - Fixed ward name mismatches and filled missing values using neighboring areas
2. **Selected {num_variables} risk factors** - Based on {geopolitical_zone}'s malaria patterns
3. **Normalized everything** - Put all variables on the same 0-1 scale for fair comparison
4. **Ran statistical tests** - Checked if your data is suitable for advanced pattern analysis
5. **Calculated risk scores using {"both" if not pca_result.get("pca_skipped") else "the"} method{"s" if not pca_result.get("pca_skipped") else ""}:**
   ‚Ä¢ **Composite Score**: Simple average of all risk factors (transparent and easy to understand)
   {f"‚Ä¢ **PCA Score**: Statistical method that finds hidden patterns in your data" if not pca_result.get("pca_skipped") else ""}
6. **Ranked all wards** - From highest to lowest risk for intervention planning
{pca_test_summary}

What would you like to do next?
‚Ä¢ **Plan ITN/bed net distribution** - Allocate nets optimally based on these rankings
‚Ä¢ View highest risk wards that need urgent intervention
‚Ä¢ View lowest risk wards  
‚Ä¢ Create risk maps (vulnerability maps for either composite or PCA method)
‚Ä¢ Compare methods (composite vs PCA)
‚Ä¢ Export results

**To start ITN planning**, just say "I want to plan bed net distribution" or "Help me distribute ITNs"."""
            
            return response
            
        except Exception as e:
            logger.error(f"Failed to generate comprehensive summary: {e}", exc_info=True)
            # Provide a better fallback that includes what we can extract from the results
            fallback_parts = [f"‚úÖ **Analysis Complete** in {execution_time:.1f} seconds!"]
            
            # Try to extract basic info from results
            if composite_result and 'data' in composite_result:
                comp_data = composite_result['data']
                if 'wards_analyzed' in comp_data:
                    fallback_parts.append(f"**Analyzed:** {comp_data['wards_analyzed']} wards")
            
            fallback_parts.append("**Methods:** Composite Score + Principal Component Analysis")
            fallback_parts.append("**Status:** Both analyses completed successfully")
            fallback_parts.append("")
            fallback_parts.append("‚ö†Ô∏è *Detailed summary generation failed. Check the visualization results above for rankings and maps.*")
            fallback_parts.append("")
            fallback_parts.append("**Next Steps:**")
            fallback_parts.append("- Review the generated maps and charts")
            fallback_parts.append("- Ask for specific ward rankings")
            fallback_parts.append("- Request additional visualizations")
            
            return "\n".join(fallback_parts)

    def _generate_summary_from_analysis_results(self, composite_result, pca_result, comparison_summary, execution_time):
        """Generate summary from analysis results when unified dataset is not available"""
        try:
            summary_parts = [f"‚úÖ **Analysis Complete** in {execution_time:.1f} seconds!"]
            summary_parts.append("")
            
            # Extract basic information from results
            total_wards = "N/A"
            if composite_result and 'data' in composite_result:
                comp_data = composite_result['data']
                if 'wards_analyzed' in comp_data:
                    total_wards = comp_data['wards_analyzed']
            
            summary_parts.append(f"**Analyzed:** {total_wards} wards")
            summary_parts.append("**Methods:** Composite Score + Principal Component Analysis")
            summary_parts.append("")
            
            # Try to extract top wards from results
            composite_top = []
            pca_top = []
            
            if composite_result and 'data' in composite_result:
                comp_data = composite_result['data']
                if 'top_vulnerable_wards' in comp_data:
                    composite_top = comp_data['top_vulnerable_wards'][:5]
            
            if pca_result and 'data' in pca_result:
                pca_data = pca_result['data']
                if 'top_vulnerable_wards' in pca_data:
                    pca_top = pca_data['top_vulnerable_wards'][:5]
            
            # Show top wards from each method
            if composite_top:
                summary_parts.append("### üéØ Top 5 Highest Risk (Composite Method)")
                for i, ward in enumerate(composite_top, 1):
                    ward_name = ward.get('ward_name', ward.get('WardName', 'Unknown'))
                    score = ward.get('composite_score', ward.get('score', 'N/A'))
                    if isinstance(score, float):
                        summary_parts.append(f"{i}. **{ward_name}** (Score: {score:.3f})")
                    else:
                        summary_parts.append(f"{i}. **{ward_name}** (Score: {score})")
                summary_parts.append("")
            
            if pca_top:
                summary_parts.append("### üìä Top 5 Highest Risk (PCA Method)")
                for i, ward in enumerate(pca_top, 1):
                    ward_name = ward.get('ward_name', ward.get('WardName', 'Unknown'))
                    score = ward.get('pca_score', ward.get('pc1_risk_score', ward.get('score', 'N/A')))
                    if isinstance(score, float):
                        summary_parts.append(f"{i}. **{ward_name}** (Score: {score:.3f})")
                    else:
                        summary_parts.append(f"{i}. **{ward_name}** (Score: {score})")
                summary_parts.append("")
            
            # Calculate consensus if both methods have results
            if composite_top and pca_top:
                comp_names = set([w.get('ward_name', w.get('WardName', '')) for w in composite_top])
                pca_names = set([w.get('ward_name', w.get('WardName', '')) for w in pca_top])
                consensus = comp_names.intersection(pca_names)
                
                if consensus:
                    summary_parts.append("### ü§ù Consensus High-Risk Wards")
                    summary_parts.append("*Identified as high-risk by both methods:*")
                    for ward in sorted(consensus):
                        if ward:  # Skip empty names
                            summary_parts.append(f"‚Ä¢ **{ward}**")
                    summary_parts.append("")
                
                agreement_pct = (len(consensus) / len(comp_names.union(pca_names))) * 100 if comp_names.union(pca_names) else 0
                summary_parts.append(f"**Method Agreement:** {agreement_pct:.0f}% consensus on high-risk areas")
                summary_parts.append("")
            
            # Add next steps
            summary_parts.append("### üí° Next Steps")
            summary_parts.append("- Review the generated visualizations above")
            summary_parts.append("- Ask for specific ward comparisons")
            summary_parts.append("- Request additional analysis or maps")
            summary_parts.append("- Export results for intervention planning")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            logger.error(f"Failed to generate summary from analysis results: {e}", exc_info=True)
            return f"‚úÖ **Analysis Complete** in {execution_time:.1f} seconds! Both methods completed successfully. Check the visualizations above for detailed results."

    def _extract_region_info(self, gdf):
        """Extract region information for contextualized recommendations"""
        try:
            # Try to get region info from the dataframe
            if 'detected_zone' in gdf.columns:
                zone = gdf['detected_zone'].iloc[0]
                if zone and zone != 'No zone detected':
                    zone_name = zone.replace('_', ' ').title()
                    return {
                        'zone': zone_name,
                        'region_name': f"{zone_name} Zone, Nigeria"
                    }
            
            # Try to extract from state information
            if 'StateCode' in gdf.columns:
                state_code = gdf['StateCode'].iloc[0]
                if state_code == 'KN':
                    return {
                        'zone': 'North West',
                        'region_name': 'Kano State, Nigeria'
                    }
                elif state_code == 'NI':
                    return {
                        'zone': 'North Central', 
                        'region_name': 'Niger State, Nigeria'
                    }
            
            # Fallback
            return {
                'zone': 'Nigeria',
                'region_name': 'Nigeria'
            }
            
        except Exception:
            return {
                'zone': 'Nigeria',
                'region_name': 'Nigeria'
            }
    
    def _generate_smart_recommendations(self, composite_top5, pca_top5, composite_bottom5, pca_bottom5, 
                                       agreement_rate, consensus_wards, region_info, gdf):
        """Generate dynamic recommendations based on actual analysis results"""
        recommendations = []
        
        # Get ward names for easier processing
        comp_high_risk = [ward['WardName'] for ward in composite_top5]
        pca_high_risk = [ward['WardName'] for ward in pca_top5]
        
        # Immediate action recommendations
        if consensus_wards and len(consensus_wards) > 0:
            recommendations.append("### Immediate Priority Actions")
            recommendations.append("")
            recommendations.append(f"**Target Consensus High-Risk Wards:** {', '.join(consensus_wards[:5])}")
            recommendations.append("*These wards are identified as high-risk by both analytical methods*")
            recommendations.append("")
            recommendations.append("**Recommended Interventions:**")
            recommendations.append("- Prioritize bed net distribution campaigns")
            recommendations.append("- Establish additional health monitoring points")
            recommendations.append("- Implement targeted vector control measures")
            recommendations.append("")
        
        # Secondary priorities based on method-specific findings  
        if agreement_rate < 50:
            recommendations.append("### Secondary Priority Actions")
            recommendations.append("")
            recommendations.append("**Complementary Risk Patterns Identified:**")
            
            # Composite-specific high-risk wards
            comp_only = [w for w in comp_high_risk if w not in consensus_wards]
            if comp_only:
                recommendations.append(f"- **Composite Analysis Priorities:** {', '.join(comp_only[:3])}")
                recommendations.append("  *Focus on multi-factor environmental and demographic risks*")
            
            # PCA-specific high-risk wards
            pca_only = [w for w in pca_high_risk if w not in consensus_wards]
            if pca_only:
                recommendations.append(f"- **PCA Analysis Priorities:** {', '.join(pca_only[:3])}")
                recommendations.append("  *Address principal risk factor clusters*")
            
            recommendations.append("")
        
        # Monitoring and evaluation
        recommendations.append("### Monitoring & Evaluation")
        recommendations.append("")
        low_risk_comp = [ward['WardName'] for ward in composite_bottom5[:3]]
        recommendations.append(f"**Control Areas for Impact Assessment:** {', '.join(low_risk_comp)}")
        recommendations.append("*Use these low-risk wards to measure intervention effectiveness*")
        recommendations.append("")
        
        # Resource allocation guidance - IMPROVED WITH LLM-BASED ANALYSIS
        recommendations.append("### Resource Allocation Guidance")
        recommendations.append("")
        
        # Use LLM to analyze top vulnerable wards from composite score (default for implementation)
        composite_ward_names = [ward['WardName'] for ward in composite_top5]
        allocation_analysis = self._analyze_resource_allocation(composite_ward_names, len(gdf))
        
        recommendations.append(f"**Priority Implementation Areas:** {', '.join(composite_ward_names)}")
        recommendations.append(f"*{allocation_analysis}*")
        recommendations.append("")
        recommendations.append("**Implementation Strategy:**")
        recommendations.append("- Begin with composite score high-risk wards as primary targets")
        recommendations.append("- Use PCA results to understand underlying risk factors")
        recommendations.append("- Establish monitoring systems in both high and low-risk areas")
        
        return recommendations
    
    def _beautify_variable_names(self, variable_list):
        """Convert technical variable names to human-readable names using LLM"""
        if not variable_list:
            return []
        
        try:
            # Common mappings for malaria risk variables
            name_mappings = {
                'pfpr': 'Malaria Parasite Prevalence Rate',
                'housing_quality': 'Housing Quality Index',
                'elevation': 'Elevation Above Sea Level',
                'mean_EVI': 'Enhanced Vegetation Index',
                'distance_to_water': 'Distance to Water Bodies',
                'population_density': 'Population Density',
                'under_5_population': 'Under-5 Population Count',
                'pregnant_women': 'Pregnant Women Count',
                'literacy_rate': 'Literacy Rate',
                'poverty_index': 'Poverty Index',
                'urban_percentage': 'Urban Area Percentage',
                'rural_percentage': 'Rural Area Percentage',
                'health_facilities': 'Number of Health Facilities',
                'distance_to_health': 'Distance to Nearest Health Facility',
                'rainfall': 'Annual Rainfall',
                'temperature': 'Average Temperature',
                'humidity': 'Relative Humidity',
                'vector_breeding': 'Vector Breeding Site Density',
                'bed_net_coverage': 'Insecticide-Treated Net Coverage',
                'indoor_residual': 'Indoor Residual Spraying Coverage'
            }
            
            # Apply mappings where available, keep original for unmapped
            beautified = []
            for var in variable_list:
                if var in name_mappings:
                    beautified.append(name_mappings[var])
                else:
                    # Convert snake_case or other formats to Title Case
                    beautified_name = var.replace('_', ' ').replace('-', ' ').title()
                    beautified.append(beautified_name)
            
            return beautified
            
        except Exception as e:
            logger.warning(f"Variable name beautification failed: {e}")
            return variable_list
    
    def _analyze_resource_allocation(self, top_wards, total_wards):
        """Generate LLM-based analysis of resource allocation for top vulnerable wards"""
        try:
            num_priority_wards = len(top_wards)
            pct_priority = (num_priority_wards / total_wards) * 100
            
            # Generate contextual analysis based on the priority ward characteristics
            if pct_priority < 2:
                return f"Highly concentrated risk pattern with {num_priority_wards} priority wards ({pct_priority:.1f}% of total). Focused intervention strategy recommended with intensive resource deployment."
            elif pct_priority < 5:
                return f"Concentrated risk profile across {num_priority_wards} priority wards ({pct_priority:.1f}% of total). Targeted intervention approach suitable for maximum impact per resource invested."
            elif pct_priority < 10:
                return f"Moderate risk concentration in {num_priority_wards} priority wards ({pct_priority:.1f}% of total). Standard intervention protocols recommended with phased implementation."
            else:
                return f"Distributed risk pattern across {num_priority_wards} priority wards ({pct_priority:.1f}% of total). Consider regional coordination and phased rollout strategy."
                
        except Exception as e:
            logger.warning(f"Resource allocation analysis failed: {e}")
            return f"Analysis of {len(top_wards)} priority wards for targeted intervention planning"

    # Note: _has_valid_viz_paths method removed - no longer needed without auto-visualization

    def _mark_analysis_complete(self, session_id: str):
        """Mark comprehensive analysis as complete for workflow guidance"""
        try:
            # Update Flask session
            try:
                from flask import session
                session['comprehensive_analysis_complete'] = True
            except Exception as e:
                logger.warning(f"Flask session update failed (expected in worker context): {e}")
            
            # CRITICAL: Update workflow state JSON file
            try:
                from app.core.workflow_state_manager import WorkflowStateManager
                workflow_manager = WorkflowStateManager(session_id)
                success = workflow_manager.update_state(
                    {'analysis_complete': True},
                    transition_reason='Risk analysis completed successfully'
                )
                if success:
                    logger.info(f"‚úÖ Updated workflow state - analysis_complete = True for session {session_id}")
                else:
                    logger.error(f"‚ùå Failed to update workflow state for session {session_id}")
            except Exception as e:
                logger.error(f"Failed to update workflow state: {e}")
            
            # Also update agent state if available
            try:
                from app.core.analysis_state_handler import get_analysis_state_handler
                state_handler = get_analysis_state_handler()
                state_handler.on_analysis_complete(session_id, {'analysis_type': 'comprehensive'})
            except Exception as e:
                logger.warning(f"Failed to update agent state: {e}")
            
            logger.info(f"‚úÖ Marked comprehensive analysis complete for session {session_id}")
        except Exception as e:
            logger.error(f"Failed to mark analysis complete: {e}")
    
    def _mark_partial_analysis_complete(self, session_id: str, analysis_type: str):
        """Mark partial analysis as complete for workflow guidance"""
        try:
            from flask import session
            if 'partial_analyses_complete' not in session:
                session['partial_analyses_complete'] = []
            
            if analysis_type not in session['partial_analyses_complete']:
                session['partial_analyses_complete'].append(analysis_type)
                logger.info(f"‚úÖ Marked {analysis_type} analysis complete for session {session_id}")
        except Exception as e:
            logger.warning(f"Failed to mark {analysis_type} analysis complete: {e}")


class RunCompositeAnalysisInput(BaseModel):
    """Input for composite analysis only"""
    session_id: str = Field(..., description="Session identifier for data access")


class RunCompositeAnalysis(DataAnalysisTool):
    """Run composite score analysis only"""
    
    name: str = "run_composite_analysis"
    description: str = "Run composite score malaria risk analysis with equal weights for all variables"
    variables: Optional[List[str]] = Field(None, description="Custom variables for composite analysis")
    
    def execute(self, session_id: str, **kwargs) -> ToolExecutionResult:
        """Execute composite score analysis"""
        kwargs['session_id'] = session_id
        return self._execute(**kwargs)
    
    def _execute(self, **kwargs) -> ToolExecutionResult:
        """Execute composite score analysis"""
        try:
            session_id = kwargs.get('session_id')
            variables = kwargs.get('variables') or getattr(self, 'variables', None)
            
            from ..analysis.engine import AnalysisEngine
            from ..models.data_handler import DataHandler
            
            # Convert session_id to session_folder path
            session_folder = f"instance/uploads/{session_id}"
            
            # Initialize data handler for the session
            data_handler = DataHandler(session_folder)
            analysis_engine = AnalysisEngine(data_handler)
            
            # Pass custom variables if provided (all variables weighted equally)
            if variables:
                result = analysis_engine.run_composite_analysis(session_id, variables=variables)
            else:
                result = analysis_engine.run_composite_analysis(session_id)
            
            if result.get('status') == 'success':
                # Mark partial analysis complete for workflow guidance
                try:
                    from flask import session
                    session['composite_analysis_complete'] = True
                    session['analysis_complete'] = True  # Also set general flag for ITN
                except:
                    pass  # Session not available in test context
                
                return ToolExecutionResult(
                    success=True,
                    message="Composite score analysis completed successfully",
                    data=result.get('data', {}),
                    metadata={'analysis_type': 'composite_score'}
                )
            else:
                return ToolExecutionResult(
                    success=False,
                    message=f"Composite analysis failed: {result.get('message')}",
                    error_details=result.get('error_details')
                )
                
        except Exception as e:
            logger.error(f"Composite analysis failed: {e}")
            return ToolExecutionResult(
                success=False,
                message=f"Composite analysis failed: {str(e)}",
                error_details=str(e)
            )


class RunPCAAnalysisInput(BaseModel):
    """Input for PCA analysis only"""
    session_id: str = Field(..., description="Session identifier for data access")


class RunPCAAnalysis(DataAnalysisTool):
    """Run PCA analysis only"""
    
    name: str = "run_pca_analysis"
    description: str = "Run Principal Component Analysis (PCA) for malaria risk assessment"
    variables: Optional[List[str]] = Field(None, description="Custom variables for PCA analysis")
    
    def execute(self, session_id: str, **kwargs) -> ToolExecutionResult:
        """Execute PCA analysis"""
        kwargs['session_id'] = session_id
        return self._execute(**kwargs)
    
    def _execute(self, **kwargs) -> ToolExecutionResult:
        """Execute PCA analysis"""
        try:
            session_id = kwargs.get('session_id')
            variables = kwargs.get('variables') or getattr(self, 'variables', None)
            
            from ..analysis.engine import AnalysisEngine
            from ..models.data_handler import DataHandler
            
            # Convert session_id to session_folder path
            session_folder = f"instance/uploads/{session_id}"
            
            # Initialize data handler for the session
            data_handler = DataHandler(session_folder)
            analysis_engine = AnalysisEngine(data_handler)
            
            # Pass custom variables if provided
            if variables:
                result = analysis_engine.run_pca_analysis(session_id, variables=variables)
            else:
                result = analysis_engine.run_pca_analysis(session_id)
            
            if result.get('status') == 'success':
                # Mark partial analysis complete for workflow guidance
                try:
                    from flask import session
                    session['pca_analysis_complete'] = True
                    session['analysis_complete'] = True  # Also set general flag for ITN
                except:
                    pass  # Session not available in test context
                
                return ToolExecutionResult(
                    success=True,
                    message="PCA analysis completed successfully",
                    data=result.get('data', {}),
                    metadata={'analysis_type': 'pca'}
                )
            else:
                return ToolExecutionResult(
                    success=False,
                    message=f"PCA analysis failed: {result.get('message')}",
                    error_details=result.get('error_details')
                )
                
        except Exception as e:
            logger.error(f"PCA analysis failed: {e}")
            return ToolExecutionResult(
                success=False,
                message=f"PCA analysis failed: {str(e)}",
                error_details=str(e)
            )


class GenerateComprehensiveAnalysisSummaryInput(BaseModel):
    """Input for comprehensive analysis summary"""
    session_id: str = Field(..., description="Session identifier for data access")
    top_n: int = Field(10, description="Number of top wards to show for each method", ge=5, le=50)
    include_method_comparison: bool = Field(True, description="Include detailed method comparison")


class GenerateComprehensiveAnalysisSummary(DataAnalysisTool):
    """
    Generate comprehensive analysis summary showing detailed results for both composite and PCA methods.
    
    This tool provides:
    - Top vulnerable wards from both methods with scores and rankings
    - Method comparison and consensus analysis
    - Detailed breakdown of analysis results
    - Side-by-side comparison of methodologies
    """
    
    name: str = "generate_comprehensive_analysis_summary"
    description: str = "Generate detailed analysis summary showing results from both composite and PCA methods with ward rankings"
    top_n: int = Field(10, description="Number of top wards to show for each method", ge=5, le=50)
    include_method_comparison: bool = Field(True, description="Include detailed method comparison")
    
    def execute(self, session_id: str, **kwargs) -> ToolExecutionResult:
        """Execute comprehensive analysis summary generation"""
        kwargs['session_id'] = session_id
        return self._execute(**kwargs)
    
    def _execute(self, **kwargs) -> ToolExecutionResult:
        """Generate comprehensive analysis summary"""
        try:
            session_id = kwargs.get('session_id')
            top_n = kwargs.get('top_n', self.top_n)
            include_comparison = kwargs.get('include_method_comparison', self.include_method_comparison)
            
            logger.info(f"Generating comprehensive analysis summary for session {session_id}")
            
            # Get unified dataset with analysis results
            try:
                from .base import get_session_unified_dataset
                gdf = get_session_unified_dataset(session_id)
                
                if gdf is None or len(gdf) == 0:
                    return ToolExecutionResult(
                        success=False,
                        message="No analysis results found. Please run the complete analysis first."
                    )
                    
            except Exception as e:
                return ToolExecutionResult(
                    success=False,
                    message=f"Error accessing analysis results: {str(e)}"
                )
            
            # Check for required columns
            has_composite = 'composite_score' in gdf.columns and gdf['composite_score'].notna().any()
            has_pca = 'pca_score' in gdf.columns and gdf['pca_score'].notna().any()
            
            if not has_composite and not has_pca:
                return ToolExecutionResult(
                    success=False,
                    message="No analysis results found. Please run composite or PCA analysis first."
                )
            
            # Generate comprehensive summary
            summary_parts = []
            result_data = {
                'has_composite': has_composite,
                'has_pca': has_pca,
                'total_wards': len(gdf)
            }
            
            # Header
            summary_parts.append("# üìä COMPREHENSIVE ANALYSIS RESULTS SUMMARY")
            summary_parts.append(f"**Dataset:** {len(gdf)} wards analyzed")
            summary_parts.append("")
            
            # Composite Score Analysis Results
            if has_composite:
                composite_summary, composite_data = self._generate_composite_summary(gdf, top_n)
                summary_parts.extend(composite_summary)
                result_data['composite_results'] = composite_data
            
            # PCA Analysis Results
            if has_pca:
                pca_summary, pca_data = self._generate_pca_summary(gdf, top_n)
                summary_parts.extend(pca_summary)
                result_data['pca_results'] = pca_data
            
            # Method Comparison
            if include_comparison and has_composite and has_pca:
                comparison_summary, comparison_data = self._generate_method_comparison(gdf, top_n)
                summary_parts.extend(comparison_summary)
                result_data['method_comparison'] = comparison_data
            
            # Risk Level Distribution
            distribution_summary = self._generate_risk_distribution(gdf)
            summary_parts.extend(distribution_summary)
            
            # Join all parts
            full_summary = "\n".join(summary_parts)
            
            return ToolExecutionResult(
                success=True,
                message=full_summary,
                data=result_data,
                metadata={
                    'summary_type': 'comprehensive_analysis',
                    'methods_included': [m for m in ['composite', 'pca'] if result_data.get(f'has_{m}')],
                    'wards_analyzed': len(gdf),
                    'top_n_shown': top_n
                }
            )
            
        except Exception as e:
            logger.error(f"Error generating comprehensive analysis summary: {e}", exc_info=True)
            return ToolExecutionResult(
                success=False,
                message=f"Error generating analysis summary: {str(e)}",
                error_details=str(e)
            )
    
    def _generate_composite_summary(self, gdf, top_n):
        """Generate composite score analysis summary"""
        summary_parts = []
        
        # Sort by composite score (descending - higher is more vulnerable)
        composite_sorted = gdf.sort_values('composite_score', ascending=False)
        top_composite = composite_sorted.head(top_n)
        
        # Calculate statistics
        scores = gdf['composite_score'].dropna()
        mean_score = scores.mean()
        median_score = scores.median()
        std_score = scores.std()
        
        summary_parts.append("## üéØ Composite Score Analysis Results")
        summary_parts.append(f"**Method:** Multi-factor risk scoring")
        summary_parts.append(f"**Score Range:** {scores.min():.3f} - {scores.max():.3f}")
        summary_parts.append(f"**Mean Score:** {mean_score:.3f} ¬± {std_score:.3f}")
        summary_parts.append("")
        
        summary_parts.append(f"**Top {top_n} Most Vulnerable Wards:**")
        for idx, (_, ward) in enumerate(top_composite.iterrows(), 1):
            ward_name = ward['WardName']
            score = ward['composite_score']
            rank = idx
            category = ward.get('composite_category', ward.get('vulnerability_category', 'Unknown'))
            
            summary_parts.append(f"{idx:2d}. **{ward_name}** - Score: {score:.3f} (Rank: {rank}/{len(gdf)}) - *{category} Risk*")
        
        summary_parts.append("")
        
        # Prepare data for return
        composite_data = {
            'method': 'composite_score',
            'statistics': {
                'mean': float(mean_score),
                'median': float(median_score),
                'std': float(std_score),
                'min': float(scores.min()),
                'max': float(scores.max())
            },
            'top_wards': [
                {
                    'rank': idx,
                    'ward_name': ward['WardName'],
                    'score': float(ward['composite_score']),
                    'category': ward.get('composite_category', ward.get('vulnerability_category', 'Unknown'))
                }
                for idx, (_, ward) in enumerate(top_composite.iterrows(), 1)
            ]
        }
        
        return summary_parts, composite_data
    
    def _generate_pca_summary(self, gdf, top_n):
        """Generate PCA analysis summary"""
        summary_parts = []
        
        # Sort by PCA score (descending - higher is more vulnerable)
        pca_sorted = gdf.sort_values('pca_score', ascending=False)
        top_pca = pca_sorted.head(top_n)
        
        # Calculate statistics
        scores = gdf['pca_score'].dropna()
        mean_score = scores.mean()
        median_score = scores.median()
        std_score = scores.std()
        
        summary_parts.append("## üî¨ PCA Analysis Results")
        summary_parts.append(f"**Method:** Principal Component Analysis")
        summary_parts.append(f"**Score Range:** {scores.min():.3f} - {scores.max():.3f}")
        summary_parts.append(f"**Mean Score:** {mean_score:.3f} ¬± {std_score:.3f}")
        summary_parts.append("")
        
        summary_parts.append(f"**Top {top_n} Most Vulnerable Wards:**")
        for idx, (_, ward) in enumerate(top_pca.iterrows(), 1):
            ward_name = ward['WardName']
            score = ward['pca_score']
            rank = idx
            category = ward.get('pca_category', ward.get('vulnerability_category', 'Unknown'))
            
            summary_parts.append(f"{idx:2d}. **{ward_name}** - Score: {score:.3f} (Rank: {rank}/{len(gdf)}) - *{category} Risk*")
        
        summary_parts.append("")
        
        # Prepare data for return
        pca_data = {
            'method': 'pca',
            'statistics': {
                'mean': float(mean_score),
                'median': float(median_score),
                'std': float(std_score),
                'min': float(scores.min()),
                'max': float(scores.max())
            },
            'top_wards': [
                {
                    'rank': idx,
                    'ward_name': ward['WardName'],
                    'score': float(ward['pca_score']),
                    'category': ward.get('pca_category', ward.get('vulnerability_category', 'Unknown'))
                }
                for idx, (_, ward) in enumerate(top_pca.iterrows(), 1)
            ]
        }
        
        return summary_parts, pca_data
    
    def _generate_method_comparison(self, gdf, top_n):
        """Generate method comparison analysis"""
        summary_parts = []
        
        # Get top wards from both methods
        composite_top = set(gdf.nlargest(top_n, 'composite_score')['WardName'].tolist())
        pca_top = set(gdf.nlargest(top_n, 'pca_score')['WardName'].tolist())
        
        # Find consensus and disagreements
        consensus_wards = composite_top.intersection(pca_top)
        composite_only = composite_top - pca_top
        pca_only = pca_top - composite_top
        
        # Calculate agreement rate
        agreement_rate = (len(consensus_wards) / top_n) * 100
        
        summary_parts.append("## ‚öñÔ∏è Method Comparison Analysis")
        summary_parts.append(f"**Agreement Rate:** {agreement_rate:.1f}% ({len(consensus_wards)}/{top_n} wards)")
        summary_parts.append("")
        
        if consensus_wards:
            summary_parts.append(f"**ü§ù Consensus High-Risk Wards** ({len(consensus_wards)} wards):")
            for ward in sorted(consensus_wards):
                ward_data = gdf[gdf['WardName'] == ward].iloc[0]
                comp_score = ward_data['composite_score']
                pca_score = ward_data['pca_score']
                summary_parts.append(f"‚Ä¢ **{ward}** - Composite: {comp_score:.3f}, PCA: {pca_score:.3f}")
            summary_parts.append("")
        
        if composite_only:
            summary_parts.append(f"**üìä High Risk in Composite Only** ({len(composite_only)} wards):")
            for ward in sorted(composite_only):
                ward_data = gdf[gdf['WardName'] == ward].iloc[0]
                comp_rank = (gdf['composite_score'] >= ward_data['composite_score']).sum()
                pca_rank = (gdf['pca_score'] >= ward_data['pca_score']).sum()
                summary_parts.append(f"‚Ä¢ **{ward}** - Composite Rank: {comp_rank}, PCA Rank: {pca_rank}")
            summary_parts.append("")
        
        if pca_only:
            summary_parts.append(f"**üî¨ High Risk in PCA Only** ({len(pca_only)} wards):")
            for ward in sorted(pca_only):
                ward_data = gdf[gdf['WardName'] == ward].iloc[0]
                comp_rank = (gdf['composite_score'] >= ward_data['composite_score']).sum()
                pca_rank = (gdf['pca_score'] >= ward_data['pca_score']).sum()
                summary_parts.append(f"‚Ä¢ **{ward}** - Composite Rank: {comp_rank}, PCA Rank: {pca_rank}")
            summary_parts.append("")
        
        # Method insights
        summary_parts.append("**üîç Method Insights:**")
        summary_parts.append("‚Ä¢ **Composite Score**: Multi-factor weighted risk assessment")
        summary_parts.append("‚Ä¢ **PCA Score**: Pattern-based vulnerability detection")
        summary_parts.append("‚Ä¢ **Low Agreement**: Indicates methods capture different risk aspects")
        summary_parts.append("‚Ä¢ **High Agreement**: Suggests strong vulnerability consensus")
        summary_parts.append("")
        
        # Prepare data for return
        comparison_data = {
            'agreement_rate': float(agreement_rate),
            'consensus_wards': list(consensus_wards),
            'composite_only_wards': list(composite_only),
            'pca_only_wards': list(pca_only),
            'total_compared': top_n
        }
        
        return summary_parts, comparison_data
    
    def _generate_risk_distribution(self, gdf):
        """Generate risk level distribution summary"""
        summary_parts = []
        
        summary_parts.append("## üìà Risk Level Distribution")
        
        # Check for risk categories
        risk_columns = ['vulnerability_category', 'composite_category', 'pca_category']
        risk_column = None
        
        for col in risk_columns:
            if col in gdf.columns and gdf[col].notna().any():
                risk_column = col
                break
        
        if risk_column:
            risk_counts = gdf[risk_column].value_counts()
            total_wards = len(gdf)
            
            for risk_level, count in risk_counts.items():
                percentage = (count / total_wards) * 100
                summary_parts.append(f"‚Ä¢ **{risk_level} Risk**: {count} wards ({percentage:.1f}%)")
        else:
            summary_parts.append("‚Ä¢ Risk categories not available in current dataset")
        
        summary_parts.append("")
        
        return summary_parts


