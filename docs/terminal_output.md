Eugenie Y. Lai
Logo
Contact: eylai [at] mit.edu
GitHub: ey-l
Twitter: @EugenieLai
CV, transcript

News [More Updates]
2021.04 Joining the Data Systems Group (DSG) at MIT EECS CSAIL as a PhD student in Fall '21.

Theme by orderedlist

Another Annotated Example: CS PhD Statement of Purpose
Date: 2021-04-22

This post is inspired by the Statement of Objective examples provided by the MIT EECS Communication Lab. Some programs (e.g., Berkeley EECS) require a Statement of Purpose (SoP) and a Personal Statement (PS). In this post, we will dissect and annotate my SoP submitted to MIT EECS, which is a hybrid of both, in my case.

I also shamelessly include a copy of my first draft for a before-after comparison and show how far I have (and maybe anyone could) come by applying the learnings discussed in this post. It would be difficult to measure the impact of something without showing the starting point, which is often missing in the existing resources for SoP.

Also, if you are an anxious applicant, let’s not compare ourselves. I know it’s easier said than done, and I still fall into that trap too. But it is unfair to compare the ins and outs of ourselves to only the best side of others (e.g., their SoP). This note was first brought up to me by Dr.* Maria De-Arteaga back in 2019 when I just started to pivot my profile towards grad school and has been helping me get off the overthinking treadmill since.

*Note: I heard Prof. and Dr. are distinct in the states, but we use Dr. for both in Canada. I didn’t know until the visit days and only used Dr. in my SoP. So let’s use Dr. in this post to keep everything consistent.

Before We Start
Intended audience: Future and current CS PhD applicants.

The role of SoP in grad admissions: Touched on by this Twitter thread, which could be specific to MIT EECS.

My result: I applied to 8 programs and was fortunate to get in almost everywhere, with 5 offers (i.e., Berkeley, MIT, UBC, UMichigan, and UWashington) and 3 withdrawals (i.e., Columbia, Maryland, and NYU).

A non-exhaustive list of caveats that may make this post not as applicable so readers’ discretion is advised:

I applied during a pandemic, not sure how that affected my experience.
I only applied to programs in the US and Canada so the experience is subjective to the system here.
I’m in data systems so please question if the content is field-specific.
Motivation to (uncomfortably) put myself out there:

Writing crystallizes my learning and is a skill I’m working on. The pressure helps me practice.
The potential of helping someone trumps the fear of being judged.
I received so so much help and support throughout this grad application cycle and just want to give back.
Speaking of support, a list of direct help I received for my SoP:

I was privileged to be assisted by the MIT GAAP and Berkeley EAAA program. A huge thank you to my MIT GAAP mentor, Xuan, who chatted with me bi-weekly since October 2020, provided extremely valuable feedback on my SoP, and helped me navigate through the applying process. It still feels surreal that I will actually meet her in person at MIT this fall.
Many thanks to my labmates and my research parent, Dr. Rachel Pottinger, who offered both helpful comments and emotional support.
I happened to take COMM 395 Business Communication with Elizabeth Bowker that term (to fulfill my undergraduate degree requirement at UBC). I found some materials covered for presentations transferable to writing, and a big thank you to her for the additional support.
I was also privileged to get help from the Writing Centre and Student Service Centre at my undergraduate university.
Just to reiterate, the examples by the MIT EECS Communication Lab are particularly helpful. Many other schools/programs (e.g., CMU) provide such examples too.
I was also influenced by these YouTube channels: Lillian - AlphaGenesis, Casey Fiesler, The Kath Path, Andy Stapleton.
So the first takeaway is clear: Be resourceful. Ask around. Keep an eye out for opportunities and resources, which shouldn’t take up much energy. Just have that running in the background.

Overall Thought Process
The grad application as a whole is supposed to show a 3D us to let the committee make a sensible decision. If we think backwards, SoP is only included for a reason. Like any member of a K-Pop group, it has a unique proposition in the package.

I wanted to show a 3D me by leveraging the application package with little overlaps between materials. But so far, the transcript and CV only put me into numbers and project names.

So something is missing – without showing my thought process and personality, I’m boring, cold, and flat. This gap is where SoP comes in, and it is the only* opportunity to add that third dimension. We will use sketching as an analogy and go through the things that I constantly reminded myself of when thinking about my SoP at a high level.

*Note: Letters of recommendation (LoRs) help too, but they are observations of us. SoP is the one thing in our full control.

Convey the why’s. Like all drawings, SoP needs a purpose, a main message that both utilizes the space in the application and fills the gap. From most of the resources above, the purpose should be conveying the why’s (e.g., why research, why grad school, why this subfield).

Find a common thread and tell a story about professional development. Now we know what to include, but how could we organize the content in a way that shows the reader how we think? We need a skeleton first. I could tell my why’s in a plain list. But wouldn’t some layered structure to show how my research journey evolved add more character? Inevitably, we have to repeat what’s in the CV, but the added value comes from the personality and thought process illustrated through those experiences. The experiences are just a tool at this point, instead of the main focus, so don’t worry about repeating the content.

Bring in personality. Let’s colour the black-and-white skeleton with a personal pallet. I tried to make every sentence read like something only I would write. Admittedly, bringing in a personal voice while staying professional is a fine line to walk, but it’s possible. The annotated SoP is (trying to be) an example. So is this post. We will talk about a few ways to do that in the detailed comments.

Help the reader focus. We have limited paint. Be concise and precise. Every sentence is an opportunity to draw a line and should together portrait a clean image. We don’t want to waste any bits or distract the reader with random, extra lines so every word should have its place. I also thought hard about what to leave in and leave out. Although I was involved in many things throughout my undergraduate time, I only included experiences that are pertinent to my story.

Detailed Comments by Section
There is a lot to unpack. We will walk through my reasoning for each section at a detailed level, which can also be seen as a concrete embodiment of the high-level takeaways discussed above.

Statement of Objectives

Introduction
We don’t want to be a boring person. Opening with questions grabs the reader’s attention better than the laundry list of who I am and what I do in my first draft. The questions plant seeds too, as we will see later. Opening with research interests directly is also interesting.

How can we propagate breakthroughs in the scientific community to the real world? With the explosion of big data, how can we help fields outside of computer science (CS) extract and leverage its value? Inspired by these questions, my current research focuses on facilitating user interaction with databases.

Elaborate on my current research interest with the techniques (in the method space) and a use case (in the problem space). The use case also hints at my why’s, coming soon.

Specifically, I apply visualization and machine learning techniques to alleviate the barriers between users and databases to help users access and make sense of data. By helping users better explore and understand the data they have collected, I hope to enable data-driven decision-making in a wide range of fields. It is with these broad goals in mind that I am applying to pursue a PhD.

Finding My Research Interests
This section shows two things, my technical competency and why’s. I described 3 research projects and used the reflection on the experience to answer the why’s.

+1 to the example provided by the MIT EECS Communication Lab, the formula I used to describe a research project, one line each: summary + clarification of terms if necessary + need of the work (e.g., gaps) + our contributions + outcomes + my specific input. We will see this formula twice later in this section.

With a focus on data provenance summarization, my research journey began under Dr. Rachel Pottinger at the UBC Data Management and Mining Lab. The provenance of a query over a database is a subset of the data in the database that contributed to the query answer. While comprehensive, query provenance consists of large volumes of data and hence is overwhelming for users to explore. We presented an approach to provenance exploration that builds on data summarization techniques and provides an interface to visualize the summary. This work led to the first two papers I co-authored, Summarizing Provenance of Aggregation Query Results in Relational Databases (ICDE’21) and Pastwatch: On the Usability of Provenance Data in Relational Databases (ICDE’20). My main contributions include identifying the limitations in the existing methods, implementing the existing and our summarization methods, and running the experiments.

We love the dark times. Dr. Brené Brown said vulnerability and hardships help people connect and build trust. Being rejected is my true experience, and I intentionally included that to make myself relatable to the reader. In this case, it also shows resilience and segues into my first why, why research but not industry. As you may have also noticed, this concept is used everywhere in this post too.

Our work experienced a few submissions. Although I felt discouraged at first, I learned to reflect and was encouraged by how much our work had improved after each round. I also enjoyed my experience in research more than the industry for the autonomy and ownership over my work.

But I didn’t want to just tell my why’s like a list. I envisioned a story structure inspired by The Secret Structure of Great Talks by Nancy Duarte. She introduced a shape at around 6:00. Applying that concept, I first established what is, what could be, and the gap here. Like the shape, we will see me traversing between what is and what could be in the rest of this section.

However, I had some burning questions regarding my research interests going forward. Although I was engaged by the technical aspects of solving open-ended problems, I wanted to find something that would really excite me – what is the thing that would get me out of bed every morning? And how could I find it?

Transition to my next project to show more technical competency while keeping the flow of the story.

My next project, Developing a Data-Driven Electric Vehicle (EV) Strategy in Surrey, BC, Canada, helped me answer those questions.

Another example of the formula above but in a slightly different order to make things flow better.

Working with another undergraduate student under the supervision of Dr. Raymond Ng, we set out to address the challenge of how the city of Surrey should place EV charging stations. Prior to our work, the approach to determine where to install an EV charging site was solely based on expert opinions, despite a large volume of data collected by the city of Surrey. To help city planners make strategic decisions informed by evidence, I developed a web application to give them a user-friendly way to explore and make sense of the data. I used interactive maps and graphs to visualize the spatial distribution and time trends of Surrey’s vehicle stock, traffic flows, and land use. In September 2019, the city used my tool to choose 20 charger locations for a Canadian federal funding proposal, and I was proud to co-present this work at the SIGKDD’20 Social Impact Session this summer.

Talking about our values is another good way to bring in our personality while staying professional, which also helps answer some why’s. For example, what kind of research keeps us excited? I’m excited about real-world users (in the problem space), but everyone is motivated differently. Maybe you are excited about system design? Cool! Or applying new ML models? Also cool! Note that this part also ties back to the opening questions.

Through zooming in and out on a pressing, real-world issue, I realized what I should be looking for in the research I pursue: the possibility of helping others and the insight into real-world issues that would spark that possibility. I started to envision making an impact on the real world through my research. The value of our work in the scientific community can only be actualized when our tools are adopted by downstream users such as domain experts and decision-makers. Hence alleviating user-database barriers is a vital step in advancing data-driven decision-making in a wide range of fields.

Transition to the 3rd and final project. Another piece of advice I got (for almost everything grad application related) is don’t tell, but show. Earlier I said that I’m motivated by real-world issues, and here I showed that I followed through my words with actions.

With that overarching goal in mind, I initiated a project to facilitate user interaction with databases by identifying the major stakeholders and their challenges when interacting with databases, and then mapped that to their needs.

Apply the formula again to describe the project.

Database users often interact with databases via SQL query sessions. From our analysis, users pose a variety of SQL queries in sequence with changes in SQL keywords and query fragments such as tables and attributes. However, the existing approaches only consider queries individually and make recommendations based on query similarity and popularity. We presented a new approach to recommend query information by learning from the sequential knowledge exploration patterns of historical users. We modelled our query recommendation problem as a query prediction task and used sequence-to-sequence models to predict the next query. Supervised by Dr. Pottinger, this work led to Sequence-Aware Query Recommendation Using Deep Learning, submitted to VLDB’21. As the lead researcher, I identified knowledge gaps in the existing work, defined and scoped the research problem, analyzed the workload data, implemented the deep learning models, ran the experiments, discussed the results, and wrote the paper.

Tie back to the motivation and answer why grad school to wrap up the story.

Seeing a connection between my work and the quantifiable impact gives me a rush of excitement that I am contributing to help those real-world users in need. Through this project, I found myself enjoying both scoping and solving open-ended problems and hope to further improve with additional formal training in graduate studies.

Equal Access in STEM
I added this section following the same MIT EECS Communication Lab example and used the previous formula to explain the project as well.

It may seem odd to risk the flow of a research-focused SoP and make us question if this section is even relevant. But MIT EECS doesn’t require PS, and I wanted to show what I care about and where I come from. This section is also intended to help the SoP stay professional when I touched on my personal background in the last section. Again, fine line to walk. Lastly, grad school to me is more than research. This section adds another dimension to my professional development and connects to my career pursuit in academia mentioned later.

My other goal in graduate school is to further my pursuit of advancing equal access to educational resources for students in marginalized groups. Besides mentoring young women in STEM throughout my undergraduate time, for the past year, I worked on the UBC CS Undergraduate Program Evaluation and Renewal project. In the process, I realized how my experience with data visualization and user interface design could help to improve equity in education. Degree planning is challenging and time-consuming since students have to envision their career path and go to individual course pages to ensure they meet prerequisites accordingly. First-generation college students are especially vulnerable as they lack adequate guidance from their immediate support system. To solve this problem, I designed an interactive directed graph to show the dependencies between courses, provide a holistic view of the CS program, and visualize potential academic trajectories at UBC CS. I was thrilled to present my work at the UBC Board of Governors Meeting in Spring 2020. I deployed the graphs to the UBC CS website this summer and am currently helping UBC Centre for Teaching, Learning and Technology adapt the graphs campus-wide. Participating in this project allowed me to advance equal access in a higher level of education and help as many students thrive as possible.

Future Work
This section aims to convince the reader that I know the strengths of the program, our interests align, and I’m valuable specifically to them. The first part outlines my overall research interests, while I gave specific examples about the program and PIs in the second part.

I chose to put my research statement here, not anywhere else. Up to this point, I’ve been signalling pieces about my motivation and research interests using the opening questions, projects, and my why story. The reader now has enough context and is ready for a punch.

All my experiences collectively shaped my research interests and motivated me to pursue graduate studies. Today, database systems provide a vital infrastructure to access high volumes of data in a variety of applications. Seeing the user-database barriers and the potential of data-driven decision-making in areas outside of CS (e.g., city planning and sustainability) incites my urge to build my work around the theme of facilitating user interaction with databases. With a deep understanding of the problem space and skills gained through solving problems in this space, I hope to continue this line of work by applying visualization and ML techniques to help database users access and make sense of data.

I find this part becomes more candid and compelling when I write it as if the PIs would actually read it (and mine really did). Also, it only becomes attractive when the interest goes both ways. I wanted to show how they could help me but also what unique skills I could offer.

MIT CSAIL’s past and current work indicates its members’ unique strengths on this topic. Specifically, I would be excited to work with Dr. Tim Kraska and Dr. Sam Madden. Dr. Kraska has made outstanding contributions to enabling data analytics for individuals outside of CS using ML-inspired techniques. The sequential features of query sessions discussed in his recent work, IDEBench (SIGMOD’20), are fundamental to my work on sequence-aware query recommendation, where we empirically analyzed the query sequences in two real-life workloads. Extending my work under his supervision would give me strong support in leveraging query session information using ML techniques. My research interests also greatly overlap with Dr. Madden’s work, such as Data Civilizer, on building end-to-end systems to facilitate domain experts with data exploration. I would be excited to work with Dr. Madden by bringing my skills and experience in applying ML techniques to SQL queries.

Where I See Myself
I wanted to address why I spent 6 years at UBC, which is relatively uncommon and often raises questions (e.g., if I can handle a rigorous course load). However, it was difficult to word my reason in a professional way at first. So I only briefly mentioned the personal aspect while elaborating on my work experience. Xuan pointed out the key is to relate personal struggles to professional development and helped me further emphasize the value of the experience and how it contributed to my goals in graduate studies.

As a first-generation college student from a low-income, single-parent family, working puts additional constraints on my course load yet is the most effective way to support myself. Although I spent six years on my undergraduate degree, I did two years of co-op at three different places in industry, non-profit, and academia. While studying full-time, I have also worked part-time in retail, administration, and teaching. Through these valuable experiences, I not only learned about the many real-world challenges that people face on the job, but also discovered research interests that would allow me to address some of those challenges.

Let’s not leave any loose ends and tie the two goals together to wrap up.

After graduate studies, I aim to pursue a career in academia, so that I can develop the research and tools to address these challenges and more. Furthering my education at MIT would bring me one step closer to my goal of advancing data-driven decision-making in a wide range of fields and improving equal access to educational resources for students like me in marginalized groups.

Other Takeaways
I also learned and applied these general/minor things.

Just start writing. It is an iterative process. The first draft is the hardest and almost guaranteed to suck, but it gets our brain going. It gets a lot easier once we gain the momentum and just have to make incremental changes.

Start early, which goes hand-in-hand with the last point. I wanted to leave ample time for that interactive process, finished my first draft in late August, and finalized it in the first week of December 2020. I feel grateful that I took the time to reflect on my why’s, which also came in handy later in the (quite intense) interview process in January 2021.

Don’t stress too much about tailoring the SoP to each program. Partial thanks to SIGMOD ‘20*, I had a general research direction when applying. The programs and labs I applied to may have nuances in their research interests and strengths, but my motivation, research interests, and skillsets didn’t need to change much. I only swapped out the second half of the future work section for each program. However, someone with a broader interest and a more diverse set of programs may want to customize the SoP more and have different answers for each why depending on the program.

*Note: More on my experience at SIGMOD ‘20.

Read each program’s prompts and formatting requirements carefully. The point above is about the content, while this one is about the format and separation of the content. Programs like Berkeley EECS require an SoP and a PS so the separation depends on the prompts. I include my final copy of SoP and PS to Berkeley to show how I did it with minimum additional effort, which also helps illustrate the point above. The formatting requirements all have slight differences (e.g., word limits, header, title) so just be aware.

Choose what feedback and advice to take in. Going back to the point of being resourceful, we may later find ourselves getting various or even conflicted advice from different sources, which can be confusing and overwhelming. My apologies if this post is making it worse. But I always ask two questions whenever I get advice from people:

Do they know the field?
Do they know me?
Although some advice is generalizable, this sanity check is a reminder to further verify if the information is credible and applicable to me, especially when I get negative (but not necessarily constructive) feedback.

An extreme example is the words from my relatives and family friends when they laughed at my school list. It still hurt at the moment, but the rational me didn’t take their comments to heart because they’re not in CS, and they don’t know my profile. More than a filter to allow in helpful advice, the questions are also shields to protect us, much needed in such a sensitive time.

Last Words
Through the applying process, I had countless breakdowns moments where I felt that I had already tried everything, but my SoP just read shallow, and my writing would never be good enough.

But it’s because SoP is hard to write!! It not only demands writing techniques but also deep reflections of the why’s from our experiences. Although writing the SoP challenged me hard on both fronts, I’m glad that I took the time and saw it as an opportunity to grow: It reminded me that improving my writing is a never-ending process, and the reflection indeed made me question my life but also assured my decision to pursue graduate studies.

However, I do want to acknowledge that not everyone has the privilege to afford the time and energy. Further, if we consider our individual profile (e.g., GPA, LoRs) as a whole, pouring our limited resources into SoP alone may not be a strategic move. Nothing is perfect nor needs to be. So knowing when to say good enough is an important skill too (which is something I still need to work on).

Lastly, taking one step further, I find some of the takeaways transferable to other written pieces (e.g., papers), other forms of communication (e.g., presentations), or professional development in general.

Although I had much fun reflecting on my learning, I genuinely hope this post would be somewhat helpful to at least one other person on the planet, and very best of luck if you are applying soon!! <3

Back to blog




Honam Wong Application to CIS PhD Program at UPenn
Academic Statement of Purpose
While recent breakthroughs in deep learning and LLM have enabled widespread applications,
the principle behind their success remains mysterious. I aspire to understand the mechanisms
of modern deep-learning algorithms via the following approaches:
1. Theoretical: Building theories to deliver provable insights into understanding their behaviors. For instance: How can we understand the scaling law of
model’s performance with model size, data size, and amount of compute? Why do
over-parametrized neural networks generalize well despite perfect fit into noisy data
(i.e., benign overfitting)?
2. Empirical: Commercial LLMs often suffer from data contamination issues, making it
difficult to objectively measure and understand their behavior. Training language
models in a simplified and idealized setup with controlled synthetic data
can help scientifically investigate factors that affect the performance of the models and
understand their inner workings through mechanistic analysis [1].
Ensemble Methods on Out-of-distribution generalization I began my research supervised by Prof. Tong Zhang on the generalization of deep learning algorithms under
distribution shift, and found that ensemble methods on neural networks have been shown
to achieve superior out-of-distribution (OOD) generalization performance. To uncover the
underlying mechanism, we explained through the lens of invariant and spurious features,
which can be elegantly described using mathematical languages. Neural networks generalize
by acquiring invariant features that remain stable across environments, but they still suffer
from spurious correlations, which can significantly degrade performance under distribution
shift. Previous methods like Invariant Risk Minimization [2] were often proposed to maintain
invariant features and discard spurious features to improve OOD generalization. Contrary
to prior methods that focus on learning invariant features, I conducted theoretical analysis
on the simplified setup to discover that the ensemble method incorporates diverse spurious
features, which can weaken the individual contributions of spurious features while enhancing
the contributions of invariant features, resulting in improved generalization. Additionally,
when considering overlapped invariant features in different models, I found that the invariant
features’ signals would be further amplified in the weight space ensemble method compared
with the output space ensemble, effectively explaining the former’s superiority in practice [3].
This coauthored work was published at ICLR 2024 [4], and I learned that mathematical
theories could provide a precise characterization of algorithms’ behavior and deliver provable
insights, which I was eager to explore more.
Benign Overfitting in Physics-informed learning Subsequently, I collaborated with
Prof. Yiping Lu at Northwestern University on understanding the generalization properties of physics-informed learning. With advances in AI4Science, physics-informed neural
networks (PINN) have been proposed to reconstruct functions from observational data that
adhere to physical laws using deep learning methods. In particular, these efforts focus on addressing the inverse problem, which is widespread across science and engineering. However,
it remains unclear how the inverse problem’s structure and the model’s inductive bias on
smoothness affect its generalization performance under noisy observations. To understand
this theoretically, I applied kernel methods [5] to model the behavior of over-parametrized
Honam Wong Application to CIS PhD Program at UPenn
neural networks, and used Sobolev norm to characterize the imposed inductive bias. By
analyzing the bias and variance of the kernel estimator, I discovered novel results that incorporating sufficiently smooth inductive bias is necessary to attain optimal performance.
And intriguingly the inverse problem operator stabilizes the variance and exhibits benign
overfitting even in fixed-dimension. This is surprising since benign overfitting typically only
occurs in high dimensions. I verified these theoretical findings on PINN, and I am delighted
to see that our work deepens our understanding of physics-informed learning. This work
leads to a first-authored paper under review at ICLR 2025 [6].
I developed solid skills in theoretical analysis throughout these projects. However, I realized such analysis always requires strong assumptions and sometimes fails to capture the
actual behavior. For example, I found that while kernel methods have been popular for modeling the behavior of over-parametrized neural nets, they fail to capture the feature learning
process [7]. I also read recent literature on Transformer’s ability to implement certain algorithms in context, which is beneficial to understand power of Transformer architecture [8].
However, the main drawback is that the learnability proof mostly relies on manual weight
constructions, which may not correspond to the solution learned in actual training. Therefore, I felt the necessity of adopting an empirically driven approach to studying modern deep
learning phenomena and understanding how models learn in practice. Inspired by AllenZhu’s talk on the physics of Language models [1], I became interested in monitoring the
behavior of small models in an idealized environment, in which tweaking hyperparameters
and conducting controlled experiments is feasible to gain insights into their behaviors.
Compositional Mechanism of Representation in Transformer To explore empirical
approaches, my ongoing project with Prof. Wei Hu at University of Michigan is to study
the compositional mechanism of Transformer. Human cognitive abilities can generalize to
unseen scenarios through the composition of primitive representations that constitute the
world. However, I observed that it remains unknown when and how Transformer can compose
pretrained representations to tackle unseen tasks. By conducting controlled experiments
on the number of compositions, I found that the risk on unseen tasks improved with the
former, which indicates the evidence of ability improvements of Transformer in manipulating
representations. I am curious whether this is an emergent ability that depends on a specific
threshold of the number of compositions. To investigate this phenomenon deeply, I am
analyzing its training dynamics to understand how Transformer learns representation and
when it gains the ability for compositions [9]. Besides, I am applying mechanistic analysis
to unveil the inner mechanisms: By designing sparse autoencoders and probing experiments, I
am studying how representations evolve across Transformer’s layers. This research experience
is invaluable for me as a theorist since it offers me a new perspective on empirical and
mechanistic explorations, which I plan to step further during my graduate studies.
Future research goals These experiences have shaped my passion and determination into
advancing understanding of modern deep learning paradigm through theoretical
and empirical lens. I strongly believe that the CIS PhD Program at University of
Pennsylvania is the ideal place to conduct excellent research. I want to work with Surbhi
Goel on deep learning theory, Hamed Hassani on statistical learning theory, and I want
to work with Aaron Roth on learning with guarantees. After finishing my PhD studies, I
want to stay in academia to continue my contribution to this area.
Honam Wong Application to CIS PhD Program at UPenn
References
[1] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, learning hierarchical language structures, 2024.
[2] Martin Arjovsky, L´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk
minimization, 2020.
[3] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca
Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong,
and Ludwig Schmidt. Robust fine-tuning of zero-shot models, 2022.
[4] Yong Lin*, Lu Tan*, Yifan Hao*, Ho Nam Wong, Hanze Dong, Weizhong Zhang, Yujiu
Yang, and Tong Zhang. Spurious feature diversification improves out-of-distribution
generalization. In The Twelfth International Conference on Learning Representations,
2024.
[5] Daniel Barzilai and Ohad Shamir. Generalization in kernel regression under realistic
assumptions. In Forty-first International Conference on Machine Learning, 2024.
[6] Honam Wong, Wendao Wu, Fanghui Liu, and Yiping Lu. Benign overfitting in fixed
dimension via physics-informed learning with smooth inductive bias, 2024.
[7] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature
and lazy training in deep neural networks. Journal of Statistical Mechanics: Theory and
Experiment, 2020(11):113301, November 2020.
[8] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In Thirty-seventh
Conference on Neural Information Processing Systems, 2023.
[9] Anonymous. Dynamics of concept learning and compositional generalization. In
Submitted to The Thirteenth International Conference on Learning Representations,
2024. under review.


Statement of Purpose - Jeffrey Cheng
I aim to conduct research addressing the transparency and accessibility issues of large language models (LLMs) – characterizing aspects of closed industry models and ensuring models and innovations remain computationally tractable. My existing work in this area, on examining knowledge cutoffs, has been honored by an Outstanding Paper award at the inaugural Conference on Language Modeling. As a PhD student at Princeton, I will continue to pursue a research agenda focusing on the relationship between pretraining data and model behavior and efficiency-reasoning tradeoffs during inference. 

Pretraining Data and Transparency
Understanding the relationship between pretraining data and model behavior is crucial for improving LLMs in areas such as continued learning and memorization. Because pretraining datasets for closed models are often withheld for competitive and legal reasons, I am interested in reducing the transparency gap between open and closed models by measuring these relationships under the closed data setting. For my first NLP research project, I investigated the effects of data on the temporal alignment of various LLMs’ domain knowledge. 

In place of pretraining datasets, model providers often report a knowledge cutoff date. Logically, the model’s knowledge should be aligned to this cutoff. To test this assumption, we constructed time-varying datasets consisting of monthly versions of Wikipedia documents. We identified the model's alignment as the dates when the perplexity of the versions was minimized, and observed drastic misalignments between the model’s knowledge and the reported cutoff, often by periods of many years [1]. To verify our results, we indexed over four trillions tokens from open datasets and showed correlations between the distribution of versions in the datasets and the measured alignment of models trained on them. Though base LLMs are often augmented by retrieval systems, these temporal misalignments are still important considerations when resolving knowledge conflicts between parametric and nonparametric knowledge. 

Motivated by the results of this project, I am considering various future research directions, one of which is the effect of data order. Recent studies have shown that data quality matters during the cooldown phase of pretraining [2]; would it be possible to amend the knowledge misalignments by performing continued pretraining on recent versions? Another related direction is how to determine the text inside pretraining datasets. Even when the datasets are open, efficient querying requires the use of data structures such as bloom filters [3] or suffix arrays [4]. I am more interested in methods for the closed data setting such as membership inference attacks [5, 6] and adversarial prompting techniques [7].

Even though our method did not require access to a model’s underlying pretraining data to determine knowledge alignments, our results would not have been verifiable without open pretraining datasets. What other fundamental issues of LLMs remain undetected due to the lack of data transparency? In addition to providing me with a deeper understanding of pretraining pipelines and how to efficiently work with large amounts of data, this project reaffirmed my commitment to conduct research advocating for open science.



Efficient Language Modeling and Accessibility
LLMs perform better on reasoning tasks when prompted to “think step by step” [8]. However, these performance gains come at the cost of increased decoding latency due to the need to generate the tokens in the reasoning chain. Generating these extra tokens is expensive; the recent o1 reasoning traces provided by OpenAI are hundreds of times longer than the initial queries. Moreover, processing these reasoning chains often require prohibitively large amounts of memory due to the quadratic complexity of attention mechanisms. My second research project focused on making reasoning more accessible in models by investigating methods to generate compressed representations of the reasoning chains.

Given a query, we obtained its reasoning chain and passed it through the model to select a learned subset of the hidden states to be its compressed representation. Treating the compressed representations as gold labels, we trained an adapter through teacher-forcing to autoregressively generate them. We then decoded the answer by conditioning on the unmodified query and generated representations. Our work shows that we can view reasoning through the lens of an efficiency-performance tradeoff. Rather than choosing between directly decoding an answer or generating the full reasoning chain, we can adaptively choose a desired balance by controlling the size of the selected subset [9].

This project inspires future research interests. Our project focused on generating representations that are a priori unknown; however, I am also interested in the many lines of work focusing on compressing known context [10, 11, 12]. Another interest of mine involves reasoning with embeddings and knowledge distillation [13, 14], as the compressed representations in this project constitute reasoning in continuous space. To this end, I am currently pursuing a project in improving the efficiency of character-based language models by clustering kernelized character embeddings. 

I believe techniques that balance performance and efficiency are important considerations for NLP research. They allow resource-limited devices to effectively allocate computational power and also enable scaling to larger datasets and models without needing additional hardware. I aim to continue doing research to make models more accessible to users and researchers without a large resource budget.

PhD at Princeton and Beyond	

I am excited about the opportunity to join the Princeton CS program as a PhD student. I would like to work with Prof. Danqi Chen on long-context modeling as well as data attribution. These areas are closely related to my interests in LLM efficiency. Prof. Karthik Narasimhan’s research in reasoning and efficient generation also intrigues me. Lastly, I am interested in Prof. Sanjeev Arora’s work in LLM reasoning and data-efficient finetuning.

Ultimately, my goals are to work as a research scientist in industry or as a PI in an academic lab. With every release of a new LLM, I see these technologies becoming more widespread. Doing research with the potential for global impacts excites me. I want to be a part of the group ushering in the transition to the AI age – a transition not driven by profit, but instead by scientific curiosity and the common good. Pursuing a PhD at Princeton would be a tremendous next step in pursuing my goals.
References

[1] Cheng, J., Marone, M., Weller, O., Lawrie, D., Khashabi, D., and Van Durme, B. “Dated Data: Tracing Knowledge Cutoffs in Large Language Models.” Conference on Language Modeling, 2024.

[2] Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K.R., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J.D., Muennighoff, N., Naik, A., Nam, C., Peters, M.E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L.S., Dodge, J., Lo, K., Soldaini, L., Smith, N.A., and Hajishirzi, H. “OLMo: Accelerating the Science of Language Models.” ACL, 2024.

[3] Marone, M., & Van Durme, B. “Data Portraits: Recording Foundation Model Training Data.” NeurIPS, 2023

[4] Liu, J., Min, S., Zettlemoyer, L.S., Choi, Y., & Hajishirzi, H. “Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens.” Conference on Language Modeling, 2024.

[5] Duan, M., Suri, A., Mireshghallah, N., Min, S., Shi, W., Zettlemoyer, L.S., Tsvetkov, Y., Choi, Y., Evans, D., & Hajishirzi, H. Do Membership Inference Attacks Work on Large Language Models? Conference on Language Modeling, 2024.

[6] Mozaffari, H., & Marathe, V.J. (2024). “Semantic Membership Inference Attack against Large Language Models.” NeurIPS, 2024.

[7] Carlini, N., Tramèr, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T.B., Song, D.X., Erlingsson, Ú., Oprea, A., & Raffel, C. “Extracting Training Data from Large Language Models.” USENIX Security Symposium, 2024.

[8] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.H., Xia, F., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS, 2022.

[9] Cheng, J., & Van Durme, B. “Compressed Chain of Thought: Efficient and Adaptive Reasoning through Dense Representations.” Preprint

[10] Qin, G., Rosset, C., Chau, E.C., Rao, N., & Van Durme, B. “Dodo: Dynamic Contextual Compression for Decoder-only LMs.” ACL, 2023

[11] Kumari, L., Wang, S., Zhou, T., Sarda, N., Rowe, A., & Bilmes, J. “BumbleBee : Dynamic KV-Cache Streaming Submodular Summarization for Infinite-Context Transformers.” Conference on Language Modeling, 2024

[12] Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Ré, C., Barrett, C.W., Wang, Z., & Chen, B. “H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.” NeurIPS, 2023.

[13] Deng, Y., Prasad, K., Fernandez, R., Smolensky, P., Chaudhary, V., & Shieber, S. “Implicit Chain of Thought Reasoning via Knowledge Distillation.” ArXiv, abs/2311.01460, 2024.

[14] Teehan, R., Lake, B., & Ren, M. (2024). CoLLEGe: Concept Embedding Generation for Large Language Models. arXiv preprint arXiv:2403.15362.



Statement of Purpose Ananya Harsh Jha 
Research Interests 
My primary research interest is efficient machine learning, as it helps to democratize access to research artifacts. Billion-parameter models have been relatively successful in modeling massive data distributions scraped from the internet. However, several efficiency concerns are overlooked when scaling up these methods during training. Then, for any application, models need to be compressed before deployment, personalized toward end-users, and continuously updated to prevent distribution drift. Challenges within this pipeline and a growing trend away from open research prevent artifacts from being easily accessible. Thus, I want to work on one of the following efficiency themes during my Ph.D.: 
• model and sample efficiency 
• model compression for inference efficiency 
• efficient model updates for distribution drift and personalization 
OLMo I am fortunate to be a part of OLMo, an open science effort to train a large language model (LLM) at the Allen Institute for AI (AI2). Being actively involved in modeling discussions, running experiments, and writing the online zero-shot evaluation suite for the project has helped me learn about model architectures, training dynamics and stability, and the brittle nature of NLP evaluation, all of which are useful in asking the right research questions. 
From here, my efficiency goal is stable and commonly adopted low-precision training. Some initial works [19, 12, 16] have already explored this direction with 8-bit precision training, but many open ques tions remain. One such question is why specific training components like optimizer states or gradient synchronization require higher precision representation when stochasticity has been shown to benefit generalization [17]. To solve this problem, I want to analyze the signal-to-noise ratio in a low-bit gradi ent synchronization setting and develop methods to compensate if noise levels reach a point of training divergence [7]. 
In the long term, I want to move from low-bit floating point operations to low-precision training in the integer space. The goal here will be to design new optimizer update rules and gradient propagation methods. Once commonly adopted, stable low-precision training will allow researchers to work on ideas currently not feasible in a resource-limited setting, thus pushing the needle of model efficiency research. 
Model compression After pretraining, large models power applications via APIs and thus need to be deployed on inference servers with throughput and memory considerations. While the commonly available models today are autoregressive and trained on massive amounts of data, the model compression literature primarily focuses on BERT-style encoder models trained on a limited token budget. In Jha et al. [8], we propose one of the first comprehensive studies of model compression for autoregressive models trained on a large pretraining corpus on a compute vs. performance curve, finding new trends for existing compression methods in the setup of modern LLMs. Overall, the broad scope of our work opens up many new short and long-term research questions on compression efficiency, sample efficiency and inference efficiency. 
We analyze trade-offs between compression and inference efficiency across different strategies for model pruning. Comparisons against state-of-the-art [18] pruning strategies identify unstructured pruning as sample-efficient and structured pruning as inference-efficient. In addition, many recent results for com pression in smaller data regimes, like combining distillation with pruning [10], do not extend to the large data regime and the evaluation setup of modern LLMs, which we define in our work. As a next step, I want to make structured pruning more sample-efficient via influence functions [6] or the Rho-loss [15]. In the long term, I want to understand from a theoretical perspective why learning from data is better than learning from a teacher via distillation when a vast amount of data is available. 
We continue to train pruned models for an extended token budget and compare their loss curves to that of a randomly initialized model of the same size. This comparison demonstrates an optima shift for the 
ananyahjha93@gmail.com ananyahjha93.github.io
pruned models, where we initially observe them following a monotonically decreasing training dynamic, diverge for a bit, and then start following the loss curve of the randomly initialized model. I want to study this behavior from the perspective of the lottery ticket hypothesis [4], where we observe a non-winning ticket transition to a winning ticket with extended training. Furthermore, from the lens of compression efficiency, I want to explore mode connectivity [5] to design pruning strategies that lead to a faster optima shift. Finally, the last conclusion from our work is about the prunability of attention outputs that makes us believe we can further speed up flash-attention [3] for inference via sparse-kernels. 
Retrieval models Retrieval-augmented models have been shown to reduce hallucinations [1], improve performance in the long tail [11], and work with protected data [13]. However, the use of retrieval for efficient generation, continuously updating models, and personalization is under-explored. To achieve this, I want to work on models that can retrieve long phrases per timestep and retrieve from multiple data stores in parallel without explicitly training on their data distribution. 
I have recently started working on an efficient retrieval-only causal text generator, expanding on the ideas of retrieval-only infilling from NPM [14] and long-phrase retrieval-based generations from COG [9]. I plan to use a suffix automaton [2] to scale the contrastive loss for better quality long-phrase generations per timestep than previous methods. 
In the future, I want to pursue research on retrieving from multiple data stores in parallel. This direction ties up language models with ideas in classical information retrieval. This framework will allow for efficiency by parallelizing retrieval, end-user personalization by providing an abstraction to re-rank retrieved candidates from different data sources, and continuous model updates to prevent distribution drift by hot-swapping data stores. 
Why a PhD? 
Before AI2, I worked at PyTorch Lightning, a startup creating open-source libraries to simplify model train ing. At Lightning, I co-authored TorchMetrics, an API to implement distributed metrics for research repro ducibility. Until then, my contributions to open science came from an engineering perspective, making me a confident engineer. But, the experience taught me that research and engineering go hand-in-hand in deep learning. The predoctoral stint at AI2 gave me an excellent platform to develop research ideas and build scientific rigor. Now, I am motivated to get a Ph.D. to develop my research skills further and continue contributing toward open and reproducible science from a research perspective. 
Open science is critical for democratic control of the community’s research direction by a diverse group of people to benefit everyone. However, more labs are moving towards proprietary research to benefit indi vidual entities. Working toward efficient machine learning ensures that academia remains at the forefront of research, which is one way to ensure research democratization. A Ph.D. at UW is a step toward my goal to stay in academia and be one of the voices advocating to keep research open and democratic. During my Ph.D., I am interested in working with Noah Smith and Luke Zettlemoyer on model and sample efficiency. I am also interested in working with Hannaneh Hajishirzi on retrieval models and personalization. 
After a Ph.D. I plan to become a professor and lead a research group. As a PI, I want to bring diverse perspectives into the group, ask questions, and mentor people according to a style that fits them. I want to establish a culture of scientific rigor and slow science, make claims only with the proper evidence, and not put out minimum publishable units. 
2
References 
[1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. ArXiv, abs/2310.11511, 2023. 
[2] Anselm Blumer, J. Blumer, David Haussler, Andrzej Ehrenfeucht, M. T. Chen, and Joel I. Seiferas. The smallest automaton recognizing the subwords of a text. Theor. Comput. Sci., 40:31–55, 1985. 
[3] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R’e. Flashattention: Fast and memory efficient exact attention with io-awareness. ArXiv, abs/2205.14135, 2022. 
[4] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv: Learning, 2018. 
[5] T. Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P. Vetrov, and Andrew Gordon Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. ArXiv, abs/1802.10026, 2018. 
[6] Roger Baker Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamil.e Lukovsiut.e, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Sam Bowman. Studying large language model generalization with influence functions. ArXiv, abs/2308.03296, 2023. 
[7] Kanan Gupta, Jonathan W. Siegel, and Stephan Wojtowytsch. Achieving acceleration despite very noisy gradients. ArXiv, abs/2302.05515, 2023. 
[8] Ananya Harsh Jha, Tom Sherborne, Evan Pete Walsh, Dirk Groeneveld, Emma Strubell, and Iz Beltagy. How to train your (compressed) large language model, 2023. 
[9] Tian Lan, Deng Cai, Yan Wang, Heyan Huang, and Xian-Ling Mao. Copy is all you need. ArXiv, abs/2307.06962, 2023. 
[10] Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin Yin, and Tuo Zhao. Homodistil: Homotopic task-agnostic distillation of pre-trained transformers. ArXiv, abs/2302.09632, 2023. 
[11] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memo ries. In Annual Meeting of the Association for Computational Linguistics, 2022. 
[12] Naveen Mellempudi, Sudarshan M. Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision training with 8-bit floating point. ArXiv, abs/1905.12334, 2019. 
[13] Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore. ArXiv, abs/2308.04430, 2023. 
[14] Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen tau Yih, Hannaneh Hajishirzi, and Luke Zettle moyer. Nonparametric masked language modeling. In Annual Meeting of the Association for Com putational Linguistics, 2022. 
[15] Sören Mindermann, Muhammed Razzak, Winnie Xu, Andreas Kirsch, Mrinank Sharma, Adrien Morisot, Aidan N. Gomez, Sebastian Farquhar, Janina Brauner, and Yarin Gal. Prioritized training on points that are learnable, worth learning, and not yet learnt. ArXiv, abs/2206.07137, 2022. 
[16] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng. Fp8-lm: Training fp8 large language models. ArXiv, abs/2310.18313, 2023. 
3
[17] Samuel L. Smith, Erich Elsen, and Soham De. On the generalization benefit of noise in stochastic gradient descent. In International Conference on Machine Learning, 2020. 
[18] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A simple and effective pruning approach for large language models. ArXiv, abs/2306.11695, 2023. 
[19] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and K. Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. ArXiv, abs/1812.08011, 2018. 
4


Sukwon Yun website: https://sukwonyun.github.io/ swyun@kaist.ac.kr 
Graph Neural Networks (GNNs) have emerged as powerful tools for managing graph-structured data, leading to significant advancements in algorithm design. Despite these advancements, the foun dational assumption or inductive bias inherent in GNNs remains somewhat limited, creating a notice able gap in their application across various scientific domains, each with its unique set of challenges. Driven by this motivation, my research interests primarily lie in two main directions: (1) Addressing the inductive bias in GNNs [6, 2, 8, 3, 1] and (2) their application in the field of medical and bi ological fields, specifically in single-cell RNA-sequencing (scRNA-seq) [7, 4] and tumor biology [9], as well as in other diverse areas [5]. These focus areas are at the heart of what I aim to explore under the umbrella of AI4Science during my Ph.D. studies at the University of North Carolina at Chapel Hill. The following sections detail my breakdown of AI4Science into two components: ‘AI’ and ‘4Science’. 
AI: Graph Neural Networks 
During my master’s at KAIST under Prof. Chanyoung Park, I specialized in AI, focusing on GNNs for their capability to handle non-Euclidean data and complex relationships. Despite GNNs’ popularity, I identified a significant oversight: the inductive bias in GNNs often presupposes balanced data scenar ios, contrasting real-world applications. I tackled the long-tail problem [6] in graph-structured data, where imbalanced class distribution forms a long-tailed shape. However, I discovered existing works overlooked the graph’s degree distribution, which is also long-tailed. This insight was especially crucial because, in a graph, tail-degreed (a node with few neighbors) nodes take the majority part of a graph. Driven by this motivation, I developed a GNN model addressing both class and degree long-tailedness, featuring four expert models for class and degree long-tailedness combinations. This work was the first approach that addresses both the class and degree long-tailedness and published at CIKM 2022. We extended such joint consideration viewpoint to Sequential Recommendation System [3] in terms of user sequence length and item frequency, resulting in a publication at SIGIR 2023. 
More recently, since my time as a visiting researcher at Tokyo Tech under Prof. Tsuyoshi Murata, I tackled the inductive bias of fully observed features [8] in GNNs. Efforts in the graph domain to address this bias, especially where missing features are common, have often relied on hypothesizing missing situations by manually masking parts of the feature matrix, typically lacking initial missing features. This led me to ponder the implications if the feature matrix were inherently incomplete and the graph structure not provided, as in bio-medical domains with profound missing feature issues. I found that initially missing features often lead to suboptimal kNN graphs, bottlenecking effective graph-based imputation. To address this, I explored graph-based imputation methods in the biomedical domain, finding limited generalizability. Stepping aside from directly building kNN graphs, I trained a simple MLP to extract feature gradients, leading to the creation of an initial graph structure. This approach, incorporating feature propagation and graph regeneration, aimed to enhance graph-based imputation efficacy in biomedical data. This research is under review at ICLR 2024. 
4Science: Single-cell RNA-sequencing & Tumor Biology 
Building on the AI perspective discussed above, my research direction is geared toward making sig nificant contributions to scientific fields. In my view, the key is not just to naively apply existing AI methods to longstanding scientific problems, but to identify and address significant bottlenecks within specific domains using AI. This approach involves tailoring AI solutions to effectively tackle the unique challenges of the target problem. In the scRNA-seq domain, for instance, the prominent challenge is the dropout phenomenon [7], where many genes appear unexpressed in individual cells due to low and uneven mRNA distribution, leading to a high prevalence of zeros in the cell-gene count matrix. I observed that current graph-based imputation methods targeting the dropout issue don’t adequately address the impact of zeros and non-zeros, resulting in non-zero values being disproportionately influ enced during diffusion and relying too heavily on the initial kNN graph constructed from a sparse and noisy matrix. To mitigate this, I pioneered the use of feature propagation, which preserves non-zero val ues throughout iterations, focusing on imputing zero values, and suggested refining the graph structure using this imputed matrix. This innovative approach, well-suited to the scRNA-seq domain, earned us the best paper award at ICML Workshop on Computational Biology 2023. We have further expanded this research, incorporating not just cell-cell but also gene-gene interactions [4], in collaboration with Prof. Manolis Kellis at MIT, and have submitted our manuscript to Nature Methods.
Sukwon Yun website: https://sukwonyun.github.io/ swyun@kaist.ac.kr 
Furthermore, during my internship with Dr. Tianlong Chen, an incoming professor at the Univer sity of North Carolina at Chapel Hill, I delved into analyzing tumor-related multiplexed immunofluores cence tissue images, incorporating a GNN perspective. The primary challenge was ensuring scalability and accurate phenotype prediction in each image, considering cellular heterogeneity [9]. I noticed that existing works fell short in handling such heterogeneous environments, where cells are often con nected to various other cell types, deviating from the typical homophilous assumptions of GNNs. To address this, I proposed a novel multiplex network approach, adding a cell-type network layer to fos ter a cellular assortativity on top of the geometric layer in a scalable manner using a precomputing technique. This work is currently under review at CVPR 2024. 
Why Especially UNC-CH? The University of North Carolina at Chapel Hill, known for its prestigious Computer Science program and its collaborative atmosphere, has distinguished departments such as Biology and the School of Medicine that align seamlessly with my research goal, AI4Science. I am confident that I can enhance my understanding of AI and positively contribute to scientific areas. For example, working with the incoming Professor Tianlong Chen, I am prepared to contribute to AI by moving the frontiers of GNNs and recent Large Language Models, as well as impacting scientific ar eas covering scRNA-seq, tissue phenotyping, and the analysis of protein sequences and structures. Moreover, collaborating with Professor Natalie Stanley will enable me to explore the multimodality of single-cell data, integrating scRNA-seq, scATAC-seq, and CITE-seq data seamlessly in a scalable way. 
[1] Tai Hasegawa, Sukwon Yun, Xin Liu, Yin Jun Phua, and Tsuyoshi Murata. Degnn: Dual experts graph neural network handling both edge and node feature noise. Under Review at PAKDD 2024. 
[2] Junghurn Kim*, Sukwon Yun*, and Chanyoung Park. S-mixup: Structural mixup for graph neural networks. CIKM 2023. (*: equal contribution). 
[3] Kibum Kim, Dongmin Hyun, Sukwon Yun, and Chanyoung Park. Melt: Mutual enhancement of long-tailed user and item for sequential recommendation. SIGIR 2023. 
[4] Junseok Lee*, Sukwon Yun*, Yeongmin Kim, Tianlong Chen, Manolis Kellis, and Chanyoung Park. Single-cell rna sequencing data imputation using bi-level feature propagation. Under Review at Nature Methods. (*: equal contribution). 
[5] Yunhak Oh*, Sukwon Yun*, Dongmin Hyun, Sein Kim, and Chanyoung Park. Muse: Music rec ommender system with shuffle play recommendation enhancement. CIKM 2023. (*: equal contri bution). 
[6] Sukwon Yun, Kibum Kim, Kanghoon Yoon, and Chanyoung Park. Lte4g: long-tail experts for graph neural networks. CIKM 2022. 
[7] Sukwon Yun*, Junseok Lee*, and Chanyoung Park. Single-cell rna-seq data imputation using feature propagation. ICML 2023 Workshop on Computational Biology. (*: equal contribution). 
[8] Sukwon Yun, Yunhak Oh, Junseok Lee, Xin Liu, Tsuyoshi Murata, Dongmin Hyun, Sein Kim, Tianlong Chen, and Chanyoung Park. Toward generalizability of graph-based imputation on bio medical missing data. . Under Review at ICLR 2024 Openreview link. 
[9] Sukwon Yun, Jie Peng, Chanyoung Park, and Tianlong Chen. Multiplexed immunofluorescence image analysis through an efficient multiplex network. . Under Review at CVPR 2024.


Statement of Purpose: Haotian Liu https://seanliu7081.github.io/ 
Research Interests: We are now in a new renaissance era of robotics and artificial intelligence. Consider the Large Language Model (LLM) as an example; once an academic dream swiftly transforms into a practical reality. Rather than online AI assistance like ChatGPT in a virtual environment, using robots in the real world would face significant challenges such as zero-shot generalization ability, task-level intelligence, safety constraints, and robustness against physical uncertainty. Captivated by Isaac Asimov’s robot novels from a young age, I have long pondered whether we can develop robotic systems capable of moving and interacting with humans and environments in a clever, stable, and efficient manner. 
Research Experiences: My research begins with integrating visual perception and manipulation. I first engaged in a vision-based FDM-printing project: Vision-based FDM Printing for Fabricating Airtight Soft Actuators (RoboSoft 2024 Oral Presentation)[1]. This project enhances the fabrication quality of desktop FDM printers by correcting defects during printing with a webcam. My contributions to this project were: first, propose a layer-wise image segmentation method to map the G-code contours with each layer’s captured images, and second, provide a computation efficiency solution for real-time defects segmentation using the Laplacian of Gaussian method. Ultimately, airtightness was achieved for printed soft pneumatic actuators. 
Furthermore, considering human-level vision, we can capture 3D geometrical information in the real world easily. I raised my interest in 3D point cloud tasks. A crucial prerequisite for vision-based robotics manipu lation is the availability of comprehensive data input, and I finished a paper as co-first author focused on loss function optimization for deep learning-based point cloud completion: Loss Distillation via Gradient Matching for Point Cloud Completion with Weighted Chamfer Distance (IROS 2024 Oral Presen tation)[2]. In this project, I proposed a family of CD-based losses (weighted CD) using a gradient weighting scheme to mimic the teaching neural network learning behavior and developed a bilevel optimization formula to train the backbone network based on the weighted CD loss, which needs no hyperparameter tuning. In the end, I conducted comprehensive experiments with novel networks in real (KITTI) and synthesis (ShapeNet) datasets to examine the findings. 
Beyond focusing solely on point cloud completion. I have realized the importance of high-level distributional tools to guide neural networks in learning robust representations, which are essential for solving a wide range of set-to-set matching problems and our understanding of such vision tasks. Inspired by previous work, I explored statistical learning methods and extreme value theory to identify shared distributional representations that can address distinct tasks, such as point cloud completion, zero-shot image classification, and sparse point cloud registration, which can all be identified as set-to-set matching. To this end, I finished a paper as co-first author: GPS: A Probabilistic Distributional Similarity with Gumbel Priors for Set-to Set Matching (ICLR 2025). In this project, I proposed a similarity learning framework that measures the similarity between the underlying distributions generating the sets. It is achieved by learning a Gumbel prior with minimum distances between the set items to maximize the log-likelihood. This was motivated by the observation that the distributions of minimum distances metrics, as encountered in the aforementioned tasks. Next, I formulated a reparametrizing technique based on distribution shift. Finally, I demonstrated multiple analyses; training efficiency dramatically improved compared with Optimal Transport methods in zero-shot image classification, completion results showed both visual and metrics improvements, and registration recall was boosted in sparse point cloud registration. 
The aforementioned projects developed my knowledge of deep learning, optimization, probabilistic methods, and research mindset. However, to reach the ultimate intelligence, the only study on vision tasks cannot satisfy my research ambitions. Robot learning, a complex system of interaction and reasoning with the real world like humans, came to mind. To this end, I joined Professor Robert Platt’s The Helping Hands Lab as a research intern student. My first engaged project is ”Imagination Policy (CoRL 2024)[3]” guided by Haojie Huang. In this project, my contributions are the following. First, I investigated various baseline models in robot manipulation learning. Then, I was responsible for the real robot experiments. At last, I introduced an articulate object task (open microwave) and a multi-step task (stack chairs) to show the generalization ability of our method. 
Based on the Imagination Policy, we finished a follow-up project: ”Match Policy (ICRA 2025)[4],” a plug and-play policy designed for leveraging point cloud registration methods, which needs no training after demo
Statement of Purpose: Haotian Liu https://seanliu7081.github.io/ 
collection. The ”Match” is to capture vital local geometry features and calculate the correspondence between objects. I am responsible for implementing point cloud registration methods onto real robot demos. Here are the details of this approach: First, our data inputs of manipulation objects and test environments are all represented by point clouds. Second, the problem can be described as, given point clouds Pa, Pb and Pab, where Pab shares a similar part with Pa and Pb but there is no overlapping between Pa and Pb. Our goal is to infer two rigid transformations Ta and Tb to match the configuration illustrated by Pab. To this end, such point cloud registration methods can effectively capture both local and global geometrical features and then calculate the transformations for robot manipulation, which satisfies our requirements. 
These two projects astonished me with their incredible geometrical learning abilities and promising performance in robot learning by applied equivariance property. And I will continue to study related knowledge to develop my original contribution. 
Research Vision and Role of NEU: My goal is to work in academia, contributing to foundational re search and interdisciplinary problem-solving. At NEU, I aspire to continue working on geometrical learning to empower the generalization of robot learning for manipulation, i.e., skill learning. Furthermore, task-level reasoning and planning, i.e., task and motion planning (TAMP), are underexplored in robot learning methods. My PhD goal is to leverage an elegant bridge between the TAMP and robot skill learning. The robot can exe cute stably under a complex task domain by understanding geometry to develop new skills and doing dynamic replanning to comprise uncertainty. NEU’s collaborative environment and research strengths, particularly re search work from Professor Robert Platt, Professor Robin Walters, and Professor Lawson Wong, align perfectly with my aspirations. 
Community and Leadership Contributions: I am committed to promoting inclusivity and access to education for underprivileged communities through workshops, teaching assistant roles, and leadership in Robot Learning groups. I believe my experiences as an international student and advocate for diversity will contribute positively to NEU’s graduate community. 
References 
[1] Y. Wu, Z. Dai, H. Liu, L. Wang, and M. P. Nemitz, “Vision-based fdm printing for fabricating airtight soft actuators,” in 2024 IEEE 7th International Conference on Soft Robotics (RoboSoft). IEEE, 2024, pp. 249–254. 
[2] F. Lin, H. Liu, H. Zhou, S. Hou, K. D. Yamada, G. S. Fischer, Y. Li, H. K. Zhang, and Z. Zhang, “Loss distillation via gradient matching for point cloud completion with weighted chamfer distance,” arXiv preprint arXiv:2409.06171, 2024. 
[3] H. Huang, K. Schmeckpeper, D. Wang, O. Biza, Y. Qian, H. Liu, M. Jia, R. Platt, and R. Walters, “Imag ination policy: Using generative point cloud models for learning manipulation policies,” arXiv preprint arXiv:2406.11740, 2024. 
[4] H. Huang, H. Liu, D. Wang, R. Walters, and R. Platt, “Match policy: A simple pipeline from point cloud registration to manipulation policies,” arXiv preprint arXiv:2409.15517, 2024.
