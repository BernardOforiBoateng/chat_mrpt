# Tool #19 Implementation Notes

**Date**: 2025-10-04
**Feature**: DataExplorationAgent as Tool #19
**Status**: Complete - Ready for Staging
**Phase**: 1 (MVP - Basic Integration)

---

## Background & Context

### The Problem
Users were asking custom data queries like:
- "Show me top 10 wards by population"
- "Which wards have TPR > 0.5?"
- "What's the correlation between rainfall and TPR?"

RequestInterpreter had 18 pre-built tools but no way to handle arbitrary Python execution on user data.

### The Solution
Add DataExplorationAgent as Tool #19 - a specialized agent that can:
- Execute custom Python code on session data
- Access DataFrames (df) and GeoDataFrames (gdf)
- Perform calculations, filtering, and custom visualizations
- Work across all workflow phases (standard upload, post-TPR, post-risk)

### Architecture Decision
After exploring 3 options:
1. âŒ Replace RequestInterpreter (too risky)
2. âŒ Run parallel to RequestInterpreter (complex routing)
3. âœ… **Make agent Tool #19** (simple, evolutionary)

**Key Insight**: Agent is NOT replacing RequestInterpreter. It's just another tool the LLM can choose.

---

## Implementation Details

### Files Created

#### 1. app/data_analysis_v3/core/data_exploration_agent.py (~144 lines)

**Purpose**: Specialized agent for Python execution on session data

**Key Components**:

```python
class DataExplorationAgent(DataAnalysisAgent):
    """
    MVP: Simple Python execution on user data.
    Called by RequestInterpreter's analyze_data_with_python tool.
    Phase 1: Basic functionality
    Phase 2: Add sandbox, limits, security
    """
```

**Critical Methods**:

1. **`__init__(session_id)`**
   - Inherits from DataAnalysisAgent
   - Gets LangGraph workflow with Python tool
   - Logs initialization for debugging

2. **`analyze_sync(query)`** - Synchronous wrapper
   - Creates new event loop for async analyze()
   - Critical for RequestInterpreter compatibility
   - Handles loop cleanup properly
   ```python
   loop = asyncio.new_event_loop()
   asyncio.set_event_loop(loop)
   try:
       result = loop.run_until_complete(self.analyze(query))
       return result
   finally:
       loop.close()
   ```

3. **`_get_input_data()`** - Smart data loading
   - Detects workflow phase via flag files
   - Priority order:
     1. `unified_dataset.csv` (if .analysis_complete exists)
     2. `raw_data.csv` (if .tpr_complete or standard upload)
     3. `uploaded_data.csv` (fallback)
   - Also loads shapefile if available

4. **`_find_csv(session_folder)`**
   - Implements priority logic
   - Returns first match in priority order
   - Logs which file is being used

5. **`_find_shapefile(session_folder)`**
   - Checks raw_shapefile.zip (from TPR workflow)
   - Falls back to shapefile/*.shp (from standard upload)

### Files Modified

#### 2. app/core/request_interpreter.py (+60 lines)

**Location 1: Tool Registration** (line 143)
```python
# NEW: Python execution tool (Tool #19)
self.tools['analyze_data_with_python'] = self._analyze_data_with_python

logger.info(f"Registered {len(self.tools)} tools")  # Now shows 19
```

**Location 2: Tool Method** (line 1566)
```python
def _analyze_data_with_python(self, session_id: str, query: str) -> Dict[str, Any]:
    """
    Execute custom Python analysis on user data via DataExplorationAgent.

    Use this when users ask questions requiring:
    - Custom data queries (e.g., "show top 10 wards by population")
    - Calculations (e.g., "what's the correlation between rainfall and TPR")
    - Filtering (e.g., "which wards have TPR > 0.5")
    - Custom visualizations
    - Geospatial queries
    """
    logger.info(f"ğŸ TOOL: analyze_data_with_python called")

    try:
        from app.data_analysis_v3.core.data_exploration_agent import DataExplorationAgent

        agent = DataExplorationAgent(session_id=session_id)
        result = agent.analyze_sync(query)

        # CRITICAL: Map 'message' to 'response' for RI compatibility
        return {
            'status': result.get('status', 'success'),
            'response': result.get('message', ''),  # Map message â†’ response
            'visualizations': result.get('visualizations', []),
            'tools_used': ['analyze_data_with_python'],
            'insights': result.get('insights', [])
        }
    except Exception as e:
        logger.error(f"Error in analyze_data_with_python: {e}", exc_info=True)
        return {
            'status': 'error',
            'response': f'I encountered an error analyzing the data: {str(e)}',
            'tools_used': ['analyze_data_with_python']
        }
```

**Why This Mapping is Critical**:
- RequestInterpreter expects 'response' field
- DataAnalysisAgent returns 'message' field
- Without mapping, responses would be empty
- This was identified as critical integration point in planning

---

## Technical Decisions & Rationale

### 1. Inheritance from DataAnalysisAgent
**Decision**: DataExplorationAgent inherits from DataAnalysisAgent

**Rationale**:
- Reuses LangGraph workflow infrastructure
- Gets Python execution tool for free
- Maintains consistency with TPR workflow agent
- Minimal code duplication

**Alternative Considered**: Build standalone agent
- âŒ Would require duplicating LangGraph setup
- âŒ Would need to redefine Python tool
- âŒ More maintenance burden

### 2. Synchronous Wrapper Pattern
**Decision**: Add analyze_sync() wrapper method

**Rationale**:
- RequestInterpreter tools must be synchronous
- Agent's analyze() is async (LangGraph requirement)
- Event loop management is tricky in mixed contexts
- Clean separation of concerns

**Alternative Considered**: Make RequestInterpreter async
- âŒ Would require massive refactoring
- âŒ Would break existing tools
- âŒ Introduces complexity throughout system

### 3. Priority-Based Data Loading
**Decision**: Try unified â†’ raw â†’ uploaded in order

**Rationale**:
- Different workflow phases create different files
- unified_dataset.csv has most complete data (post-risk)
- raw_data.csv has TPR + environment (post-TPR)
- uploaded_data.csv is original (standard upload)
- This ensures agent always uses best available data

**Alternative Considered**: Flag-based explicit selection
- âŒ More complex logic
- âŒ Requires maintaining flag state
- âŒ Priority order is simpler and more resilient

### 4. Return Format Mapping
**Decision**: Explicitly map 'message' â†’ 'response'

**Rationale**:
- RequestInterpreter expects specific schema
- Agent returns different schema
- Mapping ensures compatibility
- Documented as critical integration point

**Alternative Considered**: Change agent to return 'response'
- âŒ Would break TPR workflow agent
- âŒ Agent format is standard for data analysis agents
- âŒ Better to adapt at integration boundary

### 5. Phase 1 MVP Scope (No Security)
**Decision**: Defer security hardening to Phase 2

**Rationale**:
- User explicitly requested: "can we not look at it later? like this sandbox etc"
- Focus on proving integration works first
- Easier to add security layer after basic functionality validated
- Reduces initial implementation risk

**Deferred to Phase 2**:
- Sandbox for code execution
- Resource limits (memory, CPU, time)
- Code whitelisting
- Feature flags for gradual rollout

---

## Key Integration Points

### 1. Tool Selection by LLM

The LLM (GPT-4o via RequestInterpreter) decides when to use Tool #19:

**Uses analyze_data_with_python**:
- "Show me top 10 wards" (custom query)
- "What's the correlation between rainfall and TPR?" (calculation)
- "Which wards have TPR > 0.5?" (filtering)
- "Create a scatter plot of elevation vs TPR" (custom viz)

**Uses other tools**:
- "Run the risk analysis" (run_malaria_risk_analysis)
- "Create vulnerability map" (create_vulnerability_map)
- "Explain the methodology" (explain_analysis_methodology)

**How LLM Decides**:
- Reads tool descriptions
- Analyzes user intent
- Chooses most appropriate tool
- This is standard function calling behavior

### 2. Data Flow Across Workflow Phases

```
Standard Upload
â”œâ”€â”€ uploaded_data.csv (original)
â”œâ”€â”€ shapefile/*.shp (boundaries)
â””â”€â”€ [User asks custom query]
    â””â”€â”€ Tool #19 loads uploaded_data.csv + shapefile

TPR Workflow Complete
â”œâ”€â”€ .tpr_complete (flag)
â”œâ”€â”€ raw_data.csv (TPR + environment)
â”œâ”€â”€ raw_shapefile.zip (matched boundaries)
â””â”€â”€ [User asks custom query]
    â””â”€â”€ Tool #19 loads raw_data.csv + raw_shapefile.zip

Risk Analysis Complete
â”œâ”€â”€ .analysis_complete (flag)
â”œâ”€â”€ unified_dataset.csv (master file with rankings)
â”œâ”€â”€ raw_shapefile.zip (boundaries)
â””â”€â”€ [User asks custom query]
    â””â”€â”€ Tool #19 loads unified_dataset.csv + raw_shapefile.zip
```

**Critical Insight**: Agent automatically uses the right data for each phase without explicit phase detection.

### 3. Conversation Flow

```
1. User sends message to Flask app
   â†“
2. Route calls RequestInterpreter.process_message()
   â†“
3. RI builds system prompt with tool descriptions
   â†“
4. LLM analyzes user message + available tools
   â†“
5. LLM selects tool (e.g., analyze_data_with_python)
   â†“
6. RI calls _analyze_data_with_python(session_id, query)
   â†“
7. Creates DataExplorationAgent(session_id)
   â†“
8. Agent loads data via _get_input_data()
   â†“
9. Agent calls analyze_sync(query)
   â†“
10. LangGraph executes:
    - Agent node: generates Python code
    - Tool node: executes code on data
    - Agent node: formats results
   â†“
11. Result returned to tool wrapper
   â†“
12. Tool maps 'message' â†’ 'response'
   â†“
13. RI returns formatted response to route
   â†“
14. Route sends to user
```

---

## Verification & Testing

### Test 1: File Structure Verification
**Script**: test_tool19_registration.py

**Tests**:
- âœ… DataExplorationAgent file exists
- âœ… Import successful
- âœ… analyze_sync() method exists
- âœ… Tool registration in RequestInterpreter
- âœ… Tool method implementation
- âœ… Return format mapping

**Result**: ALL PASSED âœ…

### Test 2: Integration Test (Attempted)
**Script**: test_tool19_integration.py

**Status**: Timeout (requires OPENAI_API_KEY to complete)

**What it would test**:
- Create sample session data
- Call agent directly
- Verify result format
- Test tool wrapper mapping

**Why it timed out**: Agent initialization requires OpenAI API, test environment doesn't have key configured

**Decision**: Structure verification sufficient for Phase 1, full integration test will happen on staging

### Test 3: Manual Testing Plan (Staging)
1. Upload CSV + shapefile via standard upload
2. Send query: "Show me the first 5 rows"
3. Check logs for: `ğŸ TOOL: analyze_data_with_python called`
4. Verify response contains actual data
5. Check logs for: `Registered 19 tools` (not 18)

---

## Lessons Learned

### 1. User Feedback Shaped Implementation
**Initial Plan**: Comprehensive implementation with security, feature flags, etc.

**User Feedback**: "can we not look at it later? like this sandbox etc"

**Adapted Approach**: Phase 1 (basic) â†’ Phase 2 (hardening)

**Lesson**: Listen to user priorities, don't over-engineer upfront

### 2. Return Format Compatibility is Critical
**Discovery**: Agent returns 'message', RI expects 'response'

**Impact**: Without mapping, all responses would be empty

**Lesson**: Always verify schema compatibility at integration boundaries

### 3. Async/Sync Mismatch Required Wrapper
**Problem**: Agent is async, RI tools must be sync

**Solution**: analyze_sync() wrapper with event loop management

**Lesson**: Framework constraints drive design patterns

### 4. Priority-Based Loading is Resilient
**Approach**: Try files in priority order until one exists

**Benefit**: Works across all workflow phases without phase detection

**Lesson**: Simple heuristics often beat complex state machines

### 5. Test What You Can, Deploy for Full Test
**Reality**: Full integration requires OpenAI API + Flask + session state

**Pragmatic Approach**: Verify structure locally, test integration on staging

**Lesson**: Some tests are better done in target environment

---

## Deployment Strategy

### Staging Deployment
**Script**: deploy_tool19_staging.sh

**Steps**:
1. Create backup on staging (safety net)
2. Deploy data_exploration_agent.py
3. Deploy request_interpreter.py
4. Restart service
5. Verify tool registration in logs

**Verification**:
- Check logs for "Registered 19 tools"
- Manual test with sample query
- Monitor for errors

### Production Deployment (After Staging Success)
**Target**: Both production instances (3.21.167.170, 18.220.103.20)

**Critical**: Must deploy to BOTH instances

**Steps**: Same as staging, but to both IPs

### Rollback Plan
**If issues occur**:
1. SSH to affected server
2. Extract backup: `tar -xzf ChatMRPT_pre_tool19_*.tar.gz`
3. Restore files
4. Restart service

**Recovery Time**: < 2 minutes

---

## Future Enhancements (Phase 2+)

### Security Hardening
- Sandbox for code execution (RestrictedPython or similar)
- Whitelist safe modules (pandas, numpy, geopandas)
- Block dangerous builtins (eval, exec, __import__)
- Deny filesystem/network/process access

### Resource Limits
- CPU time limit per query (e.g., 30s)
- Memory limit per session (e.g., 500MB)
- Row count limit for results (e.g., 1000 rows)
- Concurrent query limit per user

### Feature Management
- Feature flag: `ENABLE_TOOL19` (default: True)
- Gradual rollout by user segment
- A/B testing support
- Quick disable if issues found

### Data Access Abstraction
- SessionDataResolver class
- Consistent interface for data loading
- Better error messages when data missing
- Schema validation

### Testing & Monitoring
- Unit tests with LLM mocks
- Integration tests with sample data
- Performance benchmarks
- Usage metrics (tool selection rate, success rate, latency)

### Performance Optimization
- Cache parsed DataFrames per session
- Reuse agent instances where safe
- Streaming support for large results
- Query optimization hints

---

## Success Metrics

### Week 1 (Validation)
- [ ] Tool #19 registered successfully
- [ ] No errors in logs
- [ ] At least 1 successful query execution
- [ ] Response format correct

### Month 1 (Early Adoption)
- [ ] Handles 5-10% of queries
- [ ] Zero critical errors
- [ ] User satisfaction maintained
- [ ] Response time < 5s average

### Month 3 (Growing Adoption)
- [ ] Handles 20-30% of queries
- [ ] Users prefer for custom queries
- [ ] Identify redundant simple tools
- [ ] Plan tool consolidation

### Month 6 (Maturity)
- [ ] Handles 50-70% of queries
- [ ] Phase 2 security implemented
- [ ] Begin deprecating redundant tools
- [ ] Reduced maintenance burden

---

## Questions Answered During Implementation

### Q: "Where to point to?"
**A**: `app.data_analysis_v3.core.data_exploration_agent.DataExplorationAgent`

### Q: "What codes to register?"
**A**: Add 1 line in `_register_tools()`: `self.tools['analyze_data_with_python'] = self._analyze_data_with_python`

### Q: "How to make it seamless?"
**A**:
- Return format matches RI (`response` not `message`)
- Sync wrapper handles async (`analyze_sync()`)
- Error handling returns dict, doesn't raise
- Smart data loading works across workflow phases

### Q: "Will this break existing functionality?"
**A**: No - agent is just another tool, all existing logic preserved

### Q: "What if it fails?"
**A**: Backup + rollback in < 2 minutes

---

## Related Documentation

### Implementation Docs
- `tasks/PHASE1_MVP_IMPLEMENTATION.md` - Simplified implementation plan
- `tasks/implementation_plan_agent_as_tool.md` - Comprehensive plan
- `tasks/EXECUTIVE_SUMMARY.md` - High-level overview
- `tasks/PHASE1_IMPLEMENTATION_COMPLETE.md` - Completion summary

### Code Files
- `app/data_analysis_v3/core/data_exploration_agent.py` - Agent implementation
- `app/core/request_interpreter.py` - Tool registration & wrapper
- `test_tool19_registration.py` - Structure verification test
- `test_tool19_integration.py` - Integration test (requires API key)
- `deploy_tool19_staging.sh` - Deployment script

### Context Files
- `context.md` - Review feedback from another agent
- `CLAUDE.md` - Project development guide

---

## Final Notes

### What Worked Well
- âœ… User-driven scoping (Phase 1 MVP approach)
- âœ… Inheritance strategy (reuse existing agent)
- âœ… Clear integration points (tool registration, return mapping)
- âœ… Comprehensive documentation
- âœ… Simple rollback strategy

### What Could Be Improved
- âš ï¸ Full integration test requires staging environment
- âš ï¸ Security hardening deferred (acceptable for Phase 1)
- âš ï¸ No feature flag yet (can add in Phase 2)

### Key Takeaway
**"Agent as Tool #19" architecture successfully implemented with minimal changes, no breaking impact, and clear evolutionary path.**

---

**Implementation completed**: 2025-10-04
**Phase 1 Status**: âœ… COMPLETE
**Next Step**: Deploy to staging
