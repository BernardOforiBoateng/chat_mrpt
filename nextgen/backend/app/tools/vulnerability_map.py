"""Vulnerability Map Tool for NextGen - Risk Classification Visualization"""

from __future__ import annotations

import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import geopandas as gpd
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from pydantic import BaseModel, Field
from shapely.geometry import mapping

from ..services.column_store import ColumnStore
from ..services.dataset_registry import DatasetRegistry, DatasetEntry
from ..services.plot_service import PlotService

logger = logging.getLogger(__name__)


class VulnerabilityMapParams(BaseModel):
    """Parameters for vulnerability map visualization."""

    dataset_id: str = Field(..., description="Dataset ID to visualize")
    method: str = Field("composite", description="Risk method: 'composite' or 'pca'")
    level: str = Field("ward", description="Administrative level: ward, lga, or state")
    risk_categories: int = Field(3, description="Number of risk categories (3-5)", ge=3, le=5)


@dataclass
class VulnerabilityMapResult:
    """Result from vulnerability map tool execution."""

    plot_id: str
    title: str
    method: str
    level: str
    risk_categories: int
    category_counts: dict
    figure: dict


class VulnerabilityMapTool:
    """
    Create vulnerability/risk classification maps using composite or PCA scores.

    This tool creates choropleth maps showing wards colored by their risk categories.
    Requires that risk analysis (composite or PCA) has been completed first.
    Supports ward, LGA, and state-level visualization.
    """

    def __init__(
        self,
        column_store: ColumnStore,
        dataset_registry: DatasetRegistry,
        plot_service: PlotService,
    ) -> None:
        self.column_store = column_store
        self.datasets = dataset_registry
        self.plot_service = plot_service

    def execute(self, session_id: str, params: VulnerabilityMapParams) -> dict:
        """
        Execute vulnerability map visualization.

        Args:
            session_id: Current session ID
            params: Visualization parameters

        Returns:
            dict with plot_id, title, method, category info, and figure

        Raises:
            ValueError: If dataset not found, risk scores missing, or shapefile missing
        """
        try:
            # Get dataset entry
            entry = self.datasets.get_for_session(params.dataset_id, session_id)
            if not entry:
                raise ValueError(f"Dataset {params.dataset_id} not found for session")

            # Load data
            df = self._load_dataset(entry)

            # Check for risk scores
            score_col, rank_col, category_col = self._get_score_columns(df, params.method)
            if not score_col:
                raise ValueError(
                    f"{params.method.upper()} risk analysis not found. "
                    f"Please run {params.method} risk analysis first."
                )

            # Load shapefile for geographic boundaries
            gdf = self._load_shapefile(entry, params.level)
            if gdf is None:
                raise ValueError(
                    f"Shapefile required for spatial visualization. "
                    f"Please ensure shapefile was uploaded with the dataset."
                )

            # Merge data with geography
            merged = self._merge_data(gdf, df, params.level)
            if merged.empty:
                raise ValueError(f"No data could be matched with geographic boundaries at {params.level} level")

            # Create or use existing categories
            if category_col not in merged.columns or merged[category_col].isna().all():
                merged = self._create_categories(merged, score_col, params.risk_categories)
                category_col = 'risk_category'

            # Create vulnerability map
            fig = self._create_vulnerability_map(
                merged,
                score_col,
                rank_col,
                category_col,
                params.method,
                params.level
            )

            # Calculate category counts
            category_counts = merged[category_col].value_counts().to_dict()

            # Save plot
            title = f"{params.method.upper()} Vulnerability Map - {params.level.upper()} Level"
            plot_id = self.plot_service.save_plot(
                session_id=session_id,
                dataset_id=params.dataset_id,
                plot_function="vulnerability_map",
                title=title,
                figure=fig,
            )

            logger.info(
                "Vulnerability map created: session=%s, method=%s, level=%s, plot_id=%s",
                session_id,
                params.method,
                params.level,
                plot_id
            )

            return {
                "type": "plot",
                "plot_id": plot_id,
                "title": title,
                "method": params.method,
                "level": params.level,
                "risk_categories": params.risk_categories,
                "category_counts": category_counts,
                "total_wards": len(merged),
                "figure": fig,
            }

        except Exception as exc:
            logger.error(
                "Vulnerability map failed: session=%s, method=%s, error=%s",
                session_id,
                params.method,
                exc
            )
            raise ValueError(f"Failed to create vulnerability map: {exc}") from exc

    def _load_dataset(self, entry: DatasetEntry) -> pd.DataFrame:
        """Load dataset from parquet or CSV."""
        derived = entry.metadata.get("derived_assets", {})
        parquet_path = derived.get("parquet")

        if parquet_path and Path(parquet_path).exists():
            return pd.read_parquet(parquet_path)

        return pd.read_csv(entry.path)

    def _get_score_columns(self, df: pd.DataFrame, method: str) -> tuple[Optional[str], Optional[str], Optional[str]]:
        """
        Get appropriate score, rank, and category columns for the method.

        Returns:
            Tuple of (score_col, rank_col, category_col) or (None, None, None) if not found
        """
        if method == "composite":
            score_candidates = ["composite_score", "overall_score", "risk_score"]
            rank_candidates = ["composite_rank", "overall_rank", "risk_rank"]
            category_candidates = ["composite_category", "vulnerability_category", "risk_category"]
        elif method == "pca":
            score_candidates = ["pca_score", "pca_composite_score", "pc1"]
            rank_candidates = ["pca_rank", "pca_composite_rank"]
            category_candidates = ["pca_category", "pca_composite_category"]
        else:
            return None, None, None

        # Find score column
        score_col = None
        for candidate in score_candidates:
            if candidate in df.columns:
                score_col = candidate
                break

        if not score_col:
            return None, None, None

        # Find rank column
        rank_col = None
        for candidate in rank_candidates:
            if candidate in df.columns:
                rank_col = candidate
                break

        # Find category column (optional)
        category_col = None
        for candidate in category_candidates:
            if candidate in df.columns:
                category_col = candidate
                break

        return score_col, rank_col, category_col

    def _load_shapefile(self, entry: DatasetEntry, level: str) -> Optional[gpd.GeoDataFrame]:
        """Load shapefile for the specified administrative level."""
        derived = entry.metadata.get("derived_assets", {})
        shapefile_zip = derived.get("shapefile_zip")

        if not shapefile_zip or not Path(shapefile_zip).exists():
            logger.warning(f"No shapefile found for dataset {entry.dataset_id}")
            return None

        try:
            import tempfile
            import zipfile

            with tempfile.TemporaryDirectory() as tmpdir:
                with zipfile.ZipFile(shapefile_zip, 'r') as zip_ref:
                    zip_ref.extractall(tmpdir)

                shp_files = list(Path(tmpdir).glob("**/*.shp"))
                if not shp_files:
                    logger.error(f"No .shp file found in {shapefile_zip}")
                    return None

                gdf = gpd.read_file(shp_files[0])
                logger.info(f"Loaded shapefile: {len(gdf)} features, CRS: {gdf.crs}")

                if gdf.crs and gdf.crs.to_string() != "EPSG:4326":
                    gdf = gdf.to_crs("EPSG:4326")

                return gdf

        except Exception as exc:
            logger.error(f"Failed to load shapefile: {exc}")
            return None

    def _merge_data(self, gdf: gpd.GeoDataFrame, df: pd.DataFrame, level: str) -> gpd.GeoDataFrame:
        """Merge tabular data with geographic boundaries."""
        join_attempts = [
            ("WardCode", "WardCode"), ("WardName", "WardName"),
            ("ward_code", "ward_code"), ("ward_name", "ward_name"),
            ("LGACode", "LGACode"), ("LGAName", "LGAName"),
            ("lga_code", "lga_code"), ("lga_name", "lga_name"),
            ("StateCode", "StateCode"), ("StateName", "StateName"),
            ("state_code", "state_code"), ("state_name", "state_name"),
        ]

        for left_col, right_col in join_attempts:
            if left_col in gdf.columns and right_col in df.columns:
                merged = gdf.merge(df, left_on=left_col, right_on=right_col, how="left")
                logger.info(f"Merged on {left_col} = {right_col}: {len(merged)} records")
                return merged

        logger.warning("No common columns found, attempting index merge")
        return gdf.merge(df, left_index=True, right_index=True, how="left")

    def _create_categories(self, df: pd.DataFrame, score_col: str, n_categories: int) -> pd.DataFrame:
        """Create risk categories from scores using quantile classification."""
        df = df.copy()

        valid_scores = df[score_col].dropna()
        if valid_scores.empty:
            df['risk_category'] = 'Unknown'
            return df

        # Use quantile classification
        quantiles = np.linspace(0, 1, n_categories + 1)
        bins = valid_scores.quantile(quantiles).tolist()

        # Ensure unique bins
        bins = sorted(set(bins))
        if len(bins) < 2:
            df['risk_category'] = 'Medium Risk'
            return df

        # Create category labels
        if n_categories == 3:
            labels = ['Low Risk', 'Medium Risk', 'High Risk']
        elif n_categories == 4:
            labels = ['Low Risk', 'Medium-Low Risk', 'Medium-High Risk', 'High Risk']
        else:  # 5
            labels = ['Very Low Risk', 'Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk']

        # Adjust labels if we have fewer bins than expected
        labels = labels[:len(bins) - 1]

        df['risk_category'] = pd.cut(
            df[score_col],
            bins=bins,
            labels=labels,
            include_lowest=True,
            duplicates='drop'
        )
        df['risk_category'] = df['risk_category'].fillna('Unknown')

        return df

    def _create_vulnerability_map(
        self,
        gdf: gpd.GeoDataFrame,
        score_col: str,
        rank_col: Optional[str],
        category_col: str,
        method: str,
        level: str
    ) -> dict:
        """Create Plotly vulnerability choropleth map."""
        # Filter valid data
        clean = gdf.dropna(subset=[score_col])
        clean = clean[clean.geometry.notnull() & ~clean.geometry.is_empty]

        if clean.empty:
            raise ValueError(f"No valid data for {method} vulnerability mapping")

        # Build GeoJSON
        features = []
        for idx, row in clean.iterrows():
            try:
                features.append({
                    'type': 'Feature',
                    'id': str(idx),
                    'geometry': mapping(row.geometry),
                    'properties': {}
                })
            except Exception as exc:
                logger.warning(f"Skipping invalid geometry at index {idx}: {exc}")
                continue

        if not features:
            raise ValueError("No valid geometries to display")

        geojson = {
            'type': 'FeatureCollection',
            'features': features
        }

        # Prepare hover text
        label_col = self._get_label_column(clean, level)
        labels = clean.get(label_col, clean.index).astype(str)

        hover_parts = [f"<b>{labels}</b>"]
        if rank_col and rank_col in clean.columns:
            hover_parts.append(f"Rank: {clean[rank_col].fillna('N/A').astype(str)}")
        hover_parts.append(f"Category: {clean[category_col].astype(str)}")
        hover_parts.append(f"Score: {clean[score_col].round(3).astype(str)}")

        hover_text = [
            '<br>'.join([
                labels.iloc[i],
                f"Rank: {clean[rank_col].iloc[i] if rank_col and rank_col in clean.columns else 'N/A'}",
                f"Category: {clean[category_col].iloc[i]}",
                f"Score: {clean[score_col].iloc[i]:.3f}"
            ])
            for i in range(len(clean))
        ]

        # Vulnerability maps use reverse color scale (high risk = dark red)
        fig = go.Figure()

        fig.add_trace(go.Choroplethmapbox(
            geojson=geojson,
            locations=clean.index.astype(str),
            z=clean[score_col],
            colorscale='Reds',
            reversescale=False,  # High scores = dark red
            text=hover_text,
            hovertemplate='%{text}<extra></extra>',
            marker_opacity=0.7,
            marker_line_width=1,
            marker_line_color='white',
            showscale=True,
            colorbar=dict(
                title=f"{method.upper()}<br>Risk Score",
                thickness=15,
                len=0.7
            )
        ))

        # Calculate center
        bounds = clean.total_bounds
        center_lat = (bounds[1] + bounds[3]) / 2
        center_lon = (bounds[0] + bounds[2]) / 2

        # Update layout
        fig.update_layout(
            title={
                'text': f'{method.upper()} Vulnerability Map - {level.upper()} Level',
                'x': 0.5,
                'xanchor': 'center',
                'font': {'size': 18, 'color': '#2E3440'}
            },
            mapbox=dict(
                style='open-street-map',
                center=dict(lat=center_lat, lon=center_lon),
                zoom=8
            ),
            height=600,
            margin=dict(t=60, b=20, l=20, r=20),
            template='plotly_white'
        )

        return fig.to_dict()

    def _get_label_column(self, gdf: gpd.GeoDataFrame, level: str) -> str:
        """Get appropriate label column for administrative level."""
        if level == "ward":
            for col in ["WardName", "ward_name", "WardCode", "ward_code"]:
                if col in gdf.columns:
                    return col
        elif level == "lga":
            for col in ["LGAName", "lga_name", "LGACode", "lga_code"]:
                if col in gdf.columns:
                    return col
        elif level == "state":
            for col in ["StateName", "state_name", "StateCode", "state_code"]:
                if col in gdf.columns:
                    return col

        return "index"
