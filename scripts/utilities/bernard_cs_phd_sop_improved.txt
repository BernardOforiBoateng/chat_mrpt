Statement of Purpose
Bernard Boateng

How can we bridge the gap between cutting-edge AI capabilities and the urgent needs of global health practitioners? While large language models have revolutionized how we interact with information, a critical disconnect remains: health officials in resource-constrained settings still struggle with spreadsheets and manual analysis while AI systems capable of complex reasoning remain inaccessible to them. This paradox drives my research interest at the intersection of natural language processing and public health, where I aim to develop AI systems that transform how epidemiological insights are discovered and applied.

Through developing ChatMRPT—a conversational AI system that enables Nigerian health officials to analyze malaria risk through natural language—I discovered both the transformative potential and current limitations of applying LLMs to public health challenges. This experience, combined with my review of current literature, revealed several fundamental research questions I am interested in exploring.

First, how can we adapt language models to understand the nuanced terminology and reasoning patterns of public health practitioners? While models like BioBERT and ClinicalBERT excel at processing medical literature, they fail to capture the operational language of field epidemiology—the difference between "incidence rate" in a textbook versus "cases we're seeing this week" in the field. Recent work on domain-specific tokenization shows promise, yet gaps remain in contextual reasoning. I'm interested in exploring retrieval-augmented generation approaches that could incorporate dynamic knowledge graphs updating with emerging health patterns, enabling models to reason about local disease contexts while maintaining epidemiological principles.

Second, what role could AI agents play in orchestrating complex epidemiological analyses across multiple data modalities and formats? Current multi-modal transformers focus on vision-language tasks, but health applications require autonomous agents that can reason across structured databases, time-series surveillance data, and spatial information simultaneously—while also reconciling format inconsistencies within each modality. Through ChatMRPT, I observed how the same epidemiological metric (like treatment coverage rates) arrives in vastly different formats across regions and reporting periods, and how simple concatenation of modalities loses critical relationships. Understanding disease patterns requires agents that can not only dynamically query rainfall patterns, analyze population density, and assess healthcare access, but also harmonize these diverse data representations on the fly. Potential approaches include developing agentic systems with sophisticated data understanding capabilities—agents that can infer schema mappings, recognize equivalent metrics despite different naming conventions, and adapt to evolving data standards. The emergence of foundation model agents suggests exciting possibilities for health-specific agents that could autonomously conduct epidemiological investigations while handling the messy reality of global health data.

Third, how do we ensure these powerful tools reduce rather than amplify health inequities? Existing fairness frameworks in AI don't address global health's unique challenge: models trained on data from well-resourced settings performing poorly in resource-constrained contexts. Through ChatMRPT, I observed how models trained on formal medical and scientific literature often fail to understand the operational vocabulary and contextual nuances used by health practitioners in diverse field settings. I'm interested in investigating fairness-aware training objectives that optimize for equitable performance across diverse populations, potentially using techniques like distributionally robust optimization. Equally important is exploring participatory design methods that involve communities in defining what constitutes beneficial AI for their specific contexts.

My path to computer science began during my civil engineering studies at the University of Johannesburg, where I discovered the power of programming to automate complex calculations. For my final year project, I developed a machine learning system for structural load-bearing analysis that integrated sensor data with predictive models, demonstrating 25% improvement in failure prediction accuracy compared to traditional methods. This project, which earned recognition from faculty, convinced me to pursue graduate studies in data science.

During my Master's in Data Science at Loyola University Chicago (GPA: 3.73), I began exploring how computational methods could address challenges I'd observed in global health. Natural Language Processing (COMP 429) opened my eyes to the potential of transformer models and conversational AI, though I didn't yet know I'd apply these concepts to help health officials in Nigeria. Big Data Analytics (COMP 458) with Professor Abuhamad challenged me to think beyond single-machine solutions—a perspective that proved essential when I later needed to process 15GB of geospatial data efficiently. The statistical foundations from Predictive Analytics (STAT 438) gave me tools to handle the messy, incomplete data that characterizes real-world health applications.

As lead developer of ChatMRPT at Loyola's Urban Malaria Lab, I addressed a critical challenge: health officials in Nigeria spent days manually analyzing spreadsheets to prioritize malaria interventions. I transformed an existing R-based tool into a conversational AI system by developing three key innovations. First, I created a multi-modal pipeline integrating demographic CSVs, shapefiles, and satellite imagery through natural language commands, using GeoPandas for spatial operations and custom NLP parsing for query interpretation. Second, I implemented a context-aware request interpreter using a hierarchical state machine that maintains conversation history across analytical steps, enabling users to build complex queries iteratively. Third, I architected a scalable deployment on AWS using distributed caching and lazy loading strategies, reducing query response time from minutes to under 2 seconds for 15GB datasets. The system now serves health officials across multiple Nigerian states, with our analysis revealing settlement-type disparities that influenced resource allocation decisions affecting over 2 million people.

During my internship, I developed a Factorization-based Texture Segmentation model for classifying urban settlements from satellite imagery. Working with 10,000+ images covering 200 sq km, I implemented a novel feature extraction pipeline combining spectral and textural features. The model achieved 80% classification accuracy, 15% above baseline methods, and identified previously unrecognized informal settlements. This project taught me the importance of domain-specific feature engineering and sparked my interest in multi-modal learning for health applications.

Loyola offers an exceptional interdisciplinary environment for my research through established collaborations with my co-supervisors. Professor Ifeoma Ozodiegwu, who leads the Urban Malaria Lab, brings deep expertise in mathematical modeling for malaria intervention planning and urban malaria microstratification. Her work developing frameworks for sub-national intervention tailoring directly complements the technical capabilities of ChatMRPT. Through our collaboration, we've identified how AI can address critical gaps in epidemiological analysis—her field experience ensures our research solutions address real constraints faced by health programs. Professor Francisco Iacobelli from Health Informatics and Data Science bridges clinical and public health applications of AI. His expertise in dialogue systems, natural language processing for health applications, and addressing health disparities through technology aligns perfectly with my goal of making AI accessible to health practitioners in resource-constrained settings.

From Computer Science, Professor Dmitry Dligach's pioneering work in clinical NLP, particularly his research on domain-specific pre-training strategies (EntityBERT) and uncertainty estimation in medical LLMs, provides the technical foundation for advancing language models in health contexts. His methodologies for adapting LLMs to understand medical terminology offer a roadmap for extending these approaches to epidemiological applications. Professor Mohammed Abuhamad's expertise in AI security and adversarial robustness addresses a critical need—ensuring these AI systems remain reliable and secure when deployed in settings where computational resources are limited and errors have serious consequences.

This unique configuration of advisors—combining epidemiological modeling, health informatics, clinical NLP, and AI security—positions me to develop AI systems that are both technically sophisticated and grounded in real-world health needs. The synergy between Loyola's Parkinson School of Health Sciences and Public Health and the Computer Science department creates an environment where theoretical advances can be rapidly tested and refined based on field feedback, ensuring my research produces tools that genuinely serve global health practitioners.

My long-term research vision encompasses three goals. First, developing domain-specific foundation models that smaller health organizations can adapt without extensive computational resources. Second, creating open-source frameworks that enable domain experts to build specialized AI assistants through natural language specifications rather than programming. Third, establishing evaluation benchmarks that assess both technical performance and real-world utility in specialized domains. I aim to pursue an academic career where I can train the next generation of researchers while maintaining collaborations with global health organizations to ensure my work addresses real needs.

My background—from rural Ghana through engineering studies in South Africa to AI research in Chicago—provides unique perspectives on technology's potential to address global inequities. This journey, including overcoming personal challenges, motivates my commitment to making sophisticated analytical tools accessible to resource-constrained communities. Through doctoral studies at Loyola, I will advance the theoretical foundations of domain-adaptive NLP while never losing sight of the human impact that drives this work. I am ready to contribute to Loyola's research community and work towards a future where language barriers—both human and technical—no longer prevent access to life-saving analytical capabilities.